[["glossario-della-notazione.html", "Glossario della notazione Statistica descrittiva Calcolo delle probabilità Inferenza statistica Simboli Matematici", " Glossario della notazione Statistica descrittiva Simbolo Significato \\(x_1, \\dots, x_n\\) Dati nell’ordine di osservazione \\(x_{(1)}, \\dots, x_{(n)}\\) Dati ordinati, dal più piccolo al più grande \\(n\\) Ampiezza del collettivo \\(\\bar{x}\\) Media aritmetica dei dati \\(x_{p}\\) \\(p\\times 100\\%\\) percentile \\(\\sigma, \\sigma^2\\) Deviazione standard e varianza dei dati \\(S_X=\\{\\text{x}_1,...,\\text{x}_K\\}\\) Insieme delle modalità di \\(X\\) \\(K\\) Numero di modalità della \\(X\\) \\(f_j\\) Frequenza relativa della modalità/intervallo \\(j\\) \\(n_j\\) Frequenza assoluta della modalità/intervallo \\(j\\) \\(F_j\\) Frequenza cumulata relativa \\(h_j\\) Densità di frequenza relativa \\([x_j, x_{j+1})\\) Intervallo di classe (per variabili continue) Calcolo delle probabilità Simbolo Significato \\(A, B, ...\\) Eventi \\(\\Omega\\) Evento certo \\(\\emptyset\\) Evento impossibile \\(P(A)\\) Probabilità dell’evento \\(A\\) \\(X, Y, W, \\dots\\) Variabili casuali (discrete o continue) \\(x, y, w, \\dots\\) Valori possibili delle variabili casuali \\(P(X = x)\\) Probabilità che la v.c. \\(X\\) assuma valore \\(x\\) \\(f(x)\\) Probabilità/Densità di probabilità della VC \\(X\\) \\(F(x)\\) Funzione di ripartizione della VC \\(X\\) \\(E(X)\\) Valore atteso della della VC \\(X\\) \\(V(X)\\) Varianza della della VC \\(X\\) \\(\\mu\\) Parametro della media \\(E(X)\\) di \\(X\\), in particolare se \\(X \\sim N(\\mu, \\sigma^2)\\) \\(\\sigma^2, \\sigma\\) Parametri di varianza e deviazione standard, in particolare se \\(X \\sim N(\\mu, \\sigma^2)\\) \\(\\pi, \\lambda\\) Parametri di modelli (es. \\(\\pi\\) per Bernoulli, \\(\\lambda\\) per Poisson) Inferenza statistica Simbolo Significato \\(\\theta\\) Parametro da stimare \\(\\Theta\\) Spazio dei parametri \\(X\\sim\\mathscr{L}({\\theta}),\\theta\\in\\Theta\\) Modello probabilistico \\(IID\\) Indipendenti ed Identicamente Distribuite \\(\\mathcal{S}\\) Spazio dei campioni \\(\\hat{\\theta}\\) Stimatore di un parametro \\(\\theta\\) \\(\\hat\\mu\\) Media campionaria come v.c. (media di \\(X_1, \\dots, X_n\\)) \\(\\hat\\sigma^2\\) Varianza campionaria come v.c. \\(\\hat{\\pi}\\) Proporzione campionaria \\(MSE(\\hat\\theta)\\) Mean Squared Error, errore quadratico medio di \\(\\hat\\theta\\). \\(SE(\\hat\\theta)\\) Standard Error, errore standard di \\(\\hat\\theta\\) \\(H_0, H_1\\) Ipotesi nulla e alternativa \\(T\\) Statistica test \\(\\alpha\\) Livello di significatività prefissato \\(IdC\\) Intervallo di confidenza \\(p_\\text{value}\\) Probabilità, sotto \\(H_0\\), di osservare dati acnora più a favore di \\(H_1\\) Simboli Matematici Simbolo Significato \\(\\mathbb{N}\\) Numeri naturali \\(\\{0,1,2,...\\}\\) \\(\\mathbb{Z}\\) Numeri interi \\(\\{...,-3,-2,-1,0,+1,+2,+3,...\\}\\) \\(\\mathbb{Z}^+\\) Numeri interi positivi \\(\\{1,2,3,...\\}\\) \\(\\mathbb{R}\\) Numeri reali \\(-\\infty&lt;x&lt;+\\infty\\) \\(\\mathbb{R}^+\\) Numeri reali positivi \\(0\\le x&lt;+\\infty\\) \\(\\mathbb{R}^2\\) Piano cartesiano: insieme delle coppie ordinate di numeri reali \\((x_1,x_2)\\), \\(\\mathbb{R}^2 = \\mathbb{R} \\times \\mathbb{R}\\) \\(\\mathbb{R}^n\\) Insieme delle \\(n\\)-ple ordinate di numeri reali \\((x_1,...,x_n)\\), \\(\\mathbb{R}^n = \\mathbb{R} \\times ... \\times \\mathbb{R}\\) \\(\\displaystyle \\binom{n}{k}\\) Coefficiente binomiale \\(n\\) su \\(k\\) \\(\\text{sign}(x)\\) segno di \\(x\\), \\(+1\\) se \\(x&gt;0\\) e \\(-1\\) se \\(x&lt;0\\) e \\(0\\) se \\(x=0\\) \\(\\lfloor x \\rfloor\\) è la parte intera di \\(x\\), es \\(\\lfloor 3. 7 \\rfloor =3\\) \\(\\lceil x \\rceil\\) è la parte intera superiore di \\(x\\), es \\(\\lceil 3.2 \\rceil = 4\\) "],["riepilogo-1.html", "Riepilogo Satistica Descrittiva Probabilità Inferenza", " Riepilogo Satistica Descrittiva Un Variabile Statistica può essere: Qualitativa, la variabile è espressa attraverso etichette qualitative Qualitative sconnesse: le caratteristiche che la VS può assumere hanno un ordinamento soggettivo; genere, stato civile, settore di occupazione, generi musicali. Qualitative ordinate: le caratteristiche che la VS può assumere hanno un ordinamento oggettivo titolo di studio, preferenze, giudizi. Quantitativa, la variabile è espressa attraverso una scala numerica. Quantitative Discrete: le caratteristiche che la VS può assumere sono in numero finito al più numerabile \\(\\rightarrow\\) corrispondenza con i numeri interi; numero di incidenti, voto di laurea. Quantitative Continue: le caratteristiche che la VS può assumere sono in numero infinito non numerabile. misure di lunghezza, capienza e peso, temperature, reddito. Definizione (Frequenze Relative) Si definiscono le \\(f_j=n_j/n\\) le frequenze relative: la proporzione di individui che presentano la modalità \\(j\\). Definizione (Frequenze Percentuali) Si definiscono le \\(f_{\\% j}=f_j\\times 100\\) le frequenze percentuali: la percentuale di individui che presentano la modalità \\(j\\). Proposizione Le proprietà della frequenze assolute(\\(n_{j}\\)) sono: \\(0\\leq n_{j} \\leq n, \\forall j=1,...,K\\), \\(\\sum_{j=1}^{K} n_{j} = n\\). Proposizione Le proprietà della frequenze relative (\\(f_{j}\\)) sono: \\(0\\leq f_{j} \\leq 1, \\forall j=1,...,K\\), \\(\\sum_{j=1}^{K} f_{j} = 1\\). Proposizione Le proprietà della frequenze percentuali (\\(f_{\\% j}\\)) sono: \\(0\\leq f_{\\%,\\, j} \\leq 100, \\forall j=1,...,K\\), \\(\\sum_{j=1}^{K} f_{\\%,\\, j} = 100\\). Definizione (Distribuzione di Frequenza) Una distribuzione di frequenza è una tabella a cui vengono associate le modalità e le frequenze Definizione (Frequenze Cumulate) Si definisce frequenza cumulata \\(F_j\\) la somma di tutte le \\(f\\) fino a \\(j\\), \\[ F_j = f_1 + f_2 + ... + f_j \\] Definizione (Densità di frequenza) Si definisce la densità di frequenza della classe \\(j\\) il rapporto tra la frequenza e l’ampiezza della classe. \\[h_j = Const.\\times \\frac {f_j} {b_j}\\] dove \\(Const.\\) è una costante numerica e \\(b_j=\\text{x}_{j+1}-\\text{x}_{j}\\). In sintesi Estremo inf Estremo sup freq. ass. freq. relativa freq. cum. ampiezza densità \\([\\text{x}_1,\\) \\(\\text{x}_2)\\) \\(n_1\\) \\(f_1=\\frac{n_1}{n}\\) \\(F_1=f_1\\) \\(b_1=\\text{x}_2-\\text{x}_1\\) \\(h_1=100\\times \\frac{ h_1}{b_1}\\) \\([\\text{x}_2,\\) \\(\\text{x}_3)\\) \\(n_2\\) \\(f_2=\\frac{n_2}n\\) \\(F_2=F_1+f_2\\) \\(b_2=\\text{x}_3-\\text{x}_2\\) \\(h_2=100\\times \\frac{ f_2}{b_2}\\) \\(...\\) \\(...\\) \\(...\\) \\(...\\) \\(...\\) \\(...\\) \\(...\\) \\([\\text{x}_j,\\) \\(\\text{x}_{j+1})\\) \\(n_j\\) \\(f_j=\\frac{n_j}n\\) \\(F_j=F_{j-1}+f_j\\) \\(b_j=\\text{x}_{j+1}-\\text{x}_j\\) \\(h_j=100\\times \\frac{ f_j}{b_j}\\) \\(...\\) \\(...\\) \\(...\\) \\(...\\) \\(...\\) \\(...\\) \\(...\\) \\([\\text{x}_K,\\) \\(\\text{x}_{K+1})\\) \\(n_K\\) \\(f_K=\\frac{n_K}n\\) \\(F_K=F_{K-1}+f_K\\) \\(b_K=\\text{x}_{K+1}-\\text{x}_K\\) \\(f_K=100\\times \\frac{ f_K}{b_K}\\) Definizione (Media Aritmetica) Consideriamo la serie dei dati \\(\\mathbf{x}=(x_1,...,x_i,...,x_n)\\), si definisce la media aritmetica: \\[ \\bar x =\\frac 1 n \\sum_{i=1}^nx_i \\] Definizione (Media Artimetica per Dati Raccolti in Classi) \\[ \\bar x =\\frac 1 n \\sum_{j=1}^K\\mathrm{x}_j n_j \\] Proposizione (della media aritmetica) Le principale proprietà della media aritmetica sono: Internalità: \\(x_{\\min} = x_{(1)} \\le \\bar{x} \\le x_{(n)} = x_{\\max}\\) Invarianza della somma: \\[n\\bar x=\\sum_{i=1}^n x_i\\] Somma degli scarti dalla media nulla: \\(\\sum_{i=1}^{n} (x_{i} - \\bar{x}) = 0\\) Minimizza la somma degli scarti al quadrato: \\[ \\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2} &lt; \\sum_{i=1}^{n} (x_{i} - d)^{2} \\quad \\forall d \\ne \\bar{x} \\] Invarianza per trasformazioni lineari: se \\(y_i=a+bx_i\\) allora \\(\\bar{y} = a + b \\bar{x}\\) Associatività. Sia una popolazione, \\(\\mathscr{P}\\), formata da \\(K\\) gruppi con medie e numerosità: (\\(\\bar{x}_{1};\\ n_{1}\\)), (\\(\\bar{x}_{2};\\ n_{2}\\)), \\(\\ldots\\), (\\(\\bar{x}_{K};\\ n_{K}\\)). Allora, la media totale \\(\\bar{x}_{T}\\) di \\(\\mathscr{P}=\\) è data da \\[ \\bar{x}_{T} = \\frac{\\mbox{Tot}{ \\{\\mathscr{P}}_1\\} + \\cdots + \\mbox{Tot}{ \\{\\mathscr{P}}_K\\}} {n_{1} + \\cdots + n_{K}} = \\frac{n_{1}\\ \\bar{x}_{1} + \\cdots + n_{K}\\ \\bar{x}_{K}} {n_{1} + \\cdots + n_{K}} \\] Definizione (Varianza) Si definisce la varianza la quantità: \\[ \\sigma^2=\\frac 1 n \\sum_{1=1}^n(x_i-\\bar x)^2 \\] Proposizione (Formula Calcolatoria della Varianza) \\[ \\sigma^2=\\frac 1 n \\sum_{1=1}^n x_i^2 -\\bar x^2 \\] Proposizione (Varianza per Dati in Distribuzione di Frequenza) \\[ \\sigma^2=\\frac 1 n\\sum_{j=1}^k(\\mathrm{x}_j-\\bar x)^2n_j \\] Proposizione (Formula Calcolatoria per la Varianza per Dati in Distribuzione di Frequenza) \\[ \\sigma^2=\\frac 1 n\\sum_{j=1}^k\\mathrm{x}_j^2n_j-\\bar x^2 \\] Proposizione (della varianza \\(\\sigma^{2}\\)) Le principale proprietà della varianza sono: \\(\\sigma^{2} \\ge 0\\). \\(\\sigma^{2}=0\\), se e solo se \\(X\\) è costante. Se \\(y_i=a+bx_i\\) allora \\(\\sigma^{2}_Y = b^{2} \\sigma^{2}_X\\). Proposizione (della deviazione standard \\(\\sigma\\)) Le principale proprietà della deviazione standard sono: \\(\\sigma\\ge 0\\). \\(\\sigma=0\\), se e solo se \\(X\\) è costante. Se \\(y_i=a+bx_i\\) allora allora \\(\\sigma_Y = |b|\\sigma_X\\) Definizione (Mediana) Si definisce la mediana calcolata su \\(n\\) \\((x_1,...,x_n)\\) dati: se \\(n\\) è dispari \\[x_{0.5}=x_{\\left(\\frac{n+1}{2}\\right)}\\] se \\(n\\) è pari \\[x_{0.5}=\\frac 1 2 \\left(x_{\\left(\\frac{n}{2}\\right)}+x_{\\left(\\frac{n}{2}+1\\right)}\\right)\\] dove \\((x_{(1)},...,x_{(n)})\\) indica la serie dei dati riordinati. Se i dati sono raccolti in classi \\[x_{0.5}=x_{\\inf;m}+\\frac{0.5-F_{m-1}}{f_m}\\cdot \\left(x_{\\sup;m}-x_{\\inf;m} \\right),\\] Proposizione (della Mediana) La mediana di una distribuzione, \\(x_{0.5}\\), è quel valore della per \\(X\\) il quale si ha \\(F(x_{0.5}) = 0.5\\). Le proprietà della mediana (\\(x_{0.5}\\)) sono: \\(x_{\\min} \\leq x_{0.5} \\leq x_{\\max}\\), \\(\\sum_{j=1}^{n} |x_{j} - x_{0.5}|\\) è un minimo. Relazione Media-Mediana: Distribuzione simmetrica \\(\\rightarrow\\) \\(x_{0.5} = \\bar{x}\\) Distribuzione con coda lunga a destra \\(\\rightarrow\\) \\(x_{0.5} &lt; \\bar{x}\\) Distribuzione con coda lunga a sinistra \\(\\rightarrow\\) \\(x_{0.5} &gt; \\bar{x}\\) Il \\(p\\)-esimo \\(0\\le p \\le 1\\) percentile di una serie di dati \\((x_1,...,x_n)\\) è \\[ x_p = x_{(\\lceil {p\\times n}\\rceil)} \\] dove \\(\\lceil x\\rceil\\) è l’operatore che estrae la parte intera di un numero decimale e la porta all’intero successivo più vicino, ad esempio \\(\\lceil 3 \\rceil = 3\\), mentre \\(\\lceil 3.001\\rceil=\\lceil 3.21\\rceil=\\lceil 3.94\\rceil=4\\). Se i dati sono raccolti in classi: \\[x_{p}=x_{\\inf;j_p}+\\frac{p-F_{j_p-1}}{f_{j_p}}\\cdot \\left(x_{\\sup;j_p}-x_{\\inf;j_p} \\right)\\] Si definisce la moda, \\(x_{Mo}\\) la modalità cui compete frequenza maggiore. Se i dati sono sono raccolti in classi, non c’è un valore modale ma una classe modale ed è la classe cui compete densità maggiore. Se la VS \\(X\\) ha una sola classe modale, allora valgono le seguenti relazioni: Se la distribuzione presenta un’asimmetria negativa (coda lunga a sx) allora \\[\\bar x\\le x_{0.5} \\le x_{mo}\\] Se la distribuzione è simmetrica allora \\[x_{mo}\\approx x_{0.5}\\approx \\bar x\\] Se la distribuzione presenta un’asimmetria positiva (coda lunga a dx) allora \\[x_{mo}\\le x_{0.5}\\le \\bar x\\] Probabilità Definizione (Unione tra Eventi) Siano \\(A\\) e \\(B\\) due eventi, l’espressione \\[ A\\cup B \\] è vera se almeno uno dei due è vero. Definizione (Intersezione tra Eventi) Siano \\(A\\) e \\(B\\) due eventi, l’espressione \\[ A\\cap B\\qquad\\text{è vera se è vero $A$ }\\textbf{e}\\text{ è vero $B$.} \\] Definizione (Evento Complementare) Sia \\(A\\) un evento, si definisce \\(\\bar A\\) l’evento complementare di \\(A\\) Definizione (Evento Certo) Sia \\(A\\) un evento, si definisce l’evento certo \\(\\Omega\\) l’evento: \\[\\begin{eqnarray*} \\Omega &amp;=&amp; A\\cup\\bar A \\end{eqnarray*}\\] Definizione (Evento Impossibile) Sia \\(A\\) un evento, si definisce l’evento certo \\(\\emptyset\\) l’evento: \\[\\begin{eqnarray*} \\emptyset &amp;=&amp; A\\cap\\bar A \\end{eqnarray*}\\] Definizione (Approccio Classico (Laplace)) la probabilità è il rapporto tra il numero dei casi favorevoli e il numero dei casi possibili, posto che gli eventi siano tutti equiprobabili. \\[ P(A)=\\frac{\\#(\\text{casi favorevoli ad $A$} )}{\\#(\\text{casi totali} )} \\] Definizione (Approccio Frequentista) Postulato empirico del caso. In un gruppo di prove ripetute più volte nelle stesse condizioni, ciascuno degli eventi possibili si presenta con una frequenza relativa che tende alla probabilità all’aumentare del numero di prove; ossia \\[ P(A)=\\frac{n_A}{n}+\\epsilon_n \\quad\\mbox{dove}\\quad \\epsilon_{n}\\to 0 \\quad\\mbox{per}\\quad n\\to\\infty. \\] La probabilità \\(P\\) è una funzione che trasforma ogni evento \\(A\\) di \\(\\mathscr{A}\\) in un numero reale \\[P:\\mathscr{A}\\to\\mathbb{R},~~\\forall A\\in\\mathscr{A}\\] Tale che \\(~~~~i.\\phantom{i}\\phantom{i}~\\) \\(P(A)\\ge 0,~\\forall A\\in\\mathscr{A}\\) \\(~~~~ii.\\phantom{i}~\\) \\(P(\\Omega)=1\\) \\(~~~~iii.~\\) \\(\\forall A,B\\in\\mathscr{A}:A\\cap B=\\emptyset, P(A\\cup B)=P(A)+P(B)\\) Proposizione (Proprietà Principali di \\(P\\)) Tra le tante enunciamo le più immediate ed utili: \\(0\\le P(A) \\le 1,~\\forall A\\in\\mathscr{A}\\) \\(P(\\emptyset)=0\\) \\(P(A)=1-P(\\bar A)\\) \\(P(A\\cap B)=P(A)-P(A\\cap \\bar B)\\) \\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\) Definizione (Eventi Incompatibili) \\(A\\) e \\(B\\) si dicono incompatibili se e solo se \\[A\\cap B = \\emptyset\\] in figura ?? una rappresentazione grafica. Definizione (Probabilità Condizionata) Si definisce probabilità di \\(A\\) condizionata a \\(B\\) (probabilità di \\(A\\) dato \\(B\\)) la quantità \\[ P(A|B)=\\frac{P(A\\cap B)}{P(B)} \\] Definizione (Indipendenza tra Eventi) Due eventi \\(A\\) e \\(B\\) si dicono indipendenti se e solo se \\[\\begin{eqnarray*} P(A|B)&amp;=&amp;P(A)\\\\ P(B|A)&amp;=&amp;P(B) \\end{eqnarray*}\\] Teorema (Probabilità Totali versione a coppie) Siano \\(A\\) e \\(B\\) due eventi diversi dal vuoto, allora \\[\\begin{eqnarray*} P(B)&amp;=&amp;P(A)P(B|A)+P(\\bar A)P(B|\\bar A) \\end{eqnarray*}\\] Teorema (Probabilità Totali versione Generale) Siano \\(\\{A_1,...,A_n\\}\\) e \\(\\{B_1,...,B_m\\}\\) due partizioni di \\(\\Omega\\), ovvero \\(A_i\\cap A_j=\\emptyset, ~\\forall i\\ne j\\) e \\(\\Omega=\\bigcup_{i=1}^n A_i\\) e \\(B_i\\cap B_j=\\emptyset, ~\\forall i\\ne j\\) e \\(\\Omega=\\bigcup_{i=1}^n B_i\\) Allora \\[ P(B_j)=\\sum_{i=1}^nP(A_i)P(B_j|A_i),\\qquad j=1,...,m \\] Teorema (Teorema di Bayes versione a coppie.) Si considerino due eventi \\(A\\) e \\(B\\) di cui sono note \\(P(A)\\), \\(P(B|A)\\) e \\(P(B|\\bar A)\\), allora \\[ P(A|B)=\\frac{P(A)P(B|A)}{P(A)P(B|A)+P(\\bar A)P(B|\\bar A)} \\] Teorema (Teorema di Bayes versione Generale) Siano \\(\\{A_1,...,A_n\\}\\) e \\(\\{B_1,...,B_n\\}\\) due partizioni di \\(\\Omega\\), di cui sono note \\(P(A_i), \\forall i\\) e \\(P(B_j|A_i), \\forall i,j\\), allora \\[ P(A_i|B_j)=\\frac{P(A_i)P(B_j|A_i)} {\\sum_{i=1}^nP(A_i)P(B|A_i)} \\] In sintesi \\[\\begin{align*} 0&amp;\\le P(A) \\le 1,~\\forall A\\in\\mathscr{A} &amp;&amp;\\text{la probabilità è compresa tra 0 e 1.} \\\\ P(\\Omega)&amp;=1 &amp;&amp;\\text{la prob. dell&#39;evento certo è 1,} \\\\ P(\\emptyset)&amp;=0 &amp;&amp;\\text{la prob. dell&#39;insieme vuoto è zero.} \\\\ P(A)&amp;=1-P(\\bar A) &amp;&amp;\\text{regola del complementare} \\\\ P(A\\cup B)&amp;=P(A)+P(B)-P(A\\cap B) &amp;&amp;\\text{regola della somma (de Morgan)} \\\\ P(A\\cup B)&amp;=P(A)+P(B) &amp;&amp;\\textbf{se e solo se A e B sono incompatibili:} \\\\ &amp; &amp;&amp;\\text{terzo assima di Kolmogorov} \\\\ P(A\\cap B)&amp;=P(A)P(B|A)=P(B)P(A|B) &amp;&amp;\\text{regola del prodotto (chain rule)} \\\\ P(A\\cap B)&amp;=P(A)P(B) &amp;&amp;\\text{se e solo se A e B sono indipendenti} \\\\ P(B)&amp;=P(A)P(B|A)+P(\\bar A)P(B|\\bar A) &amp;&amp;\\text{Teorema delle probabilità totali} \\end{align*}\\] Variabili Casuali Definizione (Supporto) Sia \\(X\\) una VC, si definisce \\(S_X\\) il supporto di \\(X\\), l’insieme di tutti i possibili valori che \\(X\\) è suscettibile di assumere. Definizione (Funzione di Probabilità) Sia \\(X\\) una VC con supporto \\(S_X\\), si definisce \\(f\\) la funzione di probabilità: è la probabilità che la VC \\(X\\) assuma esattamente il valore \\(x\\) \\[ f(x)=P(X=x), ~x\\in S_X \\] Proposizione (Funzione di probabilità) Sia \\(X\\) una VC con supporto \\(S_X\\) e funzione di probabilità \\(f\\), allora \\(0\\le f(x)\\le1,\\forall x\\in S_X\\) \\(\\sum_{x\\in S_X} f(x) = 1.\\) Definizione (Funzione di Ripartizione di una VC) \\[ F(x)=P(X\\le x)=\\sum_{x^*\\le x} f(x^*) \\] Proposizione (Funzione di Ripartizione) La funzione di ripartizione \\(F\\) di una VC \\(X\\) è, per definizione: \\[F(x)=P(X\\leq x)\\] \\(F\\) gode delle seguenti proprietà: Non decrescente, ossia \\(x_{1}&lt;x_{2} \\Rightarrow F(x_{1}) \\le F(x_{2})\\) \\(\\lim_{x\\to -\\infty} F(x) = 0\\), \\(\\lim_{x\\to\\infty} F(x) = 1\\). Continua a destra, ossia \\(\\lim_{x\\to x_{0}^{+}} F(x) = F(x_{0})\\). \\(P(a &lt; X \\le b) = F(b) - F(a)\\). Definizione (Percentie di una VC) Sia \\(X\\) una VC con support \\(S_X\\) e con Fuzione di Ripartizione \\(F\\), si definisce il \\(p\\)-esimo percentile di \\(X\\), il vaolre \\(x_p\\), tale che: \\[ x_p:F(x_p)=p \\] Definizione (Valore Atteso di una VC discreta) Si definisce \\(E(X)\\) il valore atteso della VC \\(X\\) con supporto \\(S_X\\) e funzione di probabilita \\(f\\): \\[ E(X)=\\sum_{x\\in S_x}xf(x) \\] Definizione (Varianza di una VC discreta) Si definisce \\(V(X)\\) la varianza della VC \\(X\\) con supporto \\(S_X\\) e funzione di probabilita \\(f\\): \\[\\begin{eqnarray*} V(X)&amp;=&amp;E\\left(\\big(X-E(X)\\big)^2\\right)\\\\ &amp;=&amp;\\sum_{x\\in S_x}(x-E(X))^2f(x),\\qquad\\text{oppure equivalentemente}\\\\ &amp;=&amp; E(X^2)-E^2(X)\\\\ &amp;=&amp;\\sum_{x\\in S_X}x^2f(x)-E^2(X). \\end{eqnarray*}\\] Definizione (Standard Deviation di una VC discreta) Si definisce \\(SD(X)\\) la Standard Deviation della VC \\(X\\) con supporto \\(S_X\\) e funzione di probabilita \\(f\\), la radice della sua varianza \\[\\begin{eqnarray*} SD(X)&amp;=&amp;\\sqrt{V(X)} \\end{eqnarray*}\\] Proposizione (Proprietà del Valore Atteso di una VC) Le proprietà del valore atteso, \\(E(X)\\) sono: \\(x_{\\min} \\leq E(X) \\leq x_{\\max}, \\quad x_{\\min},\\ x_{\\max}\\in S_{X}\\), \\(E\\Big(X - E(X)\\Big) = 0\\), \\(E\\Big(X - E(X)\\Big)^{2} &lt; E(X - d)^{2} \\quad\\forall\\ d \\ne E(X)\\), \\(E(a + b X) = a + b\\ E(X)\\). \\(E(aX+bY)=aE(X)+bE(Y)\\) Proposizione (Proprietà della Varianza di una VC) Le proprietà della Varianza, \\(V(X)\\) sono: \\(V(X)\\geq 0\\), \\(V(X)=0\\) se e solo se \\(P(X=x)=1\\) \\[V(a+bX)=b^2V(X)\\] Se \\(X\\) e \\(Y\\) sono indipendenti, allora \\[V(aX+bY)=V(aX-bY)=a^2V(X)+b^2V(Y), \\forall~a,b\\in\\mathbb{R}\\] Proposizione (Proprietà della SD di una VC) Le proprietà della Standard Deviation di \\(X\\), \\(SD(X)\\) sono: \\(SD(X)\\geq 0\\), \\(SD(X)=0\\) se e solo se \\(P(X=x)=1\\) \\[SD(a+bX)=|b|V(X)\\] Se \\(X\\) e \\(Y\\) sono indipendenti, allora \\[SD(aX+bY)=SD(aX-bY)=\\sqrt{a^2V(X)+b^2V(Y)}, \\forall~a,b\\in\\mathbb{R}\\] In sintesi \\[\\begin{align*} S_X&amp; &amp; &amp; \\text{il supporto della VC $X$:} \\\\ &amp; &amp; &amp; \\text{l&#39;insieme di tutti i possibili valori che la VC può assumere. } \\\\ &amp; &amp; &amp; \\text{Se $X$ è una VD discreta il suo supporto ha:} \\\\ S_X&amp;=\\{x_1,...,x_k\\} &amp; &amp; \\text{un numero finito, } \\\\ S_X&amp;=\\{x_1,x_2,x_3,...\\} &amp; &amp; \\text{o al più numerabile di elementi.} \\\\ f(x)&amp;=P(X=x),~x\\in S_X &amp; &amp; \\text{$f$ è la funzione di probabilità,} \\\\ &amp; &amp; &amp; \\text{indica la probabilità che la VC $X$ assuma esattamente il valore $x$.} \\\\ E(a+bX)&amp;=a+bE(X) &amp; &amp; \\text{linearità} \\\\ E(aX+bY)&amp;=aE(X)+bE(Y) &amp; &amp; \\\\ V(X)&amp;=E\\left(\\big(X-E(X)\\big)^2\\right) &amp; &amp; \\text{Varianza della VC $X$} \\\\ &amp;=\\sum_{x\\in S_X}x^2f(x)-E^2(X) &amp; &amp; \\\\ V(a+bX)&amp;=b^2V(X) &amp; &amp; \\\\ SD(X)&amp;=\\sqrt{V(X)} &amp; &amp; \\text{Standard Deviation della VC $X$} \\\\ SD(a+bX)&amp;=|b| SD(X) &amp; &amp; \\\\ \\text{Indipendenza tra VC}&amp; &amp; &amp; \\\\ P(X\\in A\\cap Y\\in B)&amp;=P(X\\in A)\\cdot P(Y\\in B) &amp; &amp; \\forall A \\subset S_X,\\forall B \\subset S_Y \\\\ P(X=x\\cap Y=y)&amp;=P(X=x)\\cdot P(Y=y) &amp; &amp; \\forall x \\in S_X,\\forall y \\in S_Y \\\\ V(aX+bY)&amp;=a^2V(X)+b^2V(Y) &amp; &amp; \\text{se e solo se $X$ e $Y$ sono indipendenti} \\\\ SD(aX+bY)&amp;=\\sqrt{a^2V(X)+b^2V(Y)} &amp; &amp; \\text{se e solo se $X$ e $Y$ sono indipendenti. } \\\\ &amp; &amp; &amp; \\text{n.b. la SD di una somma non } \\\\ &amp; &amp; &amp; \\text{si può esprimere con la somma delle SD.} \\end{align*}\\] Teorema del Limite Centrale Teorema (TLC per la Somma) Siano \\(X_1....,X_n\\), \\(n\\) Variabili Casuali (VC) Indipendenti e Identicamente Distribuite (IID), tali che \\(E(X_i)=\\mu\\), \\(V(X_i)=\\sigma^2\\), \\(\\forall i=1....,n\\). Posto \\[S_n=X_1+...+X_n,\\] allora \\[S_n\\operatorname*{\\sim}_{a} N\\left(n\\mu,n\\sigma^2\\right)\\] Teorema (TLC per la Media) Siano \\(X_1....,X_n\\), \\(n\\) Variabili Casuali (VC) Indipendenti e Identicamente Distribuite (IID), tali che \\(E(X_i)=\\mu\\), \\(V(X_i)=\\sigma^2\\), \\(\\forall i=1....,n\\). Posto \\[\\bar X =\\frac {S_n} n = \\frac{X_1+...+X_n}n,\\] allora \\[\\bar X\\operatorname*{\\sim}_{a} N\\left(\\mu,\\frac{\\sigma^2}n\\right)\\] Teorema (TLC per la Proporzione) Siano \\(X_1....,X_n\\), \\(n\\) Variabili Casuali (VC) Indipendenti e Identicamente Distribuite (IID), tali che \\(X_i\\sim\\text{\\rm Ber}(\\pi)\\), \\(\\forall i=1....,n\\). Posto \\[\\hat\\pi =\\frac {S_n} n = \\frac{X_1+...+X_n}n,\\] allora \\[\\hat\\pi\\operatorname*{\\sim}_{a} N\\left(\\pi,\\frac{\\pi(1-\\pi)}n\\right)\\] Statistiche Campionarie Definizione Una statistica campionaria, \\(S\\), è una funzione dei dati \\(X_1,...,X_n\\) \\[S(X_1,...,X_n)=s\\in\\mathbb{R}\\] Definizione Siano \\(Z_1,...,Z_n\\), \\(n\\) VC, IID, \\(Z_i\\sim N(0,1)\\), posto, \\[Y=Z_1^2+...+Z_n^2, \\qquad \\text{allora} \\qquad Y\\sim \\chi^2_n\\] La distribuzione della somma del quadrato di \\(n\\) normali standard è distribuita come un chi-quadro con \\(n\\) gradi di libertà La VC \\(\\chi^2_n\\) ha come supporto tutta la retta reale positiva: \\[S_Y=\\{y&gt;0\\}=\\mathbb{R}^+\\] Lo spazio dei parametri non ha interesse statistico \\[n\\in\\mathbb{Z}^+\\] per \\(n = 1\\) ha una forma iperbolica; per \\(n&gt;2\\) e a forma campanulare con un’asimmetria positiva (coda lunga a dx); in virtù del TLC se \\(n\\) diverge allora \\(Y\\stackrel{\\sim}{a}N(n,2n)\\). \\[E(Y)=n,\\qquad V(Y)=2n\\] Definizione Siano \\(Z\\sim N(0,1)\\) e \\(Y\\sim\\chi^2_n\\), \\(Z\\) e \\(Y\\) indipendenti, posto, \\[T=\\frac Z{\\sqrt{Y/n}} \\qquad \\text{allora} \\qquad T\\sim t_n\\] Il rapporto tra una normale standard e un la radice di un chi-quadro diviso per i suoi gradi di libertà è distribuito come una \\(t\\)-Student con \\(n\\) gradi di libertà La VC \\(t_n\\) ha come supporto tutta la retta reale: \\[S_T=\\mathbb{R}\\] Lo spazio dei parametri non ha interesse statistico \\[n\\in\\mathbb{Z}^+\\] Funzione di probabilista o densità. è a forma campanulare è simmetrica rispetto a zero all’aumentare di \\(n\\) le code si abbassano Se \\(n\\to\\infty\\), allora \\(t_n\\to N(0,1)\\) \\[E(Y)=0,\\qquad V(Y)=\\frac{n}{n-2}\\] Inferenza Teoria della stima Definizione Uno stimatore puntuale (point estimator) è una statistica \\(\\hat\\theta\\) che trasforma il campione \\(X_1,...,X_n\\) in un punto dello spazio dei parametri: \\[\\hat\\theta:\\mathcal{S}\\to\\Theta\\] Definizione (Correttezza di uno stimatore) Siano \\(X_1,...,X_n\\), \\(n\\) VC, IID, replicazioni della stessa \\(X\\sim\\mathscr{L}(\\theta)\\), sia \\(\\hat\\theta\\) uno stimatore per \\(\\theta\\). Lo stimatore \\(\\hat\\theta\\) si dice corretto se \\[E(\\hat\\theta(X_1,...,X_n))=E(\\hat\\theta)=\\theta\\] Definizione (Mean Squared Error di uno stimatore) Si definisce Errore Quadratico Medio (Mean Squared Error) la quantità \\[MSE(\\hat\\theta)=E((\\hat\\theta-\\theta)^2)=V(\\hat\\theta)+B^2(\\hat\\theta)\\] dove \\[B(\\hat\\theta)=|E(\\hat\\theta)-\\theta|\\] Definizione (Efficienza di uno stimatore) Siano \\(\\hat\\theta_1\\) e \\(\\hat\\theta_2\\) due stimatori per \\(\\theta\\), si dice che \\(\\hat\\theta_1\\) è più efficiente di \\(\\hat\\theta_2\\) se e solo se \\[MSE(\\hat\\theta_1)&lt;MSE(\\hat\\theta_2)\\] Definizione (Correttezza Asintotica) Lo stimatore \\(\\hat\\theta\\) si dice asintoticamente corretto se \\[\\lim_{n\\to\\infty}E(\\hat\\theta(X_1,...,X_n))=E(\\hat\\theta)=\\theta\\] Definizione (Correttezza Asintotica) Lo stimatore \\(\\hat\\theta\\) si dice consistente (in media quadratica) se e solo se \\[\\lim_{n\\to\\infty}MSE(\\hat\\theta(X_1,...,X_n))=\\lim_{n\\to\\infty}MSE(\\hat\\theta)=0\\] Essendo \\[MSE(\\hat\\theta)=V(\\hat\\theta)+B^2(\\hat\\theta)\\] allora \\[\\lim_{n\\to\\infty} MSE(\\hat\\theta)=0, \\text{ se e solo se} \\lim_{n\\to\\infty} V(\\hat\\theta)=0 \\text{ e } \\lim_{n\\to\\infty} B^2(\\hat\\theta)=0\\] La standard deviation (SD) \\(\\sigma\\), rappresenta la dispersione degli individui dalla media, è un indicatore di variabilità della popolazione, per esempio in una popolazione finita di \\(N\\) individui: \\[\\sigma=\\sqrt{\\sigma^2}=\\sqrt{\\frac 1 N\\sum_{i=1}^N(x_i-\\mu)^2},\\] la deviazione standard \\(\\sigma\\) è la radice della varianza della popolazione \\(\\sigma^2\\). Lo standard error \\(SE(\\hat\\theta)\\) di uno stimatore \\(\\hat\\theta\\) per \\(\\theta\\) è un indicatore della variabilità dello stimatore nello spazio dei parametri \\[SE(\\hat\\theta)=\\sqrt{V(\\hat\\theta)}\\] Lo standard error \\(SE(\\hat\\theta)\\) di uno stimatore \\(\\hat\\theta\\) per \\(\\theta\\) è la radice della varianza della VC \\(\\hat\\theta\\). La standard deviation stimata \\(\\sigma\\), rappresenta la dispersione degli individui del campione dalla media del campione, è un indicatore di variabilità del campione: \\[\\hat\\sigma=\\sqrt{\\hat\\sigma^2}=\\sqrt{\\frac 1 n\\sum_{i=1}^n(x_i-\\hat\\mu)^2}\\] La deviazione standard stimata \\(\\hat\\sigma\\) è la radice della varianza del campione \\(\\hat\\sigma^2\\). Definizione (Funzione di Verosimiglianza) Siano \\(x_1,...,x_n\\) \\(n\\) osservazioni di \\(X\\sim \\mathscr{L}(\\theta)\\), \\(\\theta\\in\\Theta\\), si definisce la verosimiglianza \\(L\\) di \\(\\theta\\) la funzione: \\[L(\\theta;x_1,...,x_n)=L(\\theta)\\propto P(X_1=x_1,...,X_n=x_n;\\theta)\\] Definizione (Log Verosimiglianza) Si definisce la log-verosimiglianza \\(\\ell\\): \\[\\begin{eqnarray*} \\ell(\\theta) &amp;=&amp; \\log L(\\theta) \\\\ &amp;=&amp; \\log \\prod_{i=1}^n f(x_i;\\theta)\\\\ &amp;=&amp; \\sum_{i=1}^n \\log f(x_i;\\theta) \\end{eqnarray*}\\] Definizione (Stimatore du Massima Verosimiglianza) Lo stimatore di massima verosimiglianza per \\(\\theta\\) è \\[\\begin{eqnarray*} \\hat\\theta &amp;=&amp; \\operatorname*{\\text{argmax}}_{\\theta\\in\\Theta} L(\\theta)\\\\ &amp;=&amp; \\operatorname*{\\text{argmax}}_{\\theta\\in\\Theta} \\ell(\\theta) \\end{eqnarray*}\\] \\[\\hat\\theta:L(\\hat\\theta)&gt;L(\\theta), \\forall\\theta\\neq\\hat\\theta, \\qquad\\ell(\\hat\\theta)&gt;\\ell(\\theta), \\forall\\theta\\neq\\hat\\theta\\] Siano \\(X_1,...X_n\\) \\(n\\) VC IID, tali che \\(X_i\\sim\\text{Ber}(\\pi)\\) lo stimatore di massima verosimiglianza per \\(\\pi\\) è \\[\\hat \\pi=\\frac 1n \\sum_{i=1}^nX_i\\] \\(\\hat\\pi\\) è corretto per \\(\\pi\\), infatti \\[E(\\hat\\pi)=E\\left(\\frac{1}n\\sum_{i=1}^n X_i\\right)=\\frac{1}n\\sum_{i=1}^nE(X_i)=\\frac{\\pi+...+\\pi}{n}=\\frac n n\\pi=\\pi\\] E quindi \\[MSE(\\hat\\pi)=V(\\hat\\pi)=\\frac{\\pi(1-\\pi)}{n}\\] che è ancora funzione di \\(\\pi\\). Lo stimatore \\(\\hat\\pi\\) per \\(\\pi\\) è consistente, infatti \\[\\lim_{n\\to +\\infty}MSE(\\hat\\pi)=\\lim_{n\\to +\\infty}\\frac{\\pi(1-\\pi)}{n}=0\\] \\(\\hat\\pi\\) è corretto e consistente per \\(\\pi\\). \\[SE(\\hat\\pi)=\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\] L’errore di stima si stima sostituendo a \\(\\pi\\) la sua stima \\(\\hat\\pi\\) \\[\\widehat{SE(\\hat\\pi)}=\\sqrt{\\frac{\\hat\\pi(1-\\hat\\pi)}{n}}\\] Siano \\(X_1,...X_n\\) \\(n\\) VC IID, tali che \\(X_i\\sim\\text{Pois}(\\lambda)\\) lo stimatore di massima verosimiglianza per \\(\\lambda\\) è \\[\\hat \\lambda=\\frac 1n \\sum_{i=1}^nX_i\\] Correttezza: \\[ E(\\hat\\lambda) = E\\left(\\frac{1}n\\sum_{i=1}^n X_i\\right) = \\frac 1 n \\sum_{i=1}^n E(X_i) = \\frac 1 n \\sum_{i=1}^n \\lambda = \\lambda \\] Mean Squared Error: \\[ MSE(\\hat\\lambda) = V(\\hat\\lambda) = V\\left(\\frac 1 n \\sum_{i=1}^n X_i\\right) = \\frac 1 {n^2} \\sum_{i=1}^n V(X_i) = \\frac n {n^2} \\lambda = \\frac {\\lambda}n \\] Consistenza: \\[ \\lim_{n\\to+\\infty} MSE(\\hat\\lambda) = \\lim_{n\\to+\\infty} \\frac {\\lambda}n = 0 \\] Standard Error \\[SE(\\hat\\lambda)=\\sqrt{\\frac {\\lambda}n}\\] Standard Error stimato \\[\\widehat{SE(\\hat\\lambda)}=\\sqrt{\\frac {\\hat\\lambda}n}\\] La verosimiglianza per \\((\\mu,\\sigma^2)\\) è \\[\\begin{eqnarray*} L(\\lambda) &amp;=&amp; \\prod_{i=1}^n f(x_i;\\mu,\\sigma^2) \\end{eqnarray*}\\] Proposizione \\[\\begin{eqnarray*} \\hat\\mu &amp;=&amp; \\frac 1 n \\sum_{i=1}^n x_i\\\\ \\hat\\sigma^2 &amp;=&amp; \\frac 1 n \\sum_{i=1}^n(x_i-\\hat\\mu)^2\\\\ &amp;=&amp; \\frac 1 n \\sum_{i=1}^n x_i^2 -\\hat\\mu^2 \\end{eqnarray*}\\] Correttezza per \\(\\mu\\): \\[ E(\\hat\\mu) = E\\left(\\frac{1}n\\sum_{i=1}^n X_i\\right) = \\frac 1 n \\sum_{i=1}^n E(X_i) = \\frac 1 n \\sum_{i=1}^n \\mu = \\mu \\] Mean Squared Error per \\(\\mu\\): \\[ MSE(\\hat\\mu) = V(\\hat\\mu) = V\\left(\\frac 1 n \\sum_{i=1}^n X_i\\right) = \\frac 1 {n^2} \\sum_{i=1}^n V(X_i) = \\frac n {n^2} \\sigma^2 = \\frac {\\sigma^2}n \\] Consistenza per \\(\\mu\\): \\[ \\lim_{n\\to+\\infty} MSE(\\hat\\mu) = \\lim_{n\\to+\\infty} \\frac {\\sigma^2}n = 0 \\] E lo Standard Error: \\[SE(\\hat\\mu)=\\sqrt{\\frac {\\sigma^2}n}\\] Correttezza per \\(\\hat\\sigma^2\\): \\[ E(\\hat\\sigma^2) = \\frac {n-1}{n}\\sigma^2 \\] \\(\\hat\\sigma^2\\) non è stimatore corretto per \\(\\sigma^2\\). Correzione di \\(\\hat\\sigma^2\\) \\[ S^2=\\frac{n}{n-1}\\hat\\sigma^2=\\frac{n}{n-1}\\frac{1}n\\sum_{i=1}^n(X_i-\\hat\\mu)^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\hat\\mu)^2 \\] Standard Error \\[SE(\\hat\\mu)=\\sqrt{\\frac {\\sigma^2}n}\\] Standard Error stimato. \\[\\widehat{SE(\\hat\\mu)}=\\sqrt{\\frac {S^2}n}=\\sqrt{\\frac {\\hat\\sigma^2}{n-1}}\\] Proprietà degli stimatori di massima verosimiglianza. Siano \\(X_1,...,X_n\\) \\(n\\) VC IID, replicazioni di \\(X\\sim \\mathscr{L}(\\theta)\\) e sia \\(\\hat\\theta\\) lo stimatore di massima verosimiglianza per per \\(\\theta\\), allora \\(\\hat\\theta\\) non è sempre stimatore corretto ma è sempre corretto asintoticamente: \\[E(\\hat\\theta)\\xrightarrow{n\\to\\infty}\\theta\\] \\(\\hat\\theta\\) non è sempre stimatore a massima efficienza ma lo è sempre asintoticamente: \\[V(\\hat\\theta)\\xrightarrow{n\\to\\infty}I^{-1}(\\theta)\\] dove \\(I(\\theta)\\) è l’infromazione di Fisher. \\(\\hat\\theta\\) è asintoticamente distribuito normalmente \\[\\hat\\theta\\operatorname*{\\sim}_a N(\\theta,I^{-1}(\\theta))\\] Lo stimatore di massima verosimiglianza è invariante alle trasformazioni monotone invertibili \\(g\\): \\[ \\text{se } \\psi=g(\\theta), \\text{ allora } \\hat\\psi = g(\\hat\\psi)\\] Definizione (Intervallo di confidenza) Un intervallo di confidenza per \\(\\theta\\) al livello \\((1-\\alpha)\\times 100\\%\\) è una coppia di statistiche \\(L_1\\) e \\(L_2\\) tali che \\[P(L_1&lt;\\theta&lt;L_2)=1-\\alpha\\] Proposizione (Intervallo di Confidenza per \\(\\mu\\) (\\(\\sigma^2\\) nota)) Si definisce L’IdC al livello \\((1-\\alpha)\\times100\\%\\) per \\(\\mu\\) con \\(\\sigma^2\\) nota, l’intervallo \\[IdC:~~\\left[\\hat \\mu- z_{\\alpha/2}~\\frac\\sigma{\\sqrt n},\\hat \\mu+ z_{\\alpha/2}~\\frac\\sigma{\\sqrt n}\\right]\\] Proposizione (Intervallo di Confidenza per \\(\\mu\\) (\\(\\sigma^2\\) incognita)) Si definisce L’IdC al livello \\((1-\\alpha)\\times100\\%\\) per \\(\\mu\\) con \\(\\sigma^2\\) incognita, l’intervallo \\[IdC:~~\\left[\\hat \\mu- t_{n-1;\\alpha/2}~\\frac S{\\sqrt n},\\hat \\mu+ t_{n-1;\\alpha/2}~\\frac S{\\sqrt n}\\right]\\] Proposizione (Intervallo di Confidenza per \\(\\pi\\)) Si definisce L’IdC al livello \\((1-\\alpha)\\times100\\%\\) per \\(\\pi\\) l’intervallo \\[\\left[\\,\\hat\\pi-z_{\\alpha/2}\\sqrt\\frac{\\hat\\pi(1-\\hat\\pi)}{n};\\hat\\pi+z_{\\alpha/2}\\sqrt\\frac{\\hat\\pi(1-\\hat\\pi)}{n}\\,\\right]\\] Teoria dei Test Definizione (Errori di primo e secondo tipo) Si definiscono L’errore di primo tipo è l’errore che si commette scegliendo \\(H_1\\) quando è vera \\(H_0\\). L’errore di secondo tipo è l’errore che si commette scegliendo \\(H_0\\) quando è vera \\(H_1\\). Tavola della verità Decisione decido \\(H_0\\) decido \\(H_1\\) stato di natura \\(H_0\\) Corretta Errore I tipo stato di natura \\(H_1\\) Errore II tipo Corretta Definizione (Probabilità degli Errori di primo e secondo tipo) \\[\\alpha=P(\\text{Errore I tipo})=P(\\text{Decidere $H_1$};H_0)=P(X_1,...,X_n\\in\\mathcal{S}_1;H_0)\\] \\[\\beta=P(\\text{Errore II tipo})=P(\\text{Decidere $H_0$};H_1)=P(X_1,...,X_n\\in\\mathcal{S}_0;H_1)\\] \\(\\alpha\\) è il livello di significatività del test, \\(\\alpha\\) è la probabilità di scegliere \\(H_1\\) quando invece è vera \\(H_0\\). \\(\\beta\\) è la probabilità di scegliere \\(H_0\\) quando invece è vera \\(H_1\\). Definizione (Potenza di un Test) \\[1-\\beta =P(\\text{Decidere $H_1$}; H_1)=P(X_1,...,X_n\\in\\mathcal{S}_1;H_1)\\] \\(1-\\beta\\) è la potenza del test, \\(1-\\beta\\) è la probabilità di scegliere \\(H_1\\) quando \\(H_1\\) è vera. Definizione (Significatività osservata \\(p_\\text{value}\\)) Si definisce la significatività osservata di un test la probabilità di incontrare un campione ancora più in favore di \\(H_1\\) di quello di cui disponiamo \\[ p_\\text{value}= \\begin{cases} P(T&gt;t_\\text{obs};H_0) &amp; \\text{Se $H_1$ unilaterle destra}\\\\ P(T&lt;t_\\text{obs};H_0) &amp; \\text{Se $H_1$ unilaterle sinistra}\\\\ 2P(T&gt;|t_\\text{obs}|;H_0) &amp; \\text{Se $H_1$ unilaterle bilaterale}\\\\ \\end{cases} \\] Test per \\(\\mu\\) e \\(\\pi\\) \\[\\begin{align*} \\hline H_0:\\mu&amp;=\\mu_0 &amp;&amp;\\text{$\\sigma^2$} &amp;\\text{Dist.} &amp; &amp;&amp; \\text{Statistica Test} &amp;&amp; \\text{Zona Rifiuto} &amp;&amp;p_\\text{value}\\\\ \\hline\\\\ H_1:\\mu&amp;&gt;\\mu_0 &amp; \\text{Noto} &amp;&amp; Z &amp;&amp; z_\\text{obs}&amp;=\\frac{\\hat \\mu-\\mu_0}{\\sigma/\\sqrt{n}} &amp; z_\\text{obs}&amp;&gt;z_\\alpha &amp; p_\\text{value}&amp;= P(Z&gt;z_\\text{obs})\\\\ H_1:\\mu&amp;&lt;\\mu_0 &amp; \\text{Noto} &amp;&amp; Z &amp;&amp; z_\\text{obs}&amp;=\\frac{\\hat \\mu-\\mu_0}{\\sigma/\\sqrt{n}} &amp; z_\\text{obs}&amp;&lt;-z_\\alpha&amp; p_\\text{value}&amp;= P(Z&lt;z_\\text{obs})\\\\ H_1:\\mu&amp;\\ne\\mu_0 &amp; \\text{Noto} &amp;&amp; Z &amp;&amp; z_\\text{obs}&amp;=\\frac{\\hat \\mu-\\mu_0}{\\sigma/\\sqrt{n}} &amp; |z_\\text{obs}|&amp;&gt;|z_{\\alpha/2}| &amp; p_\\text{value}&amp;= 2P(Z&gt;|z_\\text{obs}|)\\\\ H_1:\\mu&amp;&gt;\\mu_0 &amp; \\text{Incognito} &amp;&amp; t_{n-1} &amp;&amp; z_\\text{obs}&amp;=\\frac{\\hat \\mu-\\mu_0}{S/\\sqrt{n}} &amp; t_\\text{obs}&amp;&gt;t_{n-1;~\\alpha}&amp; p_\\text{value}&amp;= P(T&gt;t_\\text{obs})\\\\ H_1:\\mu&amp;&lt;\\mu_0 &amp; \\text{Incognito} &amp;&amp; t_{n-1} &amp;&amp; z_\\text{obs}&amp;=\\frac{\\hat \\mu-\\mu_0}{S/\\sqrt{n}} &amp; t_\\text{obs}&amp;&lt;-t_{n-1;~\\alpha}&amp; p_\\text{value}&amp;= P(T&lt;t_\\text{obs})\\\\ H_1:\\mu&amp;\\ne\\mu_0 &amp; \\text{Incognito} &amp;&amp; t_{n-1} &amp;&amp; z_\\text{obs}&amp;=\\frac{\\hat \\mu-\\mu_0}{S/\\sqrt{n}} &amp; |t_\\text{obs}|&amp;&gt;|t_{n-1;~\\alpha/2}|&amp; p_\\text{value}&amp;= 2P(T&gt;|t_\\text{obs}|)\\\\ \\hline H_0:\\pi&amp;=\\pi_0 &amp;&amp; &amp;\\text{Dist.} &amp;&amp; &amp;\\text{Statistica Test} &amp;&amp; \\text{Zona Rifiuto} &amp;&amp;p_\\text{value}\\\\ \\hline\\\\ H_1:\\pi&amp;&gt;\\pi_0 &amp; &amp;&amp; Z &amp;&amp; z_\\text{obs}&amp;=\\frac{\\hat \\pi-\\pi_0}{\\sqrt{\\pi(/1-\\pi)}/\\sqrt{n}} &amp; z_\\text{obs}&amp;&gt;z_\\alpha&amp; p_\\text{value}&amp;= P(Z&gt;z_\\text{obs})\\\\ H_1:\\pi&amp;&lt;\\pi_0 &amp; &amp;&amp; Z &amp;&amp; z_\\text{obs}&amp;=\\frac{\\hat \\pi-\\pi_0}{\\sqrt{\\pi(/1-\\pi)}/\\sqrt{n}} &amp; z_\\text{obs}&amp;&lt;-z_\\alpha&amp; p_\\text{value}&amp;= P(Z&lt;z_\\text{obs})\\\\ H_1:\\pi&amp;\\ne\\pi_0 &amp; &amp;&amp; Z &amp;&amp; z_\\text{obs}&amp;=\\frac{\\hat \\pi-\\pi_0}{\\sqrt{\\pi(/1-\\pi)}/\\sqrt{n}} &amp; |z_\\text{obs}|&amp;&gt;|z_{\\alpha/2}| &amp; p_\\text{value}&amp;= 2P(Z&gt;|z_\\text{obs}|)\\\\ \\hline \\end{align*}\\] \\[\\begin{align*} \\text{Test $t$, 2 Campioni} &amp; \\qquad\\text{ Test $t$, 2 Campioni} &amp; \\text{Proporzione, 2 Campioni} \\\\ \\text{Omogeneità} &amp; \\qquad \\text{Eterogeneità} &amp; \\\\ \\hline \\begin{aligned} t_{\\text{obs}}&amp;=\\frac{\\hat\\mu_A-\\hat\\mu_B}{\\sqrt{\\frac{S_p^2}{n_A}+\\frac{S_p^2}{n_B}}}\\\\ S^2_p &amp;= \\frac{n_A\\hat\\sigma_A^2+n_B\\hat\\sigma_B^2}{n_A+n_B-2}\\\\ \\end{aligned} &amp; \\begin{aligned} \\displaystyle \\quad t_{\\text{obs}}=\\frac{\\hat\\mu_A-\\hat\\mu_B}{\\sqrt{\\frac{S_A^2}{n_A}+\\frac{S_B^2}{n_B}}}\\\\ \\end{aligned} &amp; \\begin{aligned} z_{\\text{obs}} &amp;=\\frac{\\hat\\pi_A-\\hat\\pi_B}{\\sqrt{\\frac{\\hat\\pi_C(1-\\hat\\pi_C)}{n_A}+\\frac{\\hat\\pi_C(1-\\hat\\pi_C)}{n_B}}}\\\\ \\hat\\pi_C &amp;=\\frac{\\#\\{\\text{successi A}\\}+\\#\\{\\text{successi B}\\}}{n_A+n_B}\\\\ &amp;=\\frac{n_A\\hat\\pi_A+n_B\\hat\\pi_B}{n_A+n_B} \\end{aligned} \\\\ \\hline \\end{align*}\\] Regressione Assunti del modello di regressione lineare Dati \\((x_1,y_1),...,(x_n,y_n)\\), \\(n\\) coppie di punti, si assume che \\[y_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\\] Il valore atteso dell’errore è nullo \\[E(\\varepsilon_i)=0\\] Omoschedasticità \\[V(\\varepsilon_{i}) = \\sigma_\\varepsilon^2,\\qquad \\text{costante }\\forall i\\] Indipendenza dei residui \\[\\varepsilon_i\\text{ è indipendente da }\\varepsilon_j~~\\forall i\\neq j\\] Indipendenza tra i residui e la \\(X\\) \\[X_i\\text{ è indipendente da }\\varepsilon_i~~\\forall i\\] Esogeneità della \\(X\\): la distribuzione su \\(X\\) non è oggetto di inferenza Normalità dei residui \\[\\varepsilon_i\\sim N(0,\\sigma^2_\\varepsilon)\\] Proposizione (Stimatori dei Minimi Quadrati) Gli stimatori dei minimi quadrati \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) sono \\[\\begin{eqnarray*} \\hat\\beta_1 &amp;=&amp; \\frac{\\frac 1 n\\sum_{i=1}^n{(x_i-\\bar x)(y_i-\\bar y)}}{\\frac 1 n\\sum_{i=1}^n(x_i-\\bar x)^2}=\\frac{\\text{ cov}(x,y)}{\\hat\\sigma^2_X}\\\\ \\hat\\beta_0 &amp;=&amp;\\bar y -\\hat\\beta_1\\bar x \\end{eqnarray*}\\] dove \\[\\bar y = \\frac 1 n\\sum_{i=1}^n y_i,\\qquad \\bar x=\\frac 1 n \\sum_{i=1}^n x_i\\] e \\[ \\text{ cov}(x,y) = \\frac 1 n\\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y) \\] Definizione La Covarianza \\(\\text{cov}(x,y)\\) tra due variabili \\(x\\) e \\(y\\) è una misura della loro covariazione \\[\\text{cov}(x,y)=\\frac 1 n\\sum_{i=1}^n{(x_i-\\bar x)(y_i-\\bar y)}\\] Proposizione \\[\\text{cov}(x,y)=\\frac 1 n\\sum_{i=1}^nx_i~y_i-\\bar x\\bar y\\] Proprietà dei minimi quadrati \\[ \\begin{aligned} y_i, &amp; &amp; \\text{le $y$ osservate}\\\\ \\hat y_i &amp;= \\hat \\beta_0+\\hat\\beta_1x_i,&amp;\\text{le $y$ stimate}\\\\ \\hat\\varepsilon_i &amp;= y_i-\\hat y_i,&amp;\\text{gli errori stimati}\\\\ \\bar y &amp;= \\frac 1 n\\sum_{i=1}^n y_i, &amp;\\text{la media degli $y$}\\\\ \\bar y &amp;= \\frac 1 n\\sum_{i=1}^n \\hat y_i, &amp;\\text{la media degli $\\hat y$ coince con qeulla degli $y$}\\\\ 0 &amp;=\\frac 1 n\\sum_{i=1}^n\\hat\\varepsilon_i , &amp;\\text{la media degli scarti dalla retta è zero} \\end{aligned} \\] Definizione Il coefficiente \\(r\\) \\[r=\\frac{\\text{cov}(x,y)}{\\hat\\sigma_X\\hat\\sigma_Y}\\] è chiamato coefficiente di correlazione. Proprietà di r \\(-1 \\le r \\le 1\\). Il segno indica la direzione della relazione; \\(r&gt;0\\), al crescere di \\(X\\), in media, cresce \\(Y\\); \\(r&lt;0\\), al crescere di \\(X\\), in media, decresce \\(Y\\); \\(r=1\\), associazione perfetta diretta; \\(r=-1\\), associazione perfetta indiretta. \\(r\\) è un numero puro, ovvero è privo di unità di misura è simmetrico: \\(r_{XY} = r_{YX} = r\\) è invariante per cambiamenti di scala: Se \\(W=a+bY\\), allora \\[ r_{XW}=\\text{sign}(b) r_{XY} \\] dove \\[ \\text{sign}(b)=\\begin{cases}+1, &amp;\\text{se $b&gt;0$}\\\\ -1, &amp;\\text{se $b&lt;0$} \\end{cases} \\] \\(r\\) misura l’associazione lineare: \\(r\\) misura come i punti si addensano intorno alla retta. \\(f(x)\\) non lineare \\(r\\) è parzialmente inutile il valore di \\(r\\), da solo, non è in grado di descrivere tutte le possibili relazioni che si possono realizzare tra due variabili. \\(r\\) è più elevato se i dati sono aggregati in medie o percentuali Proposizione Vale la seguente relazione \\[TSS = ESS + RSS\\] Scomposizione della varianza \\[ \\left\\{\\begin{array}{cc} \\text{varibilità di $y$}\\\\ \\text{intorno alla sua media} \\end{array}\\right\\} = \\left\\{\\begin{array}{cc} \\text{varibilità della retta}\\\\ \\text{intorno alla media} \\end{array}\\right\\} + \\left\\{\\begin{array}{cc} \\text{varibilità delle $y$}\\\\ \\text{intorno alla retta} \\end{array} \\right\\} \\] Definizione (Indice di Determinazione Lineare) Si definisce \\[R^2=\\left(\\frac{ESS}{TSS}\\right)=r^2=\\left(\\frac{\\text{cov}(x,y)}{\\hat\\sigma_x\\hat\\sigma_y}\\right)^2\\] l’indice di determinazione lineare ed è, nel contesto della regressione lineare semplice, il quadrato dell’indice di correlazione \\[0\\leq R^2\\leq 1\\] Teorema (Gauss-Markov) Sotto gli assunti dallo 0 al 5, gli stimatori dei minimi quadrati sono corretti \\[E(\\hat\\beta_1)=\\beta_1,\\qquad E(\\hat\\beta_0)=\\beta_0\\] La loro varianza è: \\[\\begin{eqnarray*} V(\\hat\\beta_{1}) &amp;=&amp; \\frac{\\sigma_{\\varepsilon}^{2}} {n \\hat{\\sigma}^{2}_{X}} \\\\ V(\\hat\\beta_{0}) &amp;=&amp; \\sigma_{\\varepsilon}^{2} \\left( \\frac{1} {n} + \\frac{\\bar{x}^{2}} {n \\hat{\\sigma}^{2}_{X}} \\right) \\\\ \\mbox{cov}(\\hat\\beta_{0}, \\hat\\beta_{1}) &amp;=&amp; - \\sigma_{\\varepsilon}^{2} \\frac{\\bar{x}} {n \\hat{\\sigma}^{2}_{X}} = - \\bar{x} V(\\hat\\beta_{1}) \\end{eqnarray*}\\] Gli stimatori \\(\\hat\\beta_{0}\\) e \\(\\hat\\beta_{1}\\) di \\(\\beta_{0}\\) e \\(\\beta_{1}\\) sono BLUE (Best Linear Unbiased Estimators). \\[\\begin{eqnarray*} \\widehat{SE(\\hat\\beta_{0})} &amp;=&amp; \\sqrt{S_{\\varepsilon}^{2} \\left( \\frac{1} {n} + \\frac{\\bar{x}^{2}} {n \\hat{\\sigma}^{2}_{X}} \\right)}\\\\ \\widehat{SE(\\hat\\beta_{1})} &amp;=&amp; \\sqrt{\\frac{S_{\\varepsilon}^{2}} {n\\hat{\\sigma}^{2}_{X}} }\\\\ \\widehat{SE(\\widehat{Y}_{X=x})}&amp;=&amp; \\sqrt{S_{\\varepsilon}^{2}\\left( \\frac{1} {n} + \\frac{(x - \\bar{x})^{2}} {n\\hat{\\sigma}^{2}_{X}} \\right)} \\end{eqnarray*}\\] \\[\\hat\\beta_1\\sim N(\\beta_1,V(\\hat\\beta_1)), ~~\\hat\\beta_0\\sim N(\\beta_0,V(\\hat\\beta_0)), ~~\\hat Y_{(X=x)}\\sim N\\left(\\beta_0+\\beta_1 x,V(\\hat Y_{(X=x)})\\right)\\] Parliamo di interpolazione dei punti se \\(\\hat Y_{(X=x)}\\) è calcolato per \\[\\min\\{x_i\\}\\leq x \\leq\\max\\{x_i\\}\\] Parliamo di estrapolazione dei punti se \\(\\hat Y_{(X=x)}\\) è calcolato per \\[x&lt;\\min\\{x_i\\}~~~\\text{oppure}~~~ x &gt;\\max\\{x_i\\}\\] Sotto \\(H_0\\) \\[\\begin{eqnarray*} \\frac{\\hat\\beta_0-\\beta_{0,H_0}}{\\widehat{SE(\\hat\\beta_0)}} &amp;\\sim&amp; t_{n-2} \\\\ \\frac{\\hat\\beta_1-\\beta_{1,H_0}}{\\widehat{SE(\\hat\\beta_1)}} &amp;\\sim&amp; t_{n-2} \\end{eqnarray*}\\] otteniamo \\[\\begin{eqnarray*} t_{0,\\text{obs}} &amp;=&amp; \\frac{\\hat\\beta_0-\\beta_{0,H_0}}{\\widehat{SE(\\hat\\beta_0)}} \\\\ t_{1,\\text{obs}} &amp;=&amp; \\frac{\\hat\\beta_1-\\beta_{1,H_0}}{\\widehat{SE(\\hat\\beta_1)}} \\end{eqnarray*}\\] Che andranno lette nella direzione di \\(H_1\\) con le solite regole "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
