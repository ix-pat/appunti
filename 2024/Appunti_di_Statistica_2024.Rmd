--- 
title: "Appunti di Statistica"
subtitle: "CLEAM AA 24/25"
author: "Patrizio Frederic"
date: "Aggiornato al `r format(Sys.Date(),'%d-%m-%Y')`"
documentclass: book
site: bookdown::bookdown_site
biblio-style: apalike
link-citations: yes
description: "Appunti sparsi"
# bookdown::epub_book:
#   pandoc_args: "--mathml"
geometry: "left=2.5cm, right=2.5cm, top=4.5cm, bottom=4.5cm, showframe=false, showcrop=true"
fontsize: 11pt
output:
   bookdown::pdf_book:
     keep_tex: true
     highlight: default
     fig_caption: true
     # fig_width: 4
     # fig_height: 3
   bookdown::gitbook:
     fig_caption: true
     # fig_width: 12
     # fig_height: 8
#documentclass: book
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}


source("intro.R")
opts_knit$set(echo=FALSE)

```


```{r,results='asis', echo=FALSE}
if (html) {
  cat("# Appunti di Statistica {-}\n\n")
  include_graphics("img/titolo.png",dpi=30)
}
```

```{r,results='asis', echo=FALSE}

if (html) cat("
<style>
/* Selettore per l'ambiente theorem */
.example {
  /* Stili CSS per l'ambiente theorem */
  background-color: #F0F0F0; /* Colore di sfondo desiderato */
  border: 1px solid #CCCCCC; /* Colore del bordo desiderato */
  padding: 10px; /* Spaziatura interna desiderata */
}
/* Selettore per il nome del teorema */
.example .name {
  /* Stili CSS per il nome del teorema */
  color: blue; /* Colore del nome del teorema */
}
/* Selettore per l'enunciato del teorema */
.example .example-body {
  /* Stili CSS per l'enunciato del teorema */
  color: red; /* Colore dell'enunciato del teorema */
}
</style>"
)
```


<!--chapter:end:index.Rmd-->


# Avvertenza {-}

\large

Questo lavoro è un work in progress, questa non è la versione definitiva, sconsiglio di stampare tutto. 

\normalsize


Appunti di Statistica © 2023 di Patrizio Frederic è distribuito
sotto licenza CC BY-NC-ND 4.0
https://creativecommons.org/licenses/by-nc-nd/4.0/

You are free to:
Share — copy and redistribute the material in any medium or format
The licensor cannot revoke these freedoms as long as you follow the license terms.
Under the following terms:
Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.

NonCommercial — You may not use the material for commercial purposes.

NoDerivatives — If you remix, transform, or build upon the material, you may not distribute the modified material.

No additional restrictions — You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.




# Introduzione {-}

Durante il lock down della prima ondata, nel marzo del 2020, ho iniziato a trascrivere
i miei appunti, raccolti negli ultimi 17 anni, in diapositive. Questa è una rielaborazione
di quelle diapositive a cui è stato aggiunto parte dell'eserciziario realizzato con Michele Lalla. 
L'esposizione degli argomenti ricalca il corso di Statistica che con Michele Lalla abbiamo 
progettato per i corsi di CLEF e CLEA nel periodo che va dal 2008 al 2019, al quale ho 
portato alcune modifiche di forma e qualche aggiunta.

<!-- Scrivono Agresti, e Franklin, (2007), nel loro celebre libro: -->

<!-- >"Statistics is the art and science of designing studies and analyzing the data that those studies produce. Its ultimate goal is translating data into knowledge and understanding of the world around us. In short, **statistics is the art and science of learning from data**". -->

<!-- > La statistica è l'arte e la scienza di pianificare la raccolta e l'analisi dei dati che tali ricerche producono. Il suo fine è di trasformare i dati in conoscenza e comprensione del mondo circostante. In sintesi: **La statistica è l'arte e la scienza di imparare dai dati**. -->

<!-- > *Agresti, A., and Franklin, C. (2007), Statistics: the Art and Science of Learning from Data, Upper Saddle River, Pearson Prentice Hall.* -->

<!-- La statistica è dunque quelle disciplina che si occupa della progettazione, raccolta, analisi, sintesi, reportistica, e molto altro di collezioni di dati. -->
<!-- In latino *datum* significa un fatto, quindi i *dati* sono collezioni di fatti. I dati sono osservazioni e misure di un fenomeno reale. Per esempio il numero di ricoverati a marzo del 2023 al reparto di pneumologia dell'ospedale di Baggiovara è un dato, i numeri dei ricoveri di ogni mese del 2023 sono i _dati dei ricoveri mensili_. Se ad ogni ricovero aggiungiamo il numero di notti di permanenza, avremo dati più complessi. La retribuzione di un operaio della provincia di modena è un dato, l'insieme delle retribuzioni di tutti operai della provincia sono _i dati sulle retribuzioni_, se alla retribuzione aggiungiamo il settore industriale avremo dati più ricchi di informazione. Il prezzo di un titolo in borsa alle 11 del 12 giugno 2024 è un singolo dato, la serie dei prezzi di quel titolo dal primo gennaio del 2024 è _la serie storica_ di quel titoli (dati osservati nel tempo), mentre i prezzi di tutti gli altri titoli di borsa al 12 giugno 2024 rappresentano _i dati_ della borsa in quel giorno, se da ultimo collezioniamo tutti i prezzi di tutti i titoli dal primo gennaio del 2023 fino ad oggi otterremo _i dati_ della borsa dal 01/01/2023. -->




Questo manoscritto è diviso in tre parti, la prima dedicata alla statistica descrittiva nella quale 
vengono date le nozioni di base di descrizione di una popolazione, le sue rappresentazioni grafiche,
la media e la variabilità. Nella seconda parte verranno date le nozioni di base di teoria e calcolo delle probabilità.
Dal concetto di evento fino alle principali variabili casuali, il teorema del limite centrale e la distribuzione
delle principali statistiche campionarie. Nella terza e ultima parte verranno sviluppati 
i principi di base di inferenza classica, dalla stima puntuale alla stima intervallare, la teoria dei test, 
il modello di regressione lineare e il test del chi-quadro.

Questo materiale, che non è ancora un libro, è da considerarsi il canovaccio d'appunti sul quale 
costruisco le mie lezioni.  Non è da ritenersi alternativo alle lezioni in aula o ad un libro di testo. 
È ancora incompleto e disseminato di errori e imprecisioni.

Un ringraziamento speciale va alle mie donne e a Michele Lalla, amico, collega e maestro.

Bologna, il `r format(Sys.Date(),"%d-%m-%Y")`.

<!--chapter:end:00-intro.Rmd-->

---
output:
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r setup01,echo=FALSE}
#rm(list = ls())


source("intro.R")
```

\part{Statistica Descrittiva}

# I Fenomeni Collettivi

Scrivono Agresti, e Franklin, (2007), nel loro celebre libro:

>"Statistics is the art and science of designing studies and analyzing the data that those studies produce. Its ultimate goal is translating data into knowledge and understanding of the world around us. In short, **statistics is the art and science of learning from data**".

> La statistica è l'arte e la scienza di pianificare la raccolta e l'analisi dei dati che tali ricerche producono. Il suo fine è di trasformare i dati in conoscenza e comprensione del mondo circostante. In sintesi: **La statistica è l'arte e la scienza di imparare dai dati**.

> *Agresti, A., and Franklin, C. (2007), Statistics: the Art and Science of Learning from Data, Upper Saddle River, Pearson Prentice Hall.*

La **Statistica Descrittiva** si occupa di analizzare in modo meramente descrittivo, senza tentare di spiegare quale meccanismo
li abbia generati. Di quello si occupa la Statistica Inferenziale nella terza parte del libro.

Definiremo dapprima cosa sono i dati, quindi \@ref(dati), come si classificano \@ref(var-stat) e come si raccolgono \@ref(riv). 

## I Dati {#dati}

In latino *datum* significa un fatto, quindi i *dati* sono collezioni di fatti.
La Statistica ha come oggetto di studio i dati dalla loro definizione fino alla sintesi della conclusioni. 
Ovvero la statistica si occupa di definire i dati di interesse, di organizzarne la raccolta, di elaborarli e di sintetizzare le conclusioni.

I dati vengono raccolti su individui (__unità statistiche__), che sono accomunate per alcuni aspetti e presentano _variabilità_ su altri, creando così un *fenomeno collettivo*.
La Statistica è dunque la scienza che studia i dati che definiscono i fenomeni collettivi e che presentano forme di variabilità.
La Statistica individua: concetti, metodi, e strumenti per la loro analisi.

Esempi di fenomeni collettivi che presentano variabilità:

-   Reddito,
-   Titolo di studio,
-   Durata ricerca di occupazione,
-   Preferenze di un consumatore,
-   Durata di una malattia,
-   Durata di una apparecchiatura

Il fenomeno è definito da concetti: caratteri o variabili (sinonimi).
l'unita statistica è l'elemento su cui si osservano i caratteri oggetto di studio.
una popolazione statistica o collettivo statistico e un insieme di unita statistiche omogenee rispetto a una o più caratteristiche o caratteri.

__Esempio.__ Variabile = genere (M, F)

-   Unità statistica = il singolo individuo in quest'aula
-   Popolazione = gli studenti di quest'aula.

__Esempio.__ Variabile = stato (difettoso, non difettoso)

-   Unità = pezzo prodotto
-   Popolazione = tutti i pezzi prodotti da settembre 2019

__Esempio.__ Variabile = numero giorni di degenza

-   Unità = individuo ricoverato
-   Popolazione = tutti i ricoverati dell'ospedale XXX dal 2012 al 2020

__Esempio.__ Variabile = kg di produzione

-   Unità = ettaro coltivato con la varietà X
-   Popolazione = tutti i possibili ettari coltivati con la varietà X

## Variabili Statistiche {#var-stat}

Ogni variabile statistica è caratterizzata dal numero e dal tipo di **modalità** che questa può assumere.

**Esempi.**

-   Variabile genere: due sole possibili modalità {M, F}
-   Variabile colore dei capelli: {Biondo, Castano, Rosso}
-   Variabile titolo di studio: {Elementari, Medie, Superiori, Laurea, Post Laurea}
-   Variabile numero di interventi: {0, 1, 2, 3, ...}

Le variabili possono essere **Qualitative** o **Quantitative**.

- Le variabili **qualitative**: 
    - possono essere *sconnesse* (o con scala nominale): le modalità possono essere solo uguali o diverse (né ordinate e né ordinabili). Ad esempio il genere, il tipo di titolo di studio, lo stato civile, la regione di provenienza, ecc-
    - Oppure *ordinate* (o con scala ordinale): le modalità sono ordinabili. Come per esempio: il titolo di studio, il livello di qualifica, le preferenze, ecc.
- Le variabili **quantitative** possono essere:
    - **Discrete**: le modalità possono avere una corrispondenza biunivoca con un sottoinsieme dei numeri interi.
*Esempio*: numero di figli, eta , voto di laurea.
    - **Continue**: le modalità possono avere una corrispondenza biunivoca con un sottoinsieme dei numeri reali.
*Esempio*: reddito, consumo, risparmio, altezza.
    - **Trasferibili**: se l'unità statistica può cedere tutto o parte del carattere posseduto a un'altra in modo sensato.

## Popolazioni Statistiche

La **Popolazione Statistica** è l'insieme di tutte le unità che rispondono alla definizione.
Una popolazione si dice 

- **Finita** se si conosce il numero esatto delle sue unità 
- **Infinita** se non si conosce il numero delle sue unità

*Esempi di popolazione finita:* 

- Gli aventi diritto al voto 
- Le imprese iscritte alla camera di commercio di Modena

*Esempi di popolazione infinita:* 

- I consumatori della marca X 
- Le aziende che hanno un gestionale con più di 5 anni

## Le rilevazioni Statistiche {#riv}

Si distinguono in due tipologie:

- **Sperimentali** utilizzate, in contesti scientifici, come Fisica, Medicina, Chimica e prevedono
  -   Ipotesi di lavoro
  -   Possibilità di controllo
- **Osservazionali** come le indagini di mercato, i sondaggi, ecc.
nei quali non si ha possibilità di controllo.

L'obiettivo principale di un'indagine statistica è la **conoscenza di una popolazione obiettivo** o di riferimento ($\mathscr{P}$) intesa come insieme di unità elementari su cui si manifesta il fenomeno oggetto di studio.
L'indagine è svolta quasi sempre su un campione, che è un sottoinsieme di $\mathscr{P}$, diversamente si avrebbe il **censimento**.

Per estrarre un campione occorre la **lista** di $\mathscr{P}$.
La lista è l'elenco degli elementi appartenenti a $\mathscr{P}$ e rappresenta lo strumento principale per la scelta delle unità statistiche campionarie.

-   **Censimento**: è una indagine completa perché esamina tutte le unità statistiche che compongono la popolazione oggetto di studio, $\mathscr{P}$.
-   **Campionamento**: è una indagine parziale perché esamina solo un sottoinsieme, detto "campione", della popolazione oggetto di studio, $\mathscr{P}$.

### Fasi dell'indagine

-   definizione degli obiettivi,
-   definizione delle unità e delle variabili da rilevare,
-   scelta del periodo di riferimento.
-   individuazione della popolazione e della lista delle unità statistiche.
-   piano di campionamento
-   raccolta dei dati,
  -   scelta della tecnica di rilevazione,
  -   formulazione del questionario e pretest,
  -   rilevazione sul campo.
-   registrazione dei dati:
-   controllo e correzione.
-   elaborazione e analisi dei dati.

| $\phantom{a}$                   | Cens     | Camp              |
|---------------------------------|----------|-------------------|
| Accuratezza delle Stime         | Pro      | Contro            |
| su livelli territoriali piccoli | perfetta | alto rischio      |
|                                 |          | di non campionare |
| Esaustività                     | Pro      | Contro            |
|                                 | sì       | no                |
| Costi                           | Contro   | Pro               |
|                                 | Alti     | Contenuti         |
| Tempi di elaborazione           | Contro   | Pro               |
|                                 | Alti     | Contenuti         |
| Qualità dei dati                | Contro   | Pro               |
|                                 | Bassa    | Alta              |
| Quantità dei variabili          | Contro   | Pro               |
|                                 | Bassa    | Alta              |

## La matrice dei dati

La matrice dei dati è una tabella che consente di raccogliere in modo efficiente molti tipi diversi di dati.

```{r 01-dati-1,echo=FALSE}
j <- c(1:3,"$n$")
età <- sample(18:80,size = 4,replace = T)
Sesso <- sample(c("M","F"),size = 4,replace = T)
StatoCivile <- sample(c("Sposato","Non sposato"),size = 4,replace = T)
Titolo <- sample(c("Elementari","Medie","Superiori","Laurea"),size = 4,replace = T)
Reddito <- round(rnorm(4,10),2)
Statura <- round(rnorm(4,170,20),2)



dat <- matrix(nrow = 5,ncol = 7)
dat[1:3,] <- cbind(j,età,Sesso,StatoCivile,Titolo,Reddito,Statura)[1:3,]
dat[4,] <- "$\\vdots$"
dat[5,] <- cbind(j,età,Sesso,StatoCivile,Titolo,Reddito,Statura)[4,]

kable(dat,booktabs = T, escape = F,linesep = "", digits = 4,col.names = c("$i$","Età","Sesso","Stato Civile","Titolo di Studio","Reddito x 1000€","Statura cm"))
```

Sulle RIGHE le UNITÀ STATISTICHE: si leggono le determinazioni dei caratteri oggetto di studio associati a una specifica unità statistica.
Sulle COLONNE i CARATTERI: si leggono le modalità delle unità statistiche associate a uno specifico carattere.

\clearpage

## Riepilogo sulle Variabili

:::: {.info data-latex=""}
-   **Qualitativa**, la variabile è espressa attraverso etichette qualitative
    -   *Qualitative sconnesse*: le caratteristiche che la VS può assumere hanno un ordinamento soggettivo;
        -   genere,
        -   stato civile,
        -   settore di occupazione,
        -   generi musicali.
    -   *Qualitative ordinate*: le caratteristiche che la VS può assumere hanno un ordinamento oggettivo
        -   titolo di studio,
        -   preferenze,
        -   giudizi.
-   **Quantitativa**, la variabile è espressa attraverso una scala numerica.
    -   *Quantitative Discrete*: le caratteristiche che la VS può assumere sono in numero finito al più numerabile $\rightarrow$ corrispondenza con i numeri interi;
        -   numero di incidenti,
        -   voto di laurea.
    -   *Quantitative Continue*: le caratteristiche che la VS può assumere sono in numero infinito non numerabile.
        -   misure di lunghezza, capienza e peso,
        -   temperature,
        -   reddito.
::::

<!--chapter:end:01-dati.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r s01, echo=FALSE}


source("intro.R")

```

# Variabili Statistiche e Distribuzioni di Frequenza

La statistica si fonda sulla necessità di comprendere e descrivere fenomeni complessi in modo sistematico e universale. Per farlo, è indispensabile adottare un linguaggio rigoroso che consenta di astrarre dai dettagli specifici e lavorare direttamente sulle relazioni tra i dati. La formalizzazione non è un vezzo, ma una necessità: permette di trattare insiemi di dati di qualunque dimensione o complessità senza cambiare le regole del ragionamento.

Il linguaggio formale che utilizzeremo in questo libro è pensato per garantire una sintesi efficace e una generalizzazione immediata. Una manciata di osservazioni e un archivio di miliardi di righe possono essere analizzati con gli stessi strumenti e le stesse regole. Questa uniformità concettuale è ciò che rende il pensiero statistico potente: non ci si perde nei dettagli numerici, ma si lavora direttamente sulle proprietà e sulle trasformazioni delle strutture che i dati rappresentano.

Adottare una notazione rigorosa consente inoltre di comunicare con chiarezza e precisione. Ogni simbolo, ogni operatore ha un significato ben definito che elimina ambiguità e rende possibile il ragionamento collaborativo, indipendentemente dal contesto applicativo. La formalizzazione non è solo un linguaggio per il calcolo, ma uno strumento per pensare, per astrarre e per collegare il particolare all'universale.

Lavorare in questo modo significa concentrarsi sui concetti fondamentali, svincolandosi dalla necessità di riferirsi continuamente alla numerosità dei dati o alla loro specifica natura. È così che il linguaggio statistico diventa non solo un mezzo per analizzare, ma anche per costruire modelli e sviluppare inferenze che si applicano a fenomeni ben oltre i dati osservati.

In statistica, la scelta dei simboli gode di una flessibilità maggiore rispetto ad altre discipline come la fisica, dove molte notazioni sono rigidamente standardizzate. Questa libertà riflette la natura più applicativa e interdisciplinare della statistica, che si adatta a contesti diversi e a una grande varietà di fenomeni. Tuttavia, nel tempo, si è consolidata una notazione comune, grazie alla diffusione di libri di testo e alla pratica accademica, che offre un riferimento condiviso pur lasciando margini per piccole personalizzazioni.

In fisica, al contrario, cambiare i simboli può portare a confusione o addirittura compromettere la comprensione. Ad esempio, la costante gravitazionale universale \( G \) e la velocità della luce nel vuoto \( c \) sono simboli universalmente riconosciuti. Rinominare \( G \) con un altro simbolo, come \( k \), o \( c \) con \( v \), rischierebbe di generare fraintendimenti, poiché quei simboli sono già associati ad altri concetti fondamentali, come la costante elastica e la velocità generica di un corpo.

In statistica, invece, sebbene esistano convenzioni consolidate – come indicare una variabile casuale con una lettera maiuscola (\( X \)) e un’osservazione specifica con una minuscola (\( x_i \)) – la comunità accetta varianti ragionevoli, purché siano chiaramente definite. In questo libro, useremo una notazione che rispetta gli standard più diffusi, integrandola con piccole personalizzazioni pensate per migliorare la chiarezza e la leggibilità, senza mai perdere il rigore. Questo approccio garantisce che il linguaggio sia al tempo stesso accessibile e conforme alle convenzioni accademiche, facilitando il collegamento con altri testi e contesti di studio.

## Variabili Statistiche

Una _Variabile Statistica_ (VS) è una qualunque caratteristica osservabile sugli individui (unità statistiche) della popolazione di riferimento, che _varia_ da individuo ad individuo.


### Notazione di Base
- $\mathbf{x}=(x_1,x_2,...,x_i,...,x_n)$, etichette simboliche per i dati, il primo dato osservato, il secondo ecc.
- $i$, indice che conta le osservazioni nell'ordine in cui sono state osservate
  - $i\in\{1,2,...,n\}$
  - il primo, il secondo, ... l'$i$-esimo, ... l'$n$-esimo (l'ultimo)
- $n$, numerosità assoluta: il numero totale di individui osservati.
- $S_X=\{\mathrm{x}_1,...,\mathrm{x}_j,...,\mathrm{x}_K\}$, l'insieme di tutte le modalità possibili che la variabile statistica è suscettibile di assumere.
- $j$, indice che conta le modalità: prima, seconda, ..., $j$-esima,..., la $K$-esima.
- $K$, numero di modalità.

:::: {.example #vg name="Variabile: genere"}
$\phantom{.}$

- $\mathbf{x}=(x_1 = M, x_2 =F, x_3 =M, x_4=F,x_5=F,x_6=F)$
- $n=6$
- $S_X=\{\mathrm{x}_1 = F,\mathrm{x}_2 = M\}$
- $K=2$
::::

:::: {.example name="Variabile: titolo di studio"}
$\phantom{.}$

- $\mathbf{x}=(x_1 = E, x_2 =M, x_3 =L, x_4=S,x_5=S,x_6=S,
x_7=L,x_8=M,x_9=L,x_{10}=S)$
- $n=10$
- $S_X=\{\mathrm{x}_1 = E,\mathrm{x}_2 = M, \mathrm{x}_3=S,\mathrm{x}_4=L\}$
- $K=4$
::::

:::: {.example name="Variabile: Numero di interventi di manutenzione giornalieri"}
$\phantom{.}$
 
- $\mathbf{x}=(x_1 = 0, x_2 =1, x_3 =0, x_4=2,x_5=1,x_6=1,
x_7=0,x_8=1,x_9=3,x_{10}=1)$
- $n=10$
- $S_X=\{\mathrm{x}_1 = 0,\mathrm{x}_2 = 1,\mathrm{x}_3=2,\mathrm{x}_4=3,...\}$
- $K=+\infty$
::::

### Ordinamento e conteggio

Se l'ordine di osservazione non è influente ai fini della conoscenza del fenomeno
i dati possono essere permutati (mescolati) a piacimento, la sequenza:

$$(x_{(1)},x_{(2)},...,x_{(i)},...,x_{(n)}),$$ 

indica i dati riordinati, dal più piccolo, al più grande. Se i dati sono numerici
l'ordinamento è univoco, se i dati sono categoriali l'ordinamento è arbitrario.

:::: {.example name="Continua"}
Continuiamo l'esempio della variabile genere discussa
nell'esempio \@ref(exm:vg)

- $\mathbf{x}=(x_1 = M, x_2 =F, x_3 =M, x_4=F,x_5=F,x_6=F)$
- $S_X=\{\mathrm{x}_1 = F,\mathrm{x}_2 = M\}$
- $x_{(1)}=F,x_{(2)}=F,x_{(3)}=F,x_{(4)}=F,x_{(5)}=M,x_{(6)}=M$
::::

:::: {.example name="Continua: codifica 0, 1"}
$\phantom{.}$

- Variabile: genere {M -> 0,F-> 1}
- $\mathbf{x}=(x_1 = 0, x_2 =1, x_3 =0, x_4=1,x_5=1,x_6=1)$
- $S_X=\{\mathrm{x}_1 = 0,\mathrm{x}_2 = 1\}$
- $x_{(1)}=0,x_{(2)}=0,x_{(3)}=1,x_{(4)}=1,x_{(5)}=1,x_{(6)}=1$
- **nota**: nella codifica 0, 1 ha senso sommare i dati:
\[x_1+x_2+x_3+x_4+x_6=4,~~\text{Numero di femmine}\]
::::

:::: {.example name="Continua: Variabile titolo di studio"}
$\phantom{.}$

- $\mathbf{x}=(x_1 = E, x_2 =M, x_3 =L, x_4=S,x_5=S,x_6=S,
x_7=L,x_8=M,x_9=L,x_{10}=S)$
- $S_X=\{\mathrm{x}_1 = E,\mathrm{x}_2 = M, x_3=S, x_4=L\}$
- $x_{(1)}=E,x_{(2)}=M,x_{(3)}=M,x_{(4)}=S,x_{(5)}=S,x_{(6)}=S,x_{(7)}=S,x_{(8)}=L,x_{(9)}=L,
x_{(10)}=L$

::::

:::: {.example name="Continua: Codifica Numerica"}
$\phantom{.}$

- Variabile: titolo di studio {E -> 1, M -> 2, S -> 3, L -> 4}
- $\mathbf{x}=(x_1 = 1, x_2 = 2, x_3 = 4, x_4= 3,x_5=3,x_6=3,
x_7=4,x_8=2,x_9=4,x_{10}=3)$
- $S_X=\{\mathrm{x}_1 = 1,\mathrm{x}_2 = 2, x_3=3, x_4=4\}$
- $x_{(1)}=1,x_{(2)}=2,x_{(3)}=2,x_{(4)}=3,x_{(5)}=3,x_{(6)}=3,x_{(7)}=3,x_{(8)}=4,x_{(9)}=4,
x_{(10)}=4$
- ha senso sommare i dati?

La codifica numerica corretta sarebbe più complessa

```{r 02-distr-freq-1}
x<-c(x_1 = 1, x_2 = 2, x_3 = 4, x_4= 3,x_5=3,x_6=3,x_7=4,x_8=2,x_9=4,`x_{10}`=3)

x_lab <- 1:4

dat <- data.frame(outer(x, x_lab, FUN = function(x, y) as.integer(x == y)))
dat <- rbind(dat,Tot = colSums(dat))
names(dat) <- c("E","M","S","L")
row.names(dat) <- paste0("$",row.names(dat),"$")
tabl(dat)
```

I totali di colonna hanno senso e indicano il numero di individui che ha un determinato titolo 
di studio.

Questa codifica è sovra abbondante infatti come per maschio e femmina possiamo contare
solo un colonna di presenza 1 (è femmina) e assenza 0 (non è femmina e quindi maschio), per una variabile a 4 modalità
possiamo contare solo 3, ad esempio

- 0,0,0 elementari
- 1,0,0 medie
- 0,1,0 superiori
- 0,0,1 università

```{r 02-distr-freq-2}
x<-c(x_1 = 1, x_2 = 2, x_3 = 4, x_4= 3,x_5=3,x_6=3,x_7=4,x_8=2,x_9=4,`x_{10}`=3)

x_lab <- 1:4

dat <- data.frame(outer(x, x_lab, FUN = function(x, y) as.integer(x == y)))[,-1]
dat <- rbind(dat,Tot = colSums(dat))
names(dat) <- c("M","S","L")
row.names(dat) <- paste0("$",row.names(dat),"$")
tabl(dat)
```

Per sapere il numero persone che ha al massimo le elementari basta fare 10 (numero totale di individui)
 meno 2 (medie) più 4 (superiori) più 3 (laureati):
 
 \[
 10-(2+4+3)=1~~~\text{con le elemntari}
 \]

::::

:::: {.example name="Continua: Numero di interventi di manutenzione giornalieri"}
$\phantom{.}$

- $\mathbf{x}=(x_1 = 0, x_2 =1, x_3 =0, x_4=2,x_5=1,x_6=1,
x_7=0,x_8=1,x_9=3,x_{10}=1)$
- $S_X=\{\mathrm{x}_1 = 0,\mathrm{x}_2 = 1,\mathrm{x}_3=2,\mathrm{x}_4=3,...\}$
- $x_{(1)}=0,x_{(2)}=0,x_{(3)}=0,x_{(4)}=1,x_{(5)}=1,x_{(6)}=1,x_{(7)}=1,x_{(8)}=1,x_{(9)}=2,
x_{(10)}=3$
- ha senso sommare i dati?
- cosa rappresenta la somma dei dati?
::::

:::: {.example name="Ore uomo dedicate a interventi di manutenzione"}
$\phantom{.}$

Supponiamo di aver collezionato il numero di ore uomo (e frazioni di ora) dedicate
ad ogni intervento di manutenzione L'unità statistica sarà l'intervento, $i=1$ il primo, $i=2$, il secondo, ecc. e assumerà un valore decimale $x_3=3.5$ significa che la terza manutenzione ha impiegato un addetto per 3 ore e mezza.

- $\mathbf{x}=(x_1 = 0.4, x_2 =2.7, x_3 =3.5, x_4=1.4,x_5=4.3,x_6=4.6,
x_7=0.2,x_8=1.9,x_9=3.4,x_{10}=0.1)$
- $S_X=\{\mathrm{x}_1 = 0.0,\mathrm{x}_2 = 0.1,\mathrm{x}_3=0.2,\mathrm{x}_4=0.3,...\}$
- $x_{(1)}=0.1, x_{(2)}=0.2, x_{(3)}=0.4, x_{(4)}=1.4, x_{(5)}=1.9, x_{(6)}=2.7, x_{(7)}=3.4, x_{(8)}=3.5, x_{(9)}=4.3, x_{(10)}=4.6$
- ha senso sommare i dati?
- cosa rappresenta la somma dei dati?
::::

### Le unità di misura

In statistica, ogni dato è sempre espresso in una specifica unità di misura, che conferisce significato al valore numerico registrato. Le unità di misura variano in base alla natura dei dati e al fenomeno osservato, e possono includere:

1. **Conteggi**: I dati che rappresentano quantità possono essere espressi in unità di conteggio, come unità singole, decine, centinaia, migliaia, ecc. Ad esempio, se registriamo il numero di visitatori in un parco in un anno, può essere più comodo esprimere 12.000 visitatori come "12 migliaia" (12 x 1000), facilitando la lettura. Analogamente, 1.200 prodotti venduti potrebbero essere espressi come "1,2 migliaia" per rendere i confronti immediati tra dati di grandezza diversa.

2. **Misure metriche**: Le grandezze fisiche come la lunghezza, la massa, la capacità e la temperatura sono espresse in unità metriche, come metri, chilogrammi, e litri. Ad esempio, per confrontare il peso di vari articoli, un dato in grammi potrebbe risultare scomodo se molto elevato; per articoli pesanti, come $12~000$ grammi, potrebbe essere più leggibile esprimere il dato in chilogrammi come 12 kg, dove 1 kg = $1~000$ grammi. Questo cambio di unità mantiene invariato il rapporto tra le osservazioni, facilitando la comprensione e l'analisi.

3. **Misure derivate o di rapporto**: Alcune unità di misura rappresentano rapporti tra grandezze, come chilometri all'ora (km/h) per la velocità, o litri per 100 chilometri (L/100 km) per il consumo di carburante. Ad esempio, se un'auto consuma 7 litri ogni 100 km, esprimere il consumo in questa unità (7 L/100 km) è più immediato e rappresentativo di un valore medio, rispetto all'uso di litri per singolo chilometro (0.07 L/km), che potrebbe risultare poco intuitivo.

4. **Misure di risultati e punteggi**: In statistica, si usano spesso trasformazioni che combinano più misure per produrre un indicatore con una scala di misura propria, utile per confrontare sinteticamente informazioni complesse. Consideriamo la seguente formula generale:
\[
x_i = f(a_{i1}, a_{i2}, a_{i3}, \dots)
\]
dove \( a_{ij} \) rappresenta le varie misure o statistiche che contribuiscono al calcolo del valore \( x_i \), cioè l'indicatore della i-esima osservazione.
Per esempio, immaginiamo di voler calcolare un **punteggio di idoneità** per i candidati a un lavoro, basato su tre misure diverse:

    - \( a_{i1} \): esperienza in anni,
    - \( a_{i2} \): punteggio di un test tecnico (da 0 a 100),
    - \( a_{i3} \): valutazione del colloquio (da 0 a 5).
  
    Per produrre un indicatore che misuri l'idoneità complessiva su una scala uniforme, potremmo usare una formula come:
      \[
      x_i = 0.5 \cdot a_{i1} + 0.3 \cdot a_{i2} + 2 \cdot a_{i3}
      \]
      Questa formula pondera ciascuna misura per dare maggiore peso all'esperienza (0.5) e al colloquio (2) rispetto al test tecnico (0.3). Per il quinto candidato ($i=5$) con 5 anni di esperienza, un punteggio di 80 al test e una valutazione di 4 al colloquio, il punteggio di idoneità risulterebbe:
      \[
      x_5 = 0.5 \cdot 5 + 0.3 \cdot 80 + 2 \cdot 4 = 2.5 + 24 + 8 = 34.5
      \]
      In questo caso, il punteggio di idoneità \( x_i \) è su una scala propria, definita dalle scelte di peso e dalle unità combinate, e consente di confrontare i candidati in modo sintetico, anche se ogni componente ha un'unità di misura diversa.
      

### Trasformazioni lineari

In matematica e statistica, una trasformazione lineare è un’operazione che modifica i dati mantenendo invariati i rapporti tra le osservazioni. La forma generale di una trasformazione lineare è:

\[
y_i = a + b x_i
\]

dove:

  - \( x_i \) rappresenta la **misura i-esima** del dato originale.
  - \( a \) rappresenta uno **spostamento dell'origine** (traslazione), che posiziona i dati su un nuovo punto di partenza, spostando l'intero grafico verso l'alto o verso il basso.
  - \( b \) è il **fattore di scala**, che ridimensiona i valori, influenzando la pendenza della retta. Un fattore maggiore di 1 espande i valori, mentre uno tra 0 e 1 li contrae.

Geometricamente, il termine \( a \) trasla il grafico lungo l'asse verticale senza modificare le distanze relative tra i punti, mentre \( b \) varia la pendenza, allargando o comprimendo i dati rispetto all'origine.

#### Cambiamento di scala come trasformazione lineare

Un cambiamento di scala è una trasformazione lineare applicata a ogni misura i-esima del dato, che permette di adattare l'unità di misura senza alterare i rapporti tra le osservazioni. Vediamo alcuni esempi pratici.

:::: {.example name="Ore uomo dedicate a interventi di manutenzione"}

Supponiamo di avere i dati su diverse spese in euro e di volerli esprimere in migliaia di euro per facilitare la lettura. La trasformazione è:

\[
y_i = \frac{1}{1000} \cdot x_i
\]

Dove \( x_i \) è il valore in euro della misura i-esima e \( y_i \) è il valore corrispondente in migliaia di euro. Per chiarire questo concetto, prendiamo \( i=4 \) con \( x_4 = 12~000 \):

\[
y_4 = \frac{1}{1000} \cdot 12~000 = 12
\]

Quindi, invece di esprimere la quarta misura come $12~000$ euro, la rappresentiamo come 12 _migliaia_ di euro, semplificando la gestione e la comparabilità dei dati.
:::: 

:::: {.example name="Ore uomo dedicate a interventi di manutenzione"}

Anche la conversione da gradi Celsius (\( C \)) a gradi Fahrenheit (\( F \)) segue una trasformazione lineare, espressa come:

\[
F_i = 32 + \frac{9}{5} \cdot C_i
\]

Qui, il termine 32 rappresenta uno **spostamento dell'origine**, poiché 0 °C corrisponde a 32 °F, mentre il fattore \( \frac{9}{5} \) espande i valori per adattarsi alla scala Fahrenheit. Ad esempio, se \( i=2 \) e \( C_2 = 25 \):

\[
F_2 = 32 + \frac{9}{5} \cdot 25 = 32 + 45 = 77
\]

Questa trasformazione mantiene la proporzionalità tra le temperature, adattando la scala e le unità di misura.
::::

#### Rappresentazione grafica delle trasformazioni lineari

```{r tranf-lin}
set.seed(1)
x <- sort(c(0,round(sample((2:8)*2,4,F))))
x_r <- round(x/2,2)

redd <- paste0("x_",1:5,"=",x,collapse = ", ")
redd_r <- paste0("x_",1:5,"=",x_r,collapse = ", ")
```

Come abbiamo visto una trasformazione lineare è data 
$$
y_i= a+ b\cdot x_i, \forall i =1,...,n
$$

**caso 1: $a=0$, $b<1$**

I dati si contraggono, lo zero resta invariato
Per esempio: supponiamo di avere osservato ($`r redd`$), e fissato $b=1/2$
$$
y_i= \frac{1}{2} \cdot x_i, \forall i = 1,...,5
$$
allora ($`r redd_r`$). Geometricamente
```{r tranf-lin-gr1}
opar <- par()
par(mar=c(0,0,0,0))
margin_x <- 0.001
margin_y <- 0.02

dx <- x - x_r
dy <- 1 - 0

# Nuove coordinate con margini separati per x e y
x0_adj <- x - margin_x * dx
y0_adj <- 1 - margin_y * dy
x1_adj <- x_r + margin_x * dx
y1_adj <- 0 + margin_y * dy
plot(range(x)*c(0,1.1),c(-.5,1.5),type="n",axes=F,xlab="",ylab="")
arrows(0,1,max(x)*1.1,1)
points(x,rep(1,times=5),pch=4,col=2)
arrows(0,0,max(x)*1.1,0)
points(x_r,rep(0,times=5),pch=4,col=2)
text(x,1.25,x,cex=.5)
text(x_r,-.25,x_r,cex=.5)
arrows(x0_adj,y0_adj,x1_adj,y1_adj,length = .1)
```

Osserviamo che i dati si restringono, i dati originari vanno da $`r min(x)`$ a $`r max(x)`$, quelli trasformati da $`r min(x_r)`$ a $`r max(x_r)`$. Notiamo che lo zero resta invariato.


**caso 2: $a= 0$, $b>1$**
```{r}
set.seed(1)
x <- sort(c(0,round(sample((2:8)*2,4,F))))
x_r <- round(x*2,2)
redd <- paste0("x_",1:5,"=",x,collapse = ", ")
redd_r <- paste0("x_",1:5,"=",x_r,collapse = ", ")
```
I dati si espandono, lo zero resta invariato.
Per esempio: supponiamo di avere osservato ($`r redd`$), e fissato $b=2$
$$
y_i= 2\cdot x_i, \forall i = 1,...,5
$$
allora ($`r redd_r`$). Geometricamente
```{r tranf-lin-gr2}
margin_x <- 0.001
margin_y <- 0.02

dx <- x - x_r
dy <- 1 - 0

# Nuove coordinate con margini separati per x e y
x0_adj <- x - margin_x * dx
y0_adj <- 1 - margin_y * dy
x1_adj <- x_r + margin_x * dx
y1_adj <- 0 + margin_y * dy
plot(c(0,36),c(-.5,1.5),type="n",axes=F,xlab="",ylab="")
arrows(0,1,max(x_r)*1.1,1)
points(x,rep(1,times=5),pch=4,col=2)
arrows(0,0,max(x_r)*1.1,0)
points(x_r,rep(0,times=5),pch=4,col=2)
text(x,1.25,x,cex=.5)
text(x,1.25,x,cex=.5)
text(x_r,-.25,x_r,cex=.5)
arrows(x0_adj,y0_adj,x1_adj,y1_adj,length = .1)
```


Osserviamo che i dati si espandono, i dati originari vanno da $`r min(x)`$ a $`r max(x)`$, quelli trasformati da $`r min(x_r)`$ a $`r max(x_r)`$. Notiamo che lo zero resta invariato.


**caso 3: $a\ne 0$, $b=0$**

```{r}
set.seed(1)
x <- sort(c(0,round(sample((2:8)*2,4,F))))
x_r <- round(1+x,2)
redd <- paste0("x_",1:5,"=",x,collapse = ", ")
redd_r <- paste0("x_",1:5,"=",x_r,collapse = ", ")
```
Le distanze tra i dati restano invariate, lo zero cambia.
Per esempio: supponiamo di avere osservato ($`r redd`$), e fissato $a=+1$
$$
y_i= 1 + x_i, \forall i = 1,...,5
$$
allora ($`r redd_r`$). Geometricamente
```{r tranf-lin-gr3}
margin_x <- 0.001
margin_y <- 0.02

dx <- x - x_r
dy <- 1 - 0

# Nuove coordinate con margini separati per x e y
x0_adj <- x - margin_x * dx
y0_adj <- 1 - margin_y * dy
x1_adj <- x_r + margin_x * dx
y1_adj <- 0 + margin_y * dy
plot(c(-1,19),c(-.5,1.5),type="n",axes=F,xlab="",ylab="")
arrows(0,1,max(x_r)*1.1,1)
points(x,rep(1,times=5),pch=4,col=2)
arrows(0,0,max(x_r)*1.1,0)
points(x_r,rep(0,times=5),pch=4,col=2)
text(x,1.25,x,cex=.5)
text(sort(c(x,x_r)),-.25,sort(c(x,x_r)),cex=.5,col=1:2)
arrows(x0_adj,y0_adj,x1_adj,y1_adj,length = .1)
segments(x,1,x,0,lty=2,col="gray")
```


Osserviamo che i dati sono stati traslati a destra di uno, i dati originari vanno da $`r min(x)`$ a $`r max(x)`$, quelli trasformati da $`r min(x_r)`$ a $`r max(x_r)`$. Notiamo che lo zero è cambiato nella nuova scala.


## Distribuzione di Frequenza

La frequenza indica quanto una modalità insiste sul collettivo. Le frequenze si dividono in:

:::: {.info data-latex=""}
::: {.definition name="Frequenze Assolute"}
Si definiscono le $n_j$ le __frequenze assolute__: il numero di individui che presentano la modalità $j$. 
:::
::::

:::: {.info data-latex=""}
::: {.definition name="Frequenze Relative"}
Si definiscono le $f_j=n_j/n$ le __frequenze relative__: la proporzione di individui che presentano la modalità $j$. 
:::
::::

:::: {.info data-latex=""}
::: {.definition name="Frequenze Percentuali"}
Si definiscono le $f_{\% j}=f_j\times 100$ le __frequenze percentuali__: la percentuale di individui che presentano la modalità $j$. 
:::
::::

:::: {.info data-latex=""}
::: {.proposition}
Le proprietà della frequenze assolute($n_{j}$) sono:

-  $0\leq n_{j} \leq n, \forall j=1,...,K$,
-  $\sum_{j=1}^{K} n_{j} = n$.

:::
::::

:::: {.info data-latex=""}
::: {.proposition}
Le proprietà della frequenze relative ($f_{j}$) sono:

-  $0\leq f_{j} \leq 1, \forall j=1,...,K$,
-  $\sum_{j=1}^{K} f_{j} = 1$.

:::
::::

:::: {.info data-latex=""}
::: {.proposition}
Le proprietà della frequenze percentuali ($f_{\% j}$) sono:

-  $0\leq f_{\%,\, j} \leq 100, \forall j=1,...,K$,
-  $\sum_{j=1}^{K} f_{\%,\, j} = 100$.
:::
::::

:::: {.info data-latex=""}
::: {.definition name="Distribuzione di Frequenza"}

Una **distribuzione di frequenza** è una tabella a cui vengono associate le modalità e le frequenze
:::
::::

:::: {.example name="Continua: Variabile: genere"}

$$x_{(1)}=F,x_{(2)}=F,x_{(3)}=F,x_{(4)}=F,x_{(5)}=M,x_{(6)}=M$$

<!-- $X$          | $n_j$   | $f_j$     | $f_{\% j}$ (\#tab:sesso) -->
<!-- -------------|---------|-----------|------------- -->
<!-- F            |  4      |  4/6=0.67 | 67% -->
<!-- M            |  2      |  3/6=0.33 | 33% -->
<!-- **Tot**      | **6**   |    **1**  | **100%** -->
<!-- Tabella: \@ref(tab:sesso) distribuzione di frequenza del genere in un collettivo di $n=6$ individui. -->


```{r sesso,echo=FALSE}
nomi<-c("$X$","$n_j$","$f_j$","$f_{\\% j}$")
sex<-c("F","M","Tot")
n<-c("$4$","$2$","$6$")
f<-c("$4/6=0.67$","$2/6=0.33$","$1.00$")
fp<-c("$67\\%$","$33\\%$","$100\\%$")
prn <- data.frame(sex,n,f,fp)
names(prn)<- nomi

kable(prn,booktabs=T,escape=F,linesep="",align = c("lrrr"),
      #caption = "Distribuzione di frequenza del genere in un collettivo di $n=6$ individui."
      )%>%
  row_spec(2,hline_after = T)%>%
  row_spec(3,hline_after = T,bold = T)%>%
  kableExtra::kable_styling(latex_options = "hold_position")

```
::::

:::: {.example name="Continua: Variabile titolo di studio"}
$\phantom{.}$

$x_{(1)}=E,x_{(2)}=M,x_{(3)}=M,x_{(4)}=S,x_{(5)}=S,x_{(6)}=S,x_{(7)}=S,x_{(8)}=L,x_{(9)}=L,
x_{(10)}=L$

```{r titolo,echo=FALSE}
nomi<-c("$X$","$n_j$","$f_j$","$f_{\\% j}$")
tit<-c("E","M","S","L","Tot")
n<-c("$2$","$4$","$8$","$6$","$20$")
f<-c("$2/20=0.1$","$4/20=0.2$","$8/20=0.4$","$6/20=0.3$","$1.0$")
fp<-c("$10\\%$","$20\\%$","$40\\%$","$30\\%$","$100\\%$")
prn <- data.frame(tit,n,f,fp)
names(prn)<- nomi

kable(prn,booktabs=T,escape=F,linesep="",align = c("lrrr"),
      #caption = "Distribuzione di frequenza del titolo di studio in un collettivo di $n=10$ individui."
      )%>%
  row_spec(4,hline_after = T)%>%
  row_spec(5,hline_after = T,bold = T)%>%
  kableExtra::kable_styling(latex_options = "hold_position")

```
::::

### Dati quantitativi continui

Se i dati sono quantitativi continui il numero delle modalità è spesso di
gran lunga superiore al numero dei dati e non sempre è possibile fissare un limite superiore 
in anticipo all'osservazione dei dati. Se per esempio volessi misurare il 
reddito di una persona in centesimi, otterrei:

:::: {.example name="Variabile: Reddito mensile lordo in migliaia di euro"}
$\phantom{x}$

- unità di rilevazione: famiglie del comune A a febbraio 2021
- $n=45$
- $S_X=\{0.00,0.01,0.02,...,100.00,100.01,...,2000.00,...,10~000,...\}$

Qui di seguito i dati nell'ordine in cui sono stati raccolti è sono mostrati sopra, mentre
i dati riordinati sono mostrati sotto:

<!-- ::: {.center data-latex=""} -->

\vspace{10pt}
<!-- \fontsize{6}{4}\selectfont -->
\scriptsize
```{r dati2,results='asis',echo=FALSE}
set.seed(13)   # per ottenere sempre la stessa simulazione
n <- 45      # ampiezza campionaria
nomex <- "Consumo"
samp <- (round(c(runif(9,0,3),runif(16,3,5),runif(10,5,10),runif(10,10,20)),2))
sams <- sort(samp)

samp1 <- format(samp,nsmall = 2)
sams1 <- format(sams,nsmall = 2)
# c1 <- paste("$x_{(", 1:9 ,")}=$",sep="")
# c2 <- paste("$x_{(",10:18,")}=$",sep="")
# c3 <- paste("$x_{(",19:27,")}=$",sep="")
# c4 <- paste("$x_{(",28:36,")}=$",sep="")
# c5 <- paste("$x_{(",37:45,")}=$",sep="")
# 
# out <- data.frame(c1,s1=samp[1:9],c2,s2=samp[10:18],c3,s2=samp[19:27],c4,s2=samp[28:36],c5,s2=samp[37:45])

#names(samp) <- nomex

# out %>%
# kbl(booktabs = T, escape = F, linesep = "",caption = "Dati riordinati in ordine crescente",
# col.names = rep("",times=10))
 # cat("\\[\\begin{array}{crcrcrcrcr|crcrcrcrcr}\n")
 # for (i in 1:9){
 # #cat("\\tiny ")
 # cat(paste(paste("x_{" ,seq(0,(4*9),by=9)+i, "}= & ",samp1[seq(0,(4*9),by=9)+i],collapse = " & ",sep=""),
 #           " & ",
 #           paste("x_{(",seq(0,(4*9),by=9)+i,")}= & ",sams1[seq(0,(4*9),by=9)+i],collapse = " & ",sep=""),
 #           "\\\\ \n"))
 # }
 # cat("\\end{array}\\]")

 
 
cat("\\[\\begin{array}{crcrcrcrcr}\n")
cat("\\hline","\n")
 for (i in 1:9){
 #cat("\\tiny ")
 cat(paste("x_{" ,seq(0,(4*9),by=9)+i, "}= & ",samp1[seq(0,(4*9),by=9)+i],collapse = " & ",sep=""),"\\\\ \n")
 }
cat("\\hline","\n")
 for (i in 1:9){
cat(paste("x_{(",seq(0,(4*9),by=9)+i,")}= & ",sams1[seq(0,(4*9),by=9)+i],collapse = " & ",sep=""), "\\\\ \n")
}
cat("\\hline","\n")
 cat("\\end{array}\\]")

 
```

\normalsize
<!-- ::: -->

Come si osserva aver rimesso in ordine i dati non ci aiuta a capire la distribuzione del fenomeno.
::::

### Raggruppamenti in Classi

L'idea è quella di raggruppare i dati in **intervalli contigui** e procedere alla
rappresentazione in distribuzione di frequenza. 
In tabella \@ref(tab:classi) vediamo a sinistra troppe poche classi, al centro troppe, 
mentre a destra vediamo che il numero delle classi e la loro ampiezza variabile rende più leggibile la distribuzione dei dati.

```{r classi,echo=FALSE}
brk  <- c(0,5,20)
k <- length(brk)-1
ls2e(stat_base(samp,brk))
dat3$`$f_{j\\%}$` <- dat3$`$f_j$`*100

poche <- dat3[1:(k+1),c(1:2,12)]

brk  <- seq(0,20,by=2.5)
k <- length(brk)-1
ls2e(stat_base(samp,brk))
dat3$`$f_{j\\%}$` <- dat3$`$f_j$`*100

# kable((dat3[1:(k+1),c(1:4,12)]),digits = 2) %>%
#   kable_styling(full_width = F)

# dat3[1:(k+1),c(1:4,12)] %>%
# kbl(booktabs = T, escape = F, digits = 2,linesep = "",caption = "Troppe classi, non si coglie la distribuzone.")%>%
#   kableExtra::kable_styling(latex_options = "hold_position")

troppe <- dat3[1:(k+1),c(1:2,12)]

brk  <- c(0,3,5,10,20)
k <- length(brk)-1
ls2e(stat_base(samp,brk))
dat3$`$f_{j\\%}$` <- dat3$`$f_j$`*100

giuste <- dat3[1:(k+1),c(1:2,12)]

kable(list(poche,troppe,giuste),booktabs = T, escape = F, digits = 2,linesep = "",caption = "A sinistra abbiamo troppe poche classi, si perde troppa variabilità. Al centro sono state scelte troppe classi, non si coglie la distribuzone. A destra infine le classi sono state scelte ad hoc per rappresentare al meglio i dati cercando un compromesso tra sintesi e ricchezza dei dati.")

```


### Frequenze Cumulate

Si definisce frequenza cumulata $F$ la seguente quantità:

- $F_1 = f_1$
- $F_2= f_1+f_2=F_1+f_2$
- $F_3= f_1+f_2+f_3=F_2+f_3$
- ...
- $F_j= f_1+f_2+...+f_j=F_{j-1}+f_j$
- ...
- $F_K= f_1+f_2+...+f_K=1$

ovvero $F_j=f_1+...+f_j$ _cumula_ tutte le frequenze dalla 1 alla $j$.

:::: {.example name="Continua: Variabile titolo di studio"}
$\phantom{.}$

:::  {.center data-latex=""}
```{r tit studio}
x <- c("E","M","M","S","S","S","S","U","U","U")
n <- (table(x))
kbl(cbind("$n_j$"= n,"$f_j$" = n/sum(n),"$F_j$"= cumsum(n)/sum(n)),booktabs = T, escape = F, digits = 2) %>%
    kable_styling(full_width = F, latex_options = "HOLD_position")
```
::: 

E si legge: $F_1=0.1$ ci dice che il 10% del collettivo in esame ha come massimo titolo ha non più delle elementari. $F_1=0.3$ ci dice che il 30% del collettivo ha come massimo titolo ha non più delle medie. $F_4=0.7$ ci dice che il 30% del collettivo ha come massimo titolo ha non più delle superiori e $F_5=1$ che il 100% del collettivo ha, al massimo, la laurea.
::::


:::: {.example name="Continua: Reddito"}
$\phantom{.}$

:::  {.center data-latex=""}
```{r classi2}
brk  <- c(0,3,5,10,20)
k <- length(brk)-1
ls2e(stat_base(samp,brk))
dat3$`$f_{j\\%}$` <- dat3$`$f_j$`*100
kbl((dat3[1:(k+1),c(1:2,4,7)]),digits = 2,booktabs = T, escape = F) %>%
    kable_styling(full_width = F, latex_options = "HOLD_position")
```
:::  

E si legge: $F_1=0.21$ ci dice che il 20% del collettivo in esame guadagna al massimo 3 (mila euro); alternativamente leggiamo che il 20% del collettivo **non** guadagna più di 3 (mila euro). $F_2=0.56$ ci dice che il 56% del collettivo guadagna al massimo 5. $F_2=0.56$ ci dice che il 56% del collettivo guadagna al massimo 5 (mila euro); il 56% non guadagna più di 5 (mila euro).  $F_3=0.78$ ci dice che il 78% del collettivo guadagna 10 (mila euro); il 78% non guadagna più di 10 (mila euro). E infine il 100% del collettivo guadagna al massimo 20  (mila euro).
::::

## Istogramma di Densità

È grafico che rappresenta rettangoli contigui la cui area è la frequenza e la base è l'intervallo di raggruppamento. Usiamo il simbolo $b_j$ per denotare l'ampiezza della base del rettangolo, l'altezza dei rettangoli viene chiamata _densità_

:::: {.info data-latex=""}
  \[h_j = Const.\times \frac {f_j} {b_j}\]
::::

Se $Const.=1$ si ottiene _l'istogramma di densità relativa_, la somma delle aree dei rettangoli è 1. Se $Const.=n$ si ottiene _l'istogramma di densità assoluta_, la somma delle aree dei rettangoli è $n$. Se $Const.=100$ si ottiene _l'istogramma di densità percentuale_, la somma delle aree dei rettangoli  è 100. Per comodità tutti gli esempi si riferiscono all'istogramma di densità percentuale.

\tiny
```{r conti}
nm <- c("$[\\text{x}_j$,", "$\\text{x}_{j+1})$", "$n_j$", "$f_j=\\frac{n_j}{n}$", "$b_j=\\text{x}_{j+1}-\\text{x}_{j}$", "$h_j=100\\times\\frac{f_j}{b_j}$ ")

r1 <- c("$[\\text{x}_1=0$,", "$\\text{x}_{2}=3)$", "$n_1=9$", "$f_1=\\frac {n_1} n =\\frac 9{45}=0.20$", "$b_1=3-0=3$", "$h_1=100\\times\\frac{0.20}{3}=6.67$")
r2 <- c("$[\\text{x}_2=3$,", "$\\text{x}_{3}=5)$", "$n_2=16$", "$f_2=\\frac {n_2} n =\\frac {16}{45}=0.36$", "$b_2=5-3=2$", "$h_2=100\\times\\frac{0.36}{2}=17.78$")
r3 <- c("$[\\text{x}_3=5$,", "$\\text{x}_{4}=10)$", "$n_3=10$", "$f_3=\\frac {n_3} n =\\frac {10}{45}=0.22$", "$b_3=10-5=5$", "$h_3=100\\times\\frac{0.22}{5}=4.44$")
r4 <- c("$[\\text{x}_4=10$", "$\\text{x}_{5}=20)$", "$n_4=10$", "$f_4=\\frac {n_4} n =\\frac {10}{45}=0.22$", "$b_4=20-10=10$", "$h_4=100\\times\\frac{0.22}{10}=2.22$")

out <- data.frame(rbind(r1,r2,r3,r4))
names(out) <- nm

kable(out,booktabs = T, escape = F, digits = 2,row.names = F,caption = "Come ricavare le quantità necessarie per calcolare l'istogramma di densità percentuale")%>%
  kableExtra::kable_styling(latex_options = "hold_position")
```
\normalsize

:::: {.example }

La tabella \@ref(tab:conti) mostra passo, passo lo sviluppo del calcolo. La figura \@ref(fig:hist) la corrispondente rappresentazione grafica.

```{r hist,fig.align='center',fig.cap="Rappresentazione grafica dell'istogramma di densità percentuale, l'area di ogni rettangolo corrisponde alla frequenza percentuale della classe, rappresentata sull'asse delle ascisse"}

brk  <- c(0,3,5,10,20)
k <- length(brk)-1
ls2e(stat_base(samp,brk))
dat3$`$f_{j\\%}$` <- dat3$`$f_j$`*100
if (!html) par(cex=.5)
histp(axes = T)

arrows(brk[-1],.1,brk[-5],.1,cod=3,length = .05,col=ared)

text((dat3[,1]+dat3[,2])/2,.5,dat3$`$b_j$`,col=ared)
text((dat3[,1]+dat3[,2])/2,dat3$`$h_j$`*5/8,round(dat3$`$f_j$`*100,2),cex=1/cex,col=iblue)
segments(-1,dat3$`$h_j$`,brk[-5]+.2,dat3$`$h_j$`,lty=2)
par(cex=cex)
```
::::

## La Funzione di Ripartizione

Se i dati sono quantitativi continui raggruppati in classi, la Funzione di Ripartizione
della VS $X$ è la funzione che misura l'area dell'istogramma di densità (le aree sommano ad 1)
dal più piccolo dei dati $x_{(0)}$ fino ad un $x$ qualunque.
Se nel caso dell'esempio precedente scegliessimo $x=7.2$, graficamente vedremmo la figura \@ref(fig:FdR).
Notiamo innanzitutto che:

\begin{eqnarray*}
   F(\text{x}_1)&=&  0\\
   F(\text{x}_2) &=& F_1\\
   F(\text{x}_3) &=& F_2\\
   \vdots~~~ && \vdots\\
   F(\text{x}_j) &=& F_{j-1}\\   
   \vdots~~~ && \vdots\\
   F(\text{x}_{K+1}) &=& 1   
\end{eqnarray*}

```{r FdR,fig.cap="Rappresentazione grafica della Funzione di Ripartizione di $X$ valutata nel punto $7.2$, $F(7.2)$ è l'area da 0 a 7.2 dell'istogramma."}
k <- length(brk)-1
br1  <- brk[-(k+1)]
br2  <- brk[-1]
histp()
axis(1,brk)
h.int(0,7.2,density=20,col=4)
axis(1,7.2)
```

Nel nostro caso $F(7.2)$ è la comma delle frequenze fino a 5 più l'area del rettangolo di base $(7.2-5)$ e altezza $h_3=`r dat2[3,6]`/100$, ovvero
\begin{eqnarray*}
   F(7.2) &=& f_1+f_2+\frac{(7.2-5)}{100}\times`r dat2[3,6]` \\
   &=& F_2 +2.5\times`r dat2[3,6]/100`\\
   &=& `r F.int(7.2)`
\end{eqnarray*}
Se per esempio ci interessasse sapere, in modo approssimato, che percentuale e quanti 
individui che guadagno meno di 7.2 (mila euro) al mese, basta moltiplicare
$F(7.2)$ per 100 e per $n$, rispettivamente.
\begin{eqnarray*}
   \%(X<7.2) &=& 100\times F(7.2) \\
   &=& `r 100*F.int(7.2)`\% \\
   \#(X<7.2) &=& 45\times F(7.2) \\
   &=& `r 45*F.int(7.2)` 
\end{eqnarray*}
Dove $\%(X<7.2)$ significa la _percentuale_ approssimata di dati minori di 7.2 e
dove $\#(X<7.2)$ significa il _numero_ approssimato di dati minori di 7.2.

Se per esempio sono interessato alla percentuale (o al numero) di dati compresi tra
2.4 e 7.2 osservo che
\begin{eqnarray*}
   \%(2.4<X<7.2) &=& 100\times (F(7.2)-F(2.4)) \\
   F(2.4) &=& \frac{2.4-0}{100}\times`r dat2[1,6]` \\
   &=&`r dat2[1,6]/100*2.4`\\
   \%(2.4<X<7.2) &=& 100\times (`r F.int(7.2)`-`r F.int(2.4)`) \\
   &=& `r 100*(F.int(7.2)-F.int(2.4))`. 
\end{eqnarray*}
Infatti calcolare l'area tra 2.4 e 7.2 equivale a calcolare l'area
fino a 7.2, l'area fino a 2.4 e sottrarle.

Più in generale la funzione di ripartizione cumula l'area dal più piccolo degli $\text{x}$ fino al
più grande.

\begin{eqnarray*}
   F(x) &=& 0 \quad \text{per ogni } x\le \text{x}_1\\
   F(x) &=& F(\text{x}_{j^*-1}) + \frac{x-\text{x}_{j^*}}{100}h_{j^*}\\
   F(x) &=& 1 \quad \text{per ogni } x\ge \text{x}_{K+1}\\
\end{eqnarray*}

dove $j^*$ è la classe che contiene $x$.
Se la volessimo rappresentare graficamente, nel nostro esempio sarebbe così:

```{r plot FdR}
curve(F.int(x),-1,22,n = 1001,axes=F,ylab="F(x)")
axis(1,brk)
axis(2,round(c(0,dat2$Fj),4),las=2)
segments(x0 = -10,x1 = dat2$xsup,y0 = dat2$Fj,y1 = dat2$Fj,lty=2,col="grey")
segments(x0 = dat2$xsup,x1 = dat2$xsup,y0 = 0,y1 = dat2$Fj,lty=2,col="grey")
segments(x0 = 7.2,x1 = 7.2,y0 = 0,y1 = F.int(7.2),lty=2,col=ared)
segments(x0 = -10,x1 = 7.2,y0 = F.int(7.2),y1 = F.int(7.2),lty=2,col=ared)
axis(1,7.2,col.axis =ared)
axis(2,round(F.int(7.2),4),las=2,col.axis =ared)
```



## L'inversa della Funzione di Ripartizione

La funzione di ripartizione è una funziona che crescente che vale zero quando
$x$ è il più piccolo dei dati e vale uno quando $x$ è il più grande dei dati.
\[
F:S_X\to [0,1]
\]
Definiamo $Q=F^{-1}$ la funzione inversa:
\[
Q:[0,1]\to S_X
\]
ed è tale che
\[
Q(p)=x_p:F(x_p)=p, 0\le p\le 1
\]


## Indicatori Sintetici di Centralità e di Variabilità

Un indicatore è un numero che sintetizza una caratteristica del fenomeno collettivo.
Esempi di indicatori sono: il massimo del fenomeno, il minimo del fenomeno, la media del fenomeno,
la modalità più ricorrente, ecc.

Gli **indicatori di centralità** sintetizzano l'intero fenomeno in un numero. Indicatori di che osserveremo  centralità sono:

- La **media aritmetica** (variabili quantitative) nella sezione \@ref(media)
- La **mediana** (variabili quantitative e variabili qualitative ordinate) nella sezione \@ref(mediana)
- La **moda** (ogni tipo di variabile) nella sezione \@ref(moda)

La media aritmetica è una _media analitica_ perché dipende dal valore che la variabile assume sulle unità. 
Mediana e Moda sono invece _medie lasche_ perché dipendono dall'ordinamento dei dati.

Gli **indicatori di variabilità** misurano lo scostamento del fenomeno oggetto di studio dall'indicatore di centralità. Vedremo:

- La **varianza** \@ref(var) e la **standard deviation** nella sezione \@ref(sd)
- Lo **scarto interquartile** nella sezione \@ref(sqi)



## Riepilogo

:::: {.info2 data-latex=""}
\vspace{10pt}\scriptsize
:::  {.center data-latex=""}
```{r riepilogo distr}

c1 <- c("$[\\text{x}_1,$","$[\\text{x}_2,$","$...$","$[\\text{x}_j,$","$...$","$[\\text{x}_K,$")
c2 <- c("$\\text{x}_2)$","$\\text{x}_3)$","$...$","$\\text{x}_{j+1})$","$...$","$\\text{x}_{K+1})$")
c3 <- c("$n_1$","$n_2$","$...$","$n_j$","$...$","$n_K$")
c4 <- c("$f_1=\\frac{n_1}{n}$","$f_2=\\frac{n_2}n$","$...$","$f_j=\\frac{n_j}n$","$...$","$f_K=\\frac{n_K}n$")
c5 <- c("$F_1=f_1$","$F_2=F_1+f_2$","$...$","$F_j=F_{j-1}+f_j$","$...$","$F_K=F_{K-1}+f_K$")
c6 <- c("$b_1=\\text{x}_2-\\text{x}_1$","$b_2=\\text{x}_3-\\text{x}_2$","$...$","$b_j=\\text{x}_{j+1}-\\text{x}_j$","$...$","$b_K=\\text{x}_{K+1}-\\text{x}_K$")
c7 <- c("$h_1=100\\times \\frac{ h_1}{b_1}$","$h_2=100\\times \\frac{ f_2}{b_2}$","$...$","$h_j=100\\times \\frac{ f_j}{b_j}$","$...$","$f_K=100\\times \\frac{ f_K}{b_K}$")


out <- data.frame(c1,c2,c3,c4,c5,c6,c7)

names(samp) <- nomex

if (html){
kable(out,col.names = c("Estremo inf","Estremo sup","freq. ass.","freq. relativa","freq. cum.","ampiezza","densità"),booktabs = T, escape = F,linesep = "") %>%
  column_spec(c(1,2),background = "white") %>%
  column_spec(3,background = "#d3dded")%>%
  column_spec(4,background = "white")%>%
  column_spec(5,background = "#d3dded")%>%
  column_spec(6,background = "white")%>%
  column_spec(7,background = "#d3dded")%>%
  kable_styling(font_size = 14)
}
if (!html){
kable(out,col.names = c("Estremo inf","Estremo sup","freq. ass.","freq. relativa","freq. cum.","ampiezza","densità"),booktabs = T, escape = F,linesep = "") %>%
  column_spec(c(1,2),background = "white") %>%
  column_spec(4,background = "white")%>%
  column_spec(6,background = "white")
}

```
:::  
::::
\normalsize

<!--chapter:end:02-distr-freq.Rmd-->

---
editor_options: 
  chunk_output_type: console
---
```{r 03-media-varianza-1,echo=F,include=FALSE}
#rm(list = ls())


source("intro.R")


```



# Media Aritmetica, Varianza e Standard Deviation 

 

## Media Aritmetica {#media}

La media tra due numeri $x_1$ e $x_2$ il punto centrale


\[\bar x = \frac{x_1+x_2}{2}\]

__Esempio.__ Posto $x_1=2$, $x_2=5$, allora
\[\bar x =\frac{2+5}{2}=3.5\]

:::  {.center data-latex=""}

```{r 03-media-varianza-2,echo=FALSE,fig.height=1.5}

par(mar=c(0, 3, 3, 0) + 0.1)

s2 <- 1
x1 <- -1
x2 <- +1
xm <-  0

par(cex=.5)

xlim <- c(x1-.5,x2+.5)
ylim <- c(-.5,.5)
plot(xlim,ylim,axes = F,xlab = "",ylab="",type = "n")
segments(xm,-.03,xm,.03)
arrows(x1,0,x2,0,length = .05,code = 3,angle = 90)

text(c(x1,0,x2),c(-.1,-.1,-.1),c(2,3.5,5))
par(mar=c(5, 4, 4, 2) + 0.1)
```
:::

:::: {.info data-latex=""}

:::: {.definition name="Media Aritmetica"}
Consideriamo la serie dei dati $\mathbf{x}=(x_1,...,x_i,...,x_n)$, si definisce la media aritmetica:
\[
\bar x =\frac 1 n \sum_{i=1}^nx_i 
\]
::::

::::

Ovvero la media tra $n$ numeri $x_1, x_2,...,x_n$ è definita da

\[\bar x = \frac{x_1+x_2+...+x_n}{n}\]

siccome la somma dei dati, rappresenta il totale ($Tot$) del fenomeno nel collettivo

\[Tot=x_1+x_2+...+x_n=\sum_{i=1}^nx_i \]

allora la media aritmetica

\[\bar x=\frac {Tot}n=\frac 1 n \sum_{i=1}^nx_i \]

rappresenta la quantità _ipotetica_ che ogni individuo possiederebbe se il totale fosse equi-ripartito.

### La Media Aritmetica come Baricentro dell'Istogramma

La media aritmetica tiene in equilibrio l'istogramma di densità come se si trattase di un sistema fisico. Se per esempio consideriamo 3 diverse serie di dati
\begin{eqnarray*}
  \mathbf{x}_1 &=&  (x_1 =1,x_2=2,x_3=2,x_4=3)\\
  \mathbf{x}_2 &=&  (x_1 =1,x_2=2,x_3=2,x_4=5)\\
  \mathbf{x}_3 &=&  (x_1 =1,x_2=2,x_3=2,x_4=7)
\end{eqnarray*}

E osserviamo che

\begin{eqnarray*}
  \bar x_1 &=&  \frac{1+2+2+3}{4}=2\\
  \bar x_2 &=&\frac{1+2+2+5}{4}=2.5\\
  \bar x_3 &=&\frac{1+2+2+7}{4}=3
\end{eqnarray*}

Ovvero spostando l'ultimo dato verso valori maggiori spingiamo la media su valori maggiori.
Graficamente osserviamo come la medie tenga in equilibrio l'istogramma nella figura \@ref(fig:medh).


```{r medh,fig.cap="La media artimetica tiene in equiibrio l'istogramma di densità, più ci sono dati estremi molto grandi più la media sale per mantenere l'equilibrio col totale."}

#par(mfrow=c(2,2))

layout(matrix(c(1,2,3,3),nrow = 2,byrow = T))
op <- par(mar=c(4,0,0,0)+.1,cex=.5)
x <- c(1,2,3)

plot(c(0,4),c(-1.5,2),axes = F,xlab = "",ylab="",type = "n")
rect(x-.5,0,x+.5,c(1,2,1))
axis(1,1:3,pos = 0)
arrows(2,-.5,2,-1.5,code = 1,length = .1,col=ared)

x <- c(1,2,5)

plot(c(0,6),c(-1.5,2),axes = F,xlab = "",ylab="",type = "n")
rect(x-.5,0,x+.5,c(1,2,1))
axis(1,1:5,pos = 0)
arrows(2.5,-.5,2.5,-1.5,code = 1,length = .1,col=ared)
axis(1,2.5,col=1,pos = 0)

x <- c(1,2,7)

plot(c(0,8),c(-1.5,2),axes = F,xlab = "",ylab="",type = "n")
rect(x-.5,0,x+.5,c(1,2,1))
axis(1,1:7,pos = 0)
arrows(3,-.5,3,-1.5,code = 1,length = .1,col=ared)
par(op)

layout(matrix(1))
```

Allo stesso modo se osserviamo:

\begin{eqnarray*}
  \mathbf{x}_4 &=&  (x_1 =-1,x_2=2,x_3=2,x_4=3)\\
  \mathbf{x}_5 &=&  (x_1 =-3,x_2=2,x_3=2,x_4=3),
\end{eqnarray*}

allora

\begin{eqnarray*}
  \bar x_1 &=&  \frac{1+2+2+3}{4}=2\\
  \bar x_4 &=&\frac{-1+2+2+3}{4}=1.5\\
  \bar x_5 &=&\frac{-3+2+2+3}{4}=1
\end{eqnarray*}

Ovvero spostando il primo dato verso valori minori spingiamo la media su valori minori.
Graficamente osserviamo come la media tenga in equilibrio l'istogramma nella figura \@ref(fig:medh2).

```{r 03-media-varianza-11}
fig.def(2)
```

```{r medh2,fig.cap="Analogamente se spostiamo un dato verso sinistra la media si sposta a sinistra anch'essa"}

par(mfrow=c(1,2),cex=.5)

x <- c(-1,2,3)

plot(c(-2,4),c(-1.5,2),axes = F,xlab = "",ylab="",type = "n")
rect(x-.5,0,x+.5,c(1,2,1))
axis(1,-1:3,pos = 0)
arrows(1.5,-.5,1.5,-1.5,code = 1,length = .1,col=ared)
axis(1,1.5,col=1,pos = 0)

x <- c(-3,2,3)

plot(c(-4,4),c(-1.5,2),axes = F,xlab = "",ylab="",type = "n")
rect(x-.5,0,x+.5,c(1,2,1))
axis(1,-3:3,pos = 0)
arrows(1,-.5,1,-1.5,code = 1,length = .1,col=ared)
axis(1,1,col=1,pos = 0)

par(mfrow=c(1,1),cex=cex)
```


### Calcolo per Distribuzioni di Frequenza

Se i dati sono raccolti in distribuzione di frequenza
```{r 03-media-varianza-3, echo=FALSE, results="asis"}
y1 <- c("$\\mathrm{x}_1$","$\\mathrm{x}_2$","$\\ldots$","$\\mathrm{x}_j$","$\\ldots$","$\\mathrm{x}_K$",NA)
n  <- c("$n_1$","$n_2$","$\\ldots$", "$n_j$","$\\ldots$","$n_K$","$n$")
tab <- data.frame("Modalità"=y1,"Frequenze"=n)
kbl(t(tab),booktabs = T, escape = F,linesep = "", digits = 4,col.names=NA)
```

:::: {.info data-latex=""}
:::: {.definition name="Media Artimetica per Dati Raccolti in Classi"}
\[
\bar x =\frac 1 n \sum_{j=1}^K\mathrm{x}_j n_j 
\]
::::
::::


:::: {.example}
Osserviamo i seguenti dati:
```{r 03-media-varianza-4,results='asis',echo=FALSE}
set.seed(112)
n <- 10
samp <- sample(c(2.7,3.4,5.1),n,T)
cat(paste("$x_{",1:n,"}=",samp,"$; ",sep=""))
```

La media 

\begin{eqnarray*}
\bar x &=&\frac 1 n \sum_{i=1}^nx_i \\
   &=& \frac{`r paste("x_{",1:n,"}",collapse = "+",sep="")`} {`r n`} \\
         &=& \frac{`r paste(samp,collapse = "+",sep="")`}  {`r n`} \\
         &=& \frac{`r sum(samp)`}  {`r n`}\\
         &=& `r mean(samp)`
\end{eqnarray*}


Riordiniamo i dati:
```{r 03-media-varianza-5,results='asis'}
cat(paste("$x_{(",1:5,")}=",sort(samp),"$; ",sep=""))
```


E raccogliamo in distribuzione di frequenza:
```{r 03-media-varianza-12}
y1 <- c("$\\mathrm{x}_1=2.7$","$\\mathrm{x}_2=3.4$","$\\mathrm{x}_3=5.1$",NA)
nn <-c(as.numeric(table(samp)),n)
#kbl(data.frame(modalita=y1,frequenze=n),booktabs = T, escape = F,linesep = "", digits = 4)

kable(t(data.frame(modalita=y1,frequenze=nn)),booktabs = T, escape = F,linesep = "", digits = 4,col.names = NA) %>%
  column_spec(5,border_left = T)
```

la media:

\begin{eqnarray*}
  \bar x &=&\frac 1 n \sum_{j=1}^K\mathrm{x}_j n_j \\
  &=& \frac{\mathrm{x}_1 n_1+\mathrm{x}_2 n_2+\mathrm{x}_3 n_3} n \\
         &=& \frac{2.7\times `r nn[1]`+3.4\times `r nn[2]`+5.1\times `r nn[3]`} {`r n`} \\
         &=& \frac{`r sum(samp)`} {`r n`} \\
         &=& `r mean(samp)`
\end{eqnarray*}
::::

### Proprietà della Media Aritmetica

:::: {.info data-latex=""}
:::: {.proposition name="della media aritmetica"}
Le principale proprietà della media aritmetica sono:

0. Internalità: $x_{\min} = x_{(1)} \le \bar{x} \le x_{(n)} = x_{\max}$

1. Invarianza della somma: \[n\bar x=\sum_{i=1}^n x_i\]

2. Somma degli scarti dalla media nulla: $\sum_{i=1}^{n} (x_{i} - \bar{x}) = 0$

3. Minimizza la somma degli scarti al quadrato:
\[
\sum_{i=1}^{n} (x_{i} - \bar{x})^{2} < \sum_{i=1}^{n} (x_{i} -
d)^{2} \quad \forall d \ne \bar{x}
\]

4. Invarianza per trasformazioni lineari: se $y_i=a+bx_i$ allora $\bar{y} = a + b \bar{x}$

5. Associatività. Sia una popolazione, $\mathscr{P}$, formata da $K$
gruppi con medie e numerosità: ($\bar{x}_{1};\ n_{1}$),
($\bar{x}_{2};\ n_{2}$), $\ldots$, ($\bar{x}_{K};\ n_{K}$). Allora,
la media totale $\bar{x}_{T}$ di $\mathscr{P}=$ è data da

\[
  \bar{x}_{T}
= \frac{\mbox{Tot}{ \{\mathscr{P}}_1\} +  \cdots +
        \mbox{Tot}{ \{\mathscr{P}}_K\}} {n_{1} + \cdots + n_{K}}
= \frac{n_{1}\ \bar{x}_{1} + \cdots + n_{K}\ \bar{x}_{K}}
       {n_{1} + \cdots + n_{K}}
\]
::::
::::

:::: {.proof} 
Qui di seguito le dimostrazioni

0. La proprietà di internalità deriva dal fatto che la somma dei dati è maggiore della
somma di $n$ volte del più piccolo dei dati $\sum_{i=1}^n x_i>\sum_{i=1}^n x_{(1)}$. Mentre 
$\sum_{i=1}^n x_i<\sum_{i=1}^n x_{(n)}$ la somma dei dati è maggiore della
somma di $n$ volte del più grande dei dati.

1. La proprietà di invarianza della somma la otteniamo direttamente dalla definizione di media aritmetica.
\begin{eqnarray*}
  \bar x &=&  \frac 1n\sum_{i=1}^nx_i\\
  n\bar x&=&\sum_{i=1}^nx_i.
\end{eqnarray*}

2. Somma degli scarti dalla media nulla.
Osserviamo che
\begin{eqnarray*}
  \sum_{i=1}^n(x_i-\bar x) &=&  \sum_{i=1}^n x_i-\sum_{i=1}^n\bar x\\
  &=& n\bar x -n\bar x\\
  &=&0.
\end{eqnarray*}

3. Minimizza la somma degli scarti al quadrato. Se poniamo $g(x)=\sum_{i=1}^{n} (x_{i} - x)^{2}$
osserviamo che
\begin{align*}
  g(x) &= \sum_{i=1}^n(x_i-x)^2 \\
  &=  (x_1- x)^2+...+(x_n- x)^2 && \text{La funzione $g$ è una somma di parabole}\\
  g'(x) &= -2(x_1- x)-...-2(x_n- x) && \text{Dove $g'$ indica la derivata prima di $g$}\\
  &= -2\sum_{i=1}^nx_i -2nx\\
  g'(x) &= 0 && \text{Eguagliamo $g'$ a zero per avere il minimo}\\
   -2\sum_{i=1}^nx_i -2nx&=0\\
   x&=\frac 1n\sum_{i=1}^nx_i.
\end{align*}

4. Invarianza per trasformazioni lineari: 
se $y_i=a+bx_i$ allora 

\begin{eqnarray*}
  \bar y &=& \frac 1n\sum_{i=1}^n y_i \\
  &=&  \frac 1n\sum_{i=1}^n (a+bx_i)\\
  &=& \frac 1n\sum_{i=1}^n a + \frac 1n\sum_{i=1}^n bx\\
  &=& a+b\bar x.
\end{eqnarray*}

5. Associatività. Sia una popolazione, $\mathscr{P}$, formata da $K$
gruppi con medie e numerosità: ($\bar{x}_{1};\ n_{1}$),
($\bar{x}_{2};\ n_{2}$), $\ldots$, ($\bar{x}_{K};\ n_{K}$). Allora,
il totale di tutte le popolazioni è $Tot=n_1\bar x_1+...+n_1\bar x_K$, mentre
il numero totale di individui di tutte e $K$ le popolazioni è $n_T=n_1+...+n_K$
E quindi la media

\[
  \bar{x}_{T}
= \frac{\mbox{Tot}{ \{\mathscr{P}}_1\} +  \cdots +
        \mbox{Tot}{ \{\mathscr{P}}_K\}} {n_{1} + \cdots + n_{K}}
= \frac{n_{1}\ \bar{x}_{1} + \cdots + n_{K}\ \bar{x}_{K}}
       {n_{1} + \cdots + n_{K}}
\]
::::


## La varianza {#var}

La media riduce un complesso di $n$ dati in uno solo. A parità di media
i dati possono essere molto diversi tra di loro. Per esempio le due serie di dati

\begin{eqnarray*}
  \mathbf{x}_1 &=&  (x_1 =2,x_2=2,x_3=2,x_4=2)\\
  \mathbf{x}_2 &=&  (x_1 =1,x_2=2,x_3=2,x_4=3)\\
  \mathbf{x}_3 &=&  (x_1 =0,x_2=0,x_3=0,x_4=8)
\end{eqnarray*}

hanno tutte la stessa media $\bar x_1=\bar x_2 =\bar x_3= 2$, ma nel primo caso tutti possiedono
la media, nel secondo chi poco e chi tanto, nel terzo caso uno possiede il totale e gli altri 3 nulla.

La varianza misura la distanza dei dati dalla media.

:::: {.info data-latex=""}
:::: {.definition name="Varianza"}
Si definisce la varianza la quantità:
\[
\sigma^2=\frac 1 n \sum_{1=1}^n(x_i-\bar x)^2
\]
::::
::::

La varianza misura lo scostamento medio quadratico dei dati dalla media aritmetica;
ovvero è la media del quadrato degli scarti.

Con un po' di algebra si dimostra che

:::: {.info data-latex=""}
:::: {.proposition name="Formula Calcolatoria della Varianza"}
\[
\sigma^2=\frac 1 n \sum_{1=1}^n x_i^2 -\bar x^2
\]
::::
::::

:::: {.proof}
\begin{eqnarray*}
   \sigma^2 &=&  \frac 1 n \sum_{1=1}^n(x_i-\bar x)^2\\
   &=& \frac 1n\sum_{1=1}^n(x_i^2+\bar x^2-2\cdot x_i\cdot \bar x)\\
   &=& \frac 1n\sum_{1=1}^n x_i^2+\frac 1n\sum_{1=1}^n \bar x^2-\frac 1n\sum_{1=1}^n2\cdot x_i\cdot \bar x\\
   &=& \frac 1n\sum_{1=1}^n x_i^2+\frac nn \bar x^2-\frac 2n\bar x \cdot n\cdot \bar x\\
   &=& \frac 1 n \sum_{1=1}^n x_i^2 -\bar x^2
\end{eqnarray*}
::::

Quindi la varianza si può calcolare o come media del quadrato degli scarti dalla media o come media dei quadrati meno il quadrato della media.

:::: {.example}
Posto $x_1=2$, $x_2=2$, $x_3=2$, $x_4=2$ allora
\[\bar x =\frac{2+2+2+2}{4}=2\]

\[
\sigma^2=\frac {(2-2)^2+(2-2)^2+(2-2)^2+(2-2)^2}{4}=0
\]

Tutti gli individui hanno la stessa quantità che è pari alla media, non c'è variabilità, la varianza vale zero.
::::

:::: {.example}
Posto $x_1=1$, $x_2=2$, $x_3=2$, $x_4=3$ allora
\[\bar x =\frac{2+2+2+2}{4}=2\]

\[
\sigma^2=\frac {(1-2)^2+(2-2)^2+(2-2)^2+(3-2)^2}{4}=0.5
\]

Non tutti gli individui hanno la stessa quantità, c'è variabilità, la varianza è diversa da zero.
::::

:::: {.example}
Posto $x_1=0$, $x_2=0$, $x_3=0$, $x_4=8$ allora
\[\bar x =\frac{0+0+0+8}{4}=2\]

\[
\sigma^2=\frac {(0-2)^2+(0-2)^2+(0-2)^2+(8-2)^2}{4}=12
\]

Tutto il totale è posseduto da un solo individuo, c'è massima variabilità.
::::

### Calcolo per Distribuzioni di Frequenza

Se i dati sono raccolti in distribuzione di frequenza

```{r 03-media-varianza-6, echo=FALSE, results="asis"}
y1 <- c("$\\mathrm{x}_1$","$\\mathrm{x}_2$","$\\ldots$","$\\mathrm{x}_j$","$\\ldots$","$\\mathrm{x}_K$",NA)
nn  <- c("$n_1$","$n_2$","$\\ldots$", "$n_j$","$\\ldots$","$n_K$","$n$")
tab <- data.frame("Modalità"=y1,"Frequenze"=nn)
kbl(t(tab),booktabs = T, escape = F,linesep = "", digits = 4,col.names = NA)%>%
  column_spec(8,border_left = T)
# kable(tab,"html")  %>%
# kable_styling(bootstrap_options = c("striped", "hover"),full_width = FALSE)

```
la varianza si può calcolare

:::: {.info data-latex=""}
:::: {.proposition name="Varianza per Dati in Distribuzione di Frequenza"}
\[
\sigma^2=\frac 1 n\sum_{j=1}^k(\mathrm{x}_j-\bar x)^2n_j
\]
::::
::::

e di conseguenza, con un po' di algebra otteniamo:

:::: {.info data-latex=""}
:::: {.proposition name="Formula Calcolatoria per la Varianza per Dati in Distribuzione di Frequenza"}
\[
\sigma^2=\frac 1 n\sum_{j=1}^k\mathrm{x}_j^2n_j-\bar x^2
\]
::::
::::


:::: {.example}

```{r 03-media-varianza-7,results='asis',echo=FALSE}
set.seed(112)
n <- 10
samp <- sample(c(2.7,3.4,5.1),n,T)
x <- samp
m <- mean(x)
cat(paste("$x_{",1:n,"}=",samp,"$; ",sep=""))
```

La media 

\begin{eqnarray*}
\bar x &=&\frac 1 n \sum_{i=1}^nx_i \\
   &=& \frac{`r paste("x_{",1:n,"}",collapse = "+",sep="")`} {`r n`} \\
         &=& \frac{`r paste(samp,collapse = "+",sep="")`}  {`r n`} \\
         &=& \frac{`r sum(samp)`}  {`r n`}\\
         &=& `r mean(samp)`
\end{eqnarray*}


La varianza `r sq <- (samp-mean(samp))^2; m<- mean(samp); x<- samp`

\begin{eqnarray*}
  \sigma^2 &=& \frac 1 n \sum_{1=1}^n(x_i-\bar x)^2\\
   &=& \frac{`r paste("(x_{",1:n,"}-\\bar x)^2",collapse = "+",sep="")`} {`r n`} \\
         &=& \frac{`r paste(paste("(",samp,"-",mean(x),")^2"),collapse = "+",sep="")`}  {`r n`} \\
         &=& \frac{`r paste(round((samp-mean(samp))^2,4),collapse = "+",sep="")`}  {`r n`} \\
         &=& \frac{`r sum((samp-mean(samp))^2)`}  {`r n`}\\
         &=& `r s2c(samp)`
\end{eqnarray*}

Osserviamo che

\begin{eqnarray*}
  \sigma^2 &=& \frac 1 n\sum_{i=1}^nx_i^2-\bar x^2\\
  &=&\frac{1} {`r n`} (`r paste(paste("x_{",1:n,"}^2",sep=""),collapse="+")`) -\bar x^2 \\
         &=& \frac1 {`r n`}(`r paste(x,"^2",collapse="+")`)-`r m`^2\\
         &=& \frac1 {`r n`}(`r paste(x^2,collapse="+")`)-`r m^2`\\
         &=& `r sum(x^2)/n` -`r m^2`\\
         &=& `r sum(x^2)/n-m^2`
\end{eqnarray*}


Riordiniamo i dati:
```{r 03-media-varianza-8,results='asis'}
cat(paste("$x_{(",1:n,")}=",sort(samp),"$; ",sep=""))
```


E raccogliamo in distribuzione di frequenza:
```{r 03-media-varianza-13}
y1 <- c("$\\mathrm{x}_1=2.7$","$\\mathrm{x}_2=3.4$","$\\mathrm{x}_3=5.1$",NA)
nn <-c(as.numeric(table(samp)),n)
#kbl(data.frame(modalita=y1,frequenze=n),booktabs = T, escape = F,linesep = "", digits = 4)



kable(t(data.frame(modalita=y1,frequenze=nn)),booktabs = T, escape = F,linesep = "", digits = 4,col.names = NA) %>%
  column_spec(5,border_left = T)
```

la media:

\begin{eqnarray*}
  \bar x &=&\frac 1 n \sum_{j=1}^K\mathrm{x}_j n_j \\
  &=& \frac{\mathrm{x}_1 n_1+\mathrm{x}_2 n_2+\mathrm{x}_3 n_3} n \\
         &=& \frac{2.7\times `r nn[1]`+3.4\times `r nn[2]`+5.1\times `r nn[3]`} {`r n`} \\
         &=& \frac{`r sum(samp)`} {`r n`} \\
         &=& `r mean(samp)`
\end{eqnarray*}

la varianza:

\begin{eqnarray*}
  \sigma^2 &=& \frac 1 n\sum_{j=1}^k(\mathrm{x}_j-\bar x)^2n_j\\
  &=&\frac 1 {`r n`}\left((\mathrm{x}_1 -\bar x)^2n_1+(\mathrm{x}_2-\bar x)^2 n_2+(\mathrm{x}_3-\bar x)^2 n_3\right)  \\
         &=& \frac1 {`r n`}\left((2.7-`r m`)^2\times `r nn[1]`+(3.4-`r m`)^2\times `r nn[2]`+(5.1-`r m`)^2\times `r nn[3]`\right)  \\
         &=& \frac{`r sum((samp-m)^2)`} {`r n`}\\
         &=& `r s2c(samp)`
\end{eqnarray*}


o alternativamente

\begin{eqnarray*}
  \sigma^2 &=& \frac 1 n\sum_{j=1}^k\mathrm{x}_j^2n_j-\bar x^2\\
  &=&\frac 1 {`r n`} (\mathrm{x}_1^2n_1+\mathrm{x}_2^2n_2+\mathrm{x}_3^2n_3)-\bar x^2\\
         &=& \frac 1 {`r n`} (`r paste(c(2.7,3.4,5.1),"^2\\times",nn[-4],sep="",collapse="+")`)-`r m`^2\\
         &=& \frac 1 {`r n`}\times `r sum(x^2)`-`r m^2`\\
         &=& `r s2c(x)`
\end{eqnarray*}
::::

:::: {.example}

```{r 03-media-varianza-9, results='asis'}
set.seed(12)
x <- round(sample(sqrt(c(10:15)),size=21,T,prob=dnorm(10:15,13,2)),2)
cat(paste("$x_{",1:21,"}=",x,"$; ",sep=""))
```

La media:

\begin{eqnarray*}
\bar x &=& \frac 1 n \sum_{i=1}^n x_i \\
       &=& \frac 1 {21} `r sum(x)`\\
       &=& `r mean(x)`
\end{eqnarray*}

La varianza 

\begin{eqnarray*}
\sigma^2 &=& \frac 1 n \sum_{i=1}^n x_i^2 - \bar x ^2 \\
       &=& \frac 1 {21} `r sum(x^2)` - `r mean(x)`^2\\
       &=& `r mean(x^2)-mean(x)`
\end{eqnarray*}

riordiniamo i dati

```{r 03-media-varianza-10,results='asis'}
cat(paste("$x_{(",1:21,")}=",sort(x),"$; ",sep=""))
```

E raccogliamo in distribuzione di frequenza:

```{r 03-media-varianza-14}
y1 <- c(paste("$\\mathrm{x}_",1:6,"=",round(sqrt(10:15),2),"$",sep=""),"Tot")
n <-c(as.numeric(table(x)),21)
nx <- c(n[-7]*(10:15),sum(n[-7]*(10:15)))
nx2 <- c(n[-7]*(10:15)^2,sum(n[-7]*(10:15)^2))
stm <-  data.frame(modalita=y1,frequenze=n,nx=nx,nx2)
names(stm) <- c("$\\mathrm{x}_j$","$n_j$","$\\mathrm{x}_j n_j$","$\\mathrm{x}_j^2 n_j$")
kable(t(stm),booktabs = T, escape = F,linesep = "", digits = 4,col.names = NA,align = 'r') %>%
  column_spec(8,border_left = T)
```

E osserviamo che

\begin{eqnarray*}
\bar x &=& \frac 1 n \sum_{j=1}^k \mathrm{x}_j n_j \\
       &=& \frac 1 {21} `r sum(x)`\\
       &=& `r mean(x)`
\end{eqnarray*}

e che

\begin{eqnarray*}
\sigma^2 &=& \frac 1 n \sum_{j=1}^n \mathrm{x}_j^2 n_j - \bar x ^2 \\
       &=& \frac 1 {21} `r sum(x^2)` - `r mean(x)`^2\\
       &=& `r mean(x^2)-mean(x)`
\end{eqnarray*}
::::

### Proprietà della Varianza

:::: {.info data-latex=""}
:::: {.proposition name="della varianza $\\sigma^{2}$"}
Le principale proprietà della varianza sono:

1.  $\sigma^{2} \ge 0$.
2.  $\sigma^{2}=0$, se e solo se $X$ è costante.
3.  Se $y_i=a+bx_i$ allora $\sigma^{2}_Y = b^{2} \sigma^{2}_X$.
::::
::::

:::: {.proof} 
Le dimostrazioni qui di seguito.

1. $\sigma^{2} \ge 0$ deriva direttamente dalla definizione, essendo $\sigma^{2}$
la media di scarti al quadrato e quindi di quantità positive, non potrà _mai_ essere negativa.

2. $\sigma^{2}=0$ solo se ogni scarto dalla media è zero e questo può avvenire solo se tutti
i dati sono uguali alla media, ovvero se i dati sono tutti uguali tra di loro e quindi non variano.

3. Se $y_i=a+bx_i$ allora 

\begin{eqnarray*}
\sigma^{2}_Y   &=&  \frac 1n \sum_{i=1}^n(y_i-\bar y)^2\\
&=& \frac 1n \sum_{i=1}^n(a+bx_i-(a+b\bar x))^2\\
&=& b^2\frac 1n \sum_{i=1}^n(x_i-\bar x)^2\\
&=& b^2\sigma_X^2
\end{eqnarray*}
  
::::


## La Standard Deviation {#sd}

La varianza non ha un'unità di misura leggibile, è una media di quadrati degli scarti. E quindi anche l'unità di misura è elevata al quadrato

Si definisce la _standard deviation_ (deviazione standard, scarto quadratico medio), la radice della varianza
\[
\sigma=\sqrt{\sigma^2}
\]

### Proprietà della Standard Deviation

:::: {.info data-latex=""}
:::: {.proposition name="della deviazione standard $\\sigma$"}
Le principale proprietà della deviazione standard sono:

1.  $\sigma\ge 0$.
2.  $\sigma=0$, se e solo se $X$ è costante.
3.  Se $y_i=a+bx_i$ allora allora $\sigma_Y = |b|\sigma_X$

::::
::::

Se la distribuzione della $X$ è abbastanza simmetrica e di forma campanulare, allora

\[
\%(\bar x-2\sigma\le X\le \bar x+2\sigma)\approx 95\%
\]

```{r 03-media-varianza-15}
fig.def()
```

```{r sd,echo=FALSE,fig.cap="Interpretazione della SD"}
set.seed(11)
samp <- rnorm(100000)
brk  <- seq(-3.5,3.5,by=.5)
k <- length(brk)-1
br1  <- brk[-(k+1)]
br2  <- brk[-1]
ls2e(stat_base(samp,brk))
dat3$`$f_{j\\%}$` <- dat3$`$f_j$`*100

 plot(range(brk),range(c(0,dat3$`$h_j$`),na.rm = T),type="n",axes=F,xlab = "",ylab = "")
 rect(xleft = br1,ybottom = 0,xright = br2,ytop = dat3$`$h_j$`)
#axis(2)
int <- c(mean(samp)-2*sd(samp),mean(samp),mean(samp)+2*sd(samp))
axis(1,int,c("media-2SD","media","media+2SD"))
rect(xleft = br1[br1 >= -2 & br1 < 2],ybottom = 0,xright = br2[br2 > -2 & br2 <= 2],ytop = dat3$`$h_j$`[br2 > -2 & br2 <= 2],density = 20,col=4)
text(0,4.5,"95%",cex=5,pos=3)
text(-3,4.5,"2.5%",cex=2.8,pos=3)
text(+3,4.5,"2.5%",cex=2.8,pos=3)
```

la figura La figura \@ref(fig:sd) la corrispondente rappresentazione grafica.

## Esempi

::::{.example}

Si si è chiesta l'età a 37500 uomini e 38100 donne di un determinato collettivo, ed è risultato 
che gli uomini di quel collettivo hanno un'età media di 45 anni e le donne un'età media di 49 anni. La sintesi dei dati qui di seguito:

```{r 03-media-varianza-16}

nn <- c(37500,38100)
em <- c(45,49)
stm <- data.frame(nj=nn,mj=em)
row.names(stm)<- c("Uomini","Donne")
kable(stm,booktabs = T, escape = F,linesep = "", digits = 4,col.names = c("$n_j$","$\\bar x_j$")) %>%
    kable_styling(full_width = F, latex_options = "HOLD_position")
```
<!-- \begin{tabbing} -->
<!-- %\begin{normalsize} -->
<!-- \begin{tabular}{l|lrrrcccccccc} -->
<!-- \hline\\ -->
<!--  &Genere  &$n_{j}$  &età media \\ -->
<!-- \hline\\ -->
<!--         &Uomo    &37500    &45 \\ -->
<!--         &Donna   &38100    &49 \\ -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- \end{tabbing} -->

Calcolare l'età media dell'intero collettivo.

__Soluzione.__ 
L'età media per l'intera popolazione è
\begin{displaymath}
  \bar{x}_{a}
= \frac{37500 \times 45 + 38100 \times 49} {37500 + 38100}
= 47.02 .
\end{displaymath}
::::

:::: {.example}
Uno studente iscritto al secondo anno di un CdL, ha superato 7
esami con un voto medio pari a 26/30. Sostiene un nuovo esame
ottenendo un voto pari a 28/30. Qual è il voto medio dopo
l'ottavo esame?

__Soluzione.__ 
Sia $\bar{x}_{7}=26$ il voto medio dopo i primi 7 esami.
Sia $\bar{x}_{8}$ il voto medio dopo l'8$^{o}$ esame.
\begin{eqnarray*}
    \bar{x}_{7}
&=& \frac{1} {7} \sum_{i=1}^{7} x_{i}  \\
    \bar{x}_{8}
&=& \frac{1} {8} \sum_{i=1}^{8} x_{i}
 =  \frac{1} {8} \left( \sum_{i=1}^{7} x_{i} + x_{8} \right)   \\
&=& \frac{1} {8} \left( 7 \bar{x}_{7} + x_{8} \right)
 =  \frac{7  \times  \bar{x}_{7} + 1  \times  x_{8}} {8} \\
&=& \frac{7 \times  26 + 1  \times  28} {8}
 =  \frac{182 + 28} {8}
 =  26.25  .
\end{eqnarray*}
::::


:::: {.example}
In una contrattazione sindacale, il rappresentante
del governo propone di alzare di un ammontare fisso di 100 euro
lo stipendio degli impiegati statali.

a. Come cambierebbero lo stipendio medio e la varianza se questa misura fosse
intrapresa?

b. Se il governo aumentasse lo stipendio di ciascun
impiegato statale del 5\%, come cambierebbe lo stipendio medio e la varianza?


__Soluzione.__

a. Sia $x$ lo stipendio degli statali. 
 
\begin{eqnarray*}
y_{i}   &=& x_{i} + 100   \\
\bar{y} &=& \bar{x} + 100   .\\
\sigma_Y^2=\sigma_X^2
\end{eqnarray*}
 
Stipendio medio aumenta esattamente di 100\euro, la varianza non cambia.

b.  Aumento percentuale pari al 5\%.
 
\begin{eqnarray*}
y_{i}   &=& x_{i} + \frac{5} {100}\ x_{i} = 1.05\ x_{i}  \\
\bar{y} &=& 1.05 \bar{x}  .\\
\sigma_Y^2=(1.05)^2\sigma_X^2
\end{eqnarray*}
 
La MEDIA aumenta del 5\%, la varianza aumenta in modo quadratico.
::::

::::{.example}
La spesa per le vacanze estive (in migliaia di euro) sostenuta
da 12 famiglie di un condominio è stata di:

```{r 03-media-varianza-17}
x <- c(0 , 0 , 2 , 2.5 , 4 , 5.1 , 5.8 , 6 , 7 , 12 , 15 , 21)
```

\begin{tabbing}
\begin{tabular}{cccccccccccc}
\hline\\
0 &0 &2 &2.5 &4 &5.1 &5.8 &6 &7 &12 &15 &21 \\
\hline
\end{tabular}
%\end{tiny}
\end{tabbing}

(a) Determinare la spesa media e la varianza per famiglia.

\begin{eqnarray*}
    \bar{x}
&=& \frac{0 +0 + 2+ 2.5 +4 +5.1 +5.8 +6} {12} + \ldots \\
&+& \frac{7 +12 +15 +21} {12}
 =  \frac{80.4} {12} = 6.7 \mbox{\euro}. \\
 \sigma_X^2 &=& \frac{0^2+0^2+2^2+...+21^2}{12}-(6.7)^2\\
 &=& `r s2c(x)`
\end{eqnarray*}

(b) Determinare la spesa media per famiglie con spesa $\ne 0$.

\begin{eqnarray*}
    \bar{x}
&=& \frac{2+ 2.5 +4 +5.1 +5.8 +6 +7 +12 +15 +21} {10} \\
&=& \frac{80.4} {10} = 8.04 \mbox{\euro}.\\
 \sigma_{X}^2 &=& \frac{2^2+...+21^2}{10}-(8.04)^2\\
 &=& `r s2c(x[x>0])`
\end{eqnarray*}
::::

::::{.example}

```{r 03-media-varianza-18}
i2 <- 0
# Basic settings
set.seed(121)   # per ottenere sempre la stessa simulazione
n <- 115      # ampiezza campionaria
a <- 5
b <- 1

brk  <- c(0,1,5,10,20,30)
nnn  <- c( 7,18,45, 25,20)
nnn  <- round(nnn/sum(nnn)*n)
k <- length(brk)-1
br1  <- brk[-(k+1)]
br2  <- brk[-1]
vunif <- function(nnn, brk){
  k <- length(brk)-1
  br1  <- brk[-(k+1)]
  br2  <- brk[-1]
  xi   <- runif(nnn[1],br1[1],br2[1])
  for (i in 2:k)
    xi <- c(xi,runif(nnn[i],br1[i],br2[i]))
  return(xi)
}

samp <- round(vunif(nnn,brk),2)
nomex <- "Utile"

names(samp) <- nomex

ls2e(stat_base(samp,brk))
dat3$`$f_{j\\%}$` <- dat3$`$f_j$`*100
dat3[k+1,1]<- "Totale"
# kable((dat3[1:(k+1),c(1:2,7)])) %>%
# 
n <- length(samp)
```

Numero di impiegati per anni di servizio di una industria


```{r 03-media-varianza-19}
kable((dat3[1:(k+1),c(1:4,7:11)]),booktabs=T,escape = F,linesep="",row.names = F)
```


Determinare media, mediana e varianza dell'età di servizio dell'industria. 

\begin{eqnarray*}
    \bar{x}
&=& \frac{0.5 \times 7 + 3.0 \times 18 + \cdots + 20 \times 25}
    {7 + 18 + \cdots + 20}
 =  \frac{1270} {115} = 11.04                       \\
    x_{0.5}
&=& x_{m;\ inf} + \frac{0.5 - F_{m-1}} {F_{m} - F_{m-1}}   (x_{m;\ sup} - x_{m;\ inf})                          \\
&=& 5 + \frac{0.5 - 0.22} {0.61 - 0.22}\ (10 - 5) = 8.59 \\
\sigma_X^2 &=& \frac{1}{115}`r sum(dat2$x2n)`-(11.04)^2\\
&=& `r sum(dat2$x2n)/115-sum(dat2$xn)/115`
\end{eqnarray*}
::::

<!--chapter:end:03-media-varianza.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r s03,echo=FALSE,include=FALSE}
#rm(list = ls())



source("intro.R")

s2 <- 0
```

# Mediana, Percentili e Moda

## La Mediana {#mediana}

La _Mediana_, _Me_ o $x_{0.5}$, è il valore centrale della serie dei dati riordinati in due: metà dei dati sono minori della mediana e metà dei dati sono maggiori della mediana. 
In simboli: sia $x_{1},...,x_{n}$ la serie dei dati e $x_{(1)},...,x_{(n)}$ i dati riordinati in modo crescente, allora:

:::: {.info data-latex=""}
1. se $n$ è dispari
\[x_{0.5}=x_{\left(\frac{n+1}{2}\right)}\]
2. se $n$ è pari
\[x_{0.5}=\frac 1 2 \left(x_{\left(\frac{n}{2}\right)}+x_{\left(\frac{n}{2}+1\right)}\right)\]
:::: 

:::: {.example}

Sia 
$$(x_1=2.9,x_2=3.5,x_3=1.2,x_4=2.7, x_5=4.2)$$ 
la serie dei dati, la serie dei dati riordinati sarà 
$$(x_{(1)}=1.2, x_{(2)}=2.7, x_{(3)}=2.9, x_{(4)}=3.5,x_{(5)}=4.2),$$ 

$n=5$ è dispari e dunque

\[x_{0.5}=x_{\left(\frac{n+1}{2}\right)}=x_{\left(\frac{5+1}{2}\right)}=x_{\left(3\right)}=2.9\]
::::

:::: {.example}

Sia 
\[(x_1=2.9,x_2=3.5,x_3=1.2,x_4=2.7, x_5=4.2, x_6=4.2)\] 
la serie dei dati, la serie dei dati riordinati sarà 
\[(x_{(1)}=1.2, x_{(2)}=2.7, x_{(3)}=2.9, x_{(4)}=3.5,x_{(5)}=4.2,x_{(6)}=4.2)\] 

$n=6$ è pari e dunque

\[x_{0.5}=\frac 1 2 \left(x_{\left(\frac{n}{2}\right)}+x_{\left(\frac{n}{2}+1\right)}\right)=
\frac 1 2 \left(x_{\left(\frac{6}{2}\right)}+x_{\left(\frac{6}{2}+1\right)}\right)=
\frac 1 2 \left(x_{\left(3\right)}+x_{\left(4\right)}\right)=\frac{2.9+3.5}2=3.2\]
::::

### Dati espressi in distribuzione di frequenza
Se il fenomeno è espresso in una tabella di distribuzione di frequenza, allora la _modalità mediana_ è la prima modalità tale per cui la frequenza cumulata è maggiore di 0.5


:::: {.example}
Fenomeno _Titolo di Studio_, $n=350$, numero di modalità $k=5$.

```{r titolo di studio,echo=F}
f    <- c(0.10,0.30,0.42,0.10,0.08)
n <- 350
Fj <- cumsum(f)
xdata <- c("Elementare","Media inferiore","Media Superiore","Laurea","Post Laurea")
X <- data.frame(1:length(f),xdata,f*n,f,Fj)
Y <- X

names(Y) <- c("$j$","$x_{j}$","$n_j$","$f_j$","$F_j$")



kable(Y[,],booktabs = T, escape = F,linesep = "", digits = 4) %>%
    kable_styling(full_width = F, latex_options = "HOLD_position")
```

La modalità mediana è la terza $j=3$, dunque _Media Superiore_, infatti $F_3=0.82>0.50$.
::::

### Dati espressi in classi 

Se il fenomeno è espresso in classi, allora l'_intervallo mediano_ è la primo intervallo tale per cui la frequenza cumulata è maggiore di 0.5.

:::: {.example}

```{r dati in classi,echo=F,include=F}
n <- 4700
xlim <- brk <- c(0,10,15,20,25,35)
k <- length(xlim) - 1
f    <- c(0.11,0.27,0.33,0.21,.08); # sum(f)

samp <- genera_dati(xlim,n = n,nnn = f*n,rand = F)
ls2e(stat_base(samp,brk))
```

Il reddito di $n=$ 4700 famiglie 
è rappresentato nella seguente tabella di frequenza
```{r dati, echo=FALSE, results="asis"}
tabl(cbind(`$j$`=c(1:k,NA),dat3[,1:7]))
```

La classe mediana è la terza classe $j=3$, ovvero la classe [15,20), in quanto $F_3=0.71>0.50$.
::::

Il _valore approssimato della mediana_ è un valore che si trova all'interno dell'intervallo mediano e si ottiene dalla formula

:::: {.info data-latex=""}
\[x_{0.5}=x_{\inf;m}+\frac{0.5-F_{m-1}}{f_m}\cdot \left(x_{\sup;m}-x_{\inf;m} \right),\]
::::

dove $m$ è l'indicatore della classe mediana, $x_{\inf;m}$ e $x_{\sup;m}$ sono, rispettivamente l'estremo inferiore e quello superiore dell'intervallo che contiene la mediana.

:::: {.example}
Nell'esempio precedente l'intervallo mediano è [15,20) otterremo:

```{r 04-mediana-percentili-15}
percentile()
```

::::

:::: {.nota data-latex=""}
La mediana è quel valore che taglia l'istogramma in due parti, entrambe di area pari al 50% dell'area totale


```{r 04-mediana-percentili-1, echo=FALSE}
nomex <- "Reddito"

histp(T)
h.int(0,Q.int(.5),density=20,col=iblue,lwd=1.5)
axis(side=1,at=round(Q.int(0.5),3),col=ared,las=3)
text(Q.int(.5)-13,3.5,"50%",cex=2,col=iblue)
text(Q.int(.5)+13,3.5,"50%",cex=2,col=iblue)

Fj <- c(0,dat2$Fj)
plot(brk,Fj,type="l",axes=F,xlab=nomex,ylab=expression(F[j]),xlim=c(min(brk)-1,max(brk)+1))
abline(h=0:1,lty=2)
axis(1,brk,pos = 0)
axis(2,Fj,pos = 0,las=2)
segments(brk,0,brk,Fj,lty=2)
segments(0,Fj,brk,Fj,lty=2)
segments(Q.int(.5),0,Q.int(.5),.5,lty=2,col=ared)
segments(0,0.5,Q.int(.5),.5,lty=2,col=ared)
axis(1,Q.int(.5),round(Q.int(.5),3),col.axis=ared,las=3,pos = 0)
axis(2,.5,col.axis=ared,las=2,pos = 0)
```

L'area tratteggiata è il 50% dell'area totale.
::::

### Proprietà della Mediana 

:::: {.info}
::: {.proposition name="della Mediana"}
La mediana di una distribuzione, $x_{0.5}$, è quel valore della per
$X$ il quale si ha $F(x_{0.5}) = 0.5$. Le proprietà della mediana
($x_{0.5}$) sono:

1.  $x_{\min} \leq x_{0.5} \leq x_{\max}$,
2.  $\sum_{j=1}^{n} |x_{j} - x_{0.5}|$ è un minimo.
3.  Relazione Media-Mediana:
    - Distribuzione simmetrica $\rightarrow$ $x_{0.5} = \bar{x}$ 
    - Distribuzione con coda lunga a destra $\rightarrow$ $x_{0.5} < \bar{x}$ 
    - Distribuzione con coda lunga a sinistra $\rightarrow$ $x_{0.5} > \bar{x}$ 
:::
::::

## I Percentili

Il $p$-esimo percentile $x_p$, $0\leq p\leq 1$, è qual valore che divide la serie dei dati riordinati in due: il $p\times100\%$ dei dati sono minori di $x_p$ e $(1-p)\times100\%$ dei dati sono maggiori di $x_p$. 
Se per esempio $p=0.30$ allora il trentesimo percentile è quel valore che ha il 30% dei dati inferiore il 70% dei dati superiore.
Il $p$-esimo percentile di una serie di dati è il valore che occupa la posizione $\lfloor {p\times n}\rfloor+1$, dove $\lfloor x\rfloor$ è l'operatore che estrae la parte intera di un numero decimale, ad esempio $\lfloor 3.001\rfloor=\lfloor 3.21\rfloor=\lfloor 3.94\rfloor=3$.

:::: {.example}
Si considerino $n=21$ osservazioni di una variabile categoriale ordinata che assume 7 valori: $-2$, $-1$, $0$, $1$, $2$ (ad esempio una scala del tipo $-2=$_in totale disaccordo_, $-1=$_più in disaccordo che in accordo_, $0=$_né d'accordo, né in disaccordo_, $1=$_più d'accordo che in disaccordo_, $2=$_totalmente d'accordo_). Qui di seguito i dati riordinati:

```{r 04-mediana-percentili-2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=4)

set.seed(4)
n <- 21
lev <- -2:2
xdat <- sort(sample(lev,size = n,replace = T))
ind <- 1:n
#ind <- paste("(",ind,")",sep = "")
tab <- data.frame(ind,xdat)
names(tab) <- c("$(i)$","$x_i$")
```


```{r 04-mediana-percentili-3, echo=FALSE, results="asis"}
kable(t(tab),booktabs = T, escape = F,linesep = "", digits = 4)
```

Il 15-esimo percentile è il dato che occupa la $\lfloor n\times p \rfloor+1 =\lfloor `r n`\times 0.15\rfloor+1= `r floor(0.15*n)`+1=`r floor(0.15*n)+1`$, e dunque il 15-esimo percentile è il quarto dato $x_{(`r floor(0.15*n)+1`)}=`r xdat[floor(.15*n)+1]`$.
È chiaro che la mediana è il 50-esimo percentile. In questo caso $x_{0.5}=x_{\left(\lfloor`r n`\times 0.5\rfloor +1\right)}=x_{(11)}=0$.
::::

### Dati espressi in distribuzione di frequenza

Se il fenomeno è espresso in una tabella di distribuzione di frequenza, allora il $p$-esimo percentile è la prima modalità tale per cui la frequenza cumulata è maggiore di $p$.

:::: {.example}

Fenomeno _Titolo di Studio_, $n=350$, numero di modalità $k=5$.

```{r 04-mediana-percentili-4,echo=F}
f    <- c(0.10,0.30,0.42,0.10,0.08)
n <- 350
Fj <- cumsum(f)
xdata <- c("Elementare","Media inferiore","Media Superiore","Laurea","Post Laurea")
X <- data.frame(1:length(f),xdata,round(f*n),f,Fj)
Y <- X

names(Y) <- c("$j$","$x_{j}$","$n_j$","$f_j$","$F_j$")



kable(Y[,],booktabs = T, escape = F,linesep = "", digits = 4)
```

Il 90-esimo percentile $x_{0.90}$ è la quarta modalità, $x_{4}=$Laurea.
::::

:::: {.example}
Fenomeno: Numero di volte che si è cercato lavoro negli ultimi 3 mesi, $n=322$

```{r 04-mediana-percentili-5,echo=F}
x    <- 0:10
n <- 322
nj    <- rpois(n,3)

set.seed(1)
Xt <- data.frame(table(nj))
f <- Xt$Freq/n
Fj <- cumsum(f)
X <- data.frame(1:length(f),Xt,f,Fj)
Y <- X

names(Y) <- c("$j$","$x_{j}$","$n_j$","$f_j$","$F_j$")



kable(Y[,],booktabs = T, escape = F,linesep = "", digits = 4) %>%
  column_spec (1,border_left = T, border_right = T) %>%
  kable_styling(full_width = F, latex_options = "HOLD_position")
```

Il 25-esimo percentile è la terza modalità in quanto $F_3=0.45>0.25$, $x_{0.25}=x_3=2$. Il 50-esimo percentile, la mediana, è $x_{0.5}=x_4=3$ e il 75-esimo percentile è $x_{0.75}=x_5=4$.
::::

### Dati espressi in classi 

Se il fenomeno è espresso in classi, allora l'_intervallo che contiene il $p$-esimo percentile_ è il primo intervallo tale per cui la frequenza cumulata è maggiore di $p$. Il _valore approssimato del percentile_ è un valore che si trova all'interno dell'intervallo e si ottiene dalla formula

:::: {.info data-latex=""}
\[x_{p}=x_{\inf;j_p}+\frac{p-F_{j_p-1}}{f_{j_p}}\cdot \left(x_{\sup;j_p}-x_{\inf;j_p} \right)\]
::::
dove $j_p$ è l'indicatore della classe che contiene il $p$-esimo percentile, $x_{\inf;j_p}$ e $x_{\sup;j_p}$ sono, rispettivamente, l'estremo inferiore e quello superiore.

:::: {.example}

```{r 04-mediana-percentili-6,echo=F,include=F}
n <- 4700
xlim <- brk <- c(0,10,15,20,25,35)
k <- length(xlim)-1
f    <- c(0.11,0.27,0.33,0.21,.08); # sum(f)

samp <- genera_dati(xlim,n = n,nnn = f*n,rand = F)
ls2e(stat_base(samp,brk))
```

Il reddito di $n=$ 4700 famiglie 
è rappresentato nella seguente tabella di frequenza
```{r 04-mediana-percentili-7, echo=FALSE, results="asis"}
tabl(cbind("$j$"=c(1:k,NA),dat3[,c(1:7)]))
```

La classe che contiene il $25$-esimo percentile è la seconda classe $j_{0.25}=2$, ovvero la classe (10,15], in quanto $F_2=`r X$Fj[2]`>0.25$.

```{r 04-mediana-percentili-8,echo=FALSE}
X <- dat2
percentile(p=.25)
```
::::

:::: {.nota data-latex=""}
Il $p$-esimo percentile $x_p$ è quel valore che taglia l'istogramma in due parti, l'area dell'istogramma alla sinistra di $x_p$ è pari a $p\times 100\%$, mentre l'area la sua destra è $(1-p)\times 100\%$

```{r 04-mediana-percentili-9, echo=FALSE}
nomex <- "Reddito"

histp(T)
h.int(0,Q.int(.25),density=20,col=iblue,lwd=1.5)
axis(side=1,at=round(Q.int(0.25),3),col=ared,las=3)
text(Q.int(.25)-13,3.5,"25%",cex=2,col=iblue)
text(Q.int(.75)+13,3.5,"75%",cex=2,col=iblue)

Fj <- c(0,dat2$Fj)
plot(brk,Fj,type="l",axes=F,xlab=nomex,ylab=expression(F[j]),xlim=c(min(brk)-1,max(brk)+1))
abline(h=0:1,lty=2)
axis(1,brk,pos = 0)
axis(2,Fj,pos = 0,las=2)
segments(brk,0,brk,Fj,lty=2)
segments(0,Fj,brk,Fj,lty=2)
segments(Q.int(.25),0,Q.int(.25),.25,lty=2,col=ared)
segments(0,0.25,Q.int(.25),.25,lty=2,col=ared)
axis(1,Q.int(.25),round(Q.int(.25),3),col.axis=ared,las=3,pos = 0)
axis(2,.25,col.axis=ared,las=2,pos = 0)
```

L'area in blue è il 25% dell'area totale, quella in grigio il 75%.
::::

### I Quartili

Si definiscono **i quartili** della VS $X$, il 25-esimo, il 50.esimo e il 75-esimo percentile di $X$:
\[
(x_{0.25},x_{0.5},x_{0.75})
\]

### Percentili e Funzione di Ripartizione

Se i dati sono quantitativi continui raccolti in classi e $F$ è la funzione di
ripartizione di $X$ allora il percentile è quel valore tale che
\[
F(x_p)=p
\]
ovvero a sinistra di $x_p$ c'è il $p\times 100\%$ dei dati e a destra di $x_p$ 
il rimanente $(1-p)\times 100\%$. Per esempio sappiamo nel caso studiato sopra che
$x_{0.25}=`r round(Q.int(.25),3)`$ e quindi $F(`r round(Q.int(.25),3)`)=0.25$.

Ogni valore di $X$ dal suo minimo al suo massimo è un percentile, per esempio
il valore 15 è il 38-esimo percentile di $X$ ($x_{0.38}=15$), infatti il 38%
dei dati è inferiore a 15:
\[
F(15)=F_{2}=0.38
\]
Mentre la funzione inversa $Q=F^{-1}$ è la funzione che ci permette di calcolare il
percentile di ordine $p$:
\[
Q(p)=x_p.
\]
Per esempio
\[
Q(0.25)=x_{0.25}=`r round(Q.int(.25),3)`
\]

Questa applicazione interattiva aiuta a comprendere meglio la relazione tra istogramma
e funzione di ripartizione: [La Funzione di Ripartizione](https://patrizio-frederic.shinyapps.io/FdR_descrittiva/)



## Lo Scarto Interquartile {#sqi}

Una misura di variabilità è lo scarto interquartile
\[
SI = x_{0.75}-x_{0.25}
\]

## La Moda {#moda}

:::: {.info data-latex=""}
Si definisce la **moda**, $x_{Mo}$ la modalità cui compete frequenza maggiore.
::::

:::: {.example}
Consideriamo la distribuzione del colore dei capelli


```{r 04-mediana-percentili-10,echo=FALSE}
colore <- c("Cast.","Biondi","Rossi","Tot")
frequenza <- c(245,68,13,326)
prn <- data.frame(colore,frequenza)
names(prn)<- c("$\\mathrm{x}_j$","$n_j$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4,col.names = NA,align = "r")
```

La modalità modale (la moda) è $x_{Mo}$=Castano.
::::

:::: {.example} 
Titolo di studio:

```{r 04-mediana-percentili-11,echo=FALSE}
colore <- c("Prim.","M. inf.","M. sup.","Univ.","Post univ.","Tot")
frequenza <- c(10,18,158,62,12,sum(c(10,18,158,62,12)))
prn <- data.frame(colore,frequenza)
names(prn)<- c("$\\mathrm{x}_j$","$n_j$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4,col.names = NA,align = "r")
```

La modalità modale (la moda) è $x_{Mo}=$M. sup.
::::

:::: {.example}
$\phantom{2}$

```{r 04-mediana-percentili-12,echo=FALSE}
set.seed(10)
xx <- 0:4
nn <- c(1,11,6,8,5)
x <- unlist(sapply(1:5,function(x) rep(xx[x],nn[x])))
#mean(x); median(x)
kable(t(table(x)),booktabs=T,escape = F,col.names =NA,align = "r")
```

La modalità modale è $\mathrm{x} = 1$ e osserviamo che la media è `r mean(x)` e la mediana è `r median(x)`
::::

### La Moda per dati raccolti in classi

:::: {.info data-latex=""}
Se i dati sono sono raccolti in classi, non c'è un valore modale ma una classe modale ed è la **classe cui compete densità maggiore**. 
:::: 

:::: {.example}
$\phantom{2}$

```{r 04-mediana-percentili-13, echo=FALSE, results="asis"}
tabl(cbind("$j$"=c(1:k,NA),dat3[,c(1:7)]))
```

la classe modale è la terza classe, la classe $[15,20)$
::::

## Relazione tra Media, Moda e Mediana

:::: {.info data-latex=""}
Se la VS $X$ ha una sola classe modale, allora valgono le seguenti relazioni:

- Se la distribuzione presenta un'asimmetria negativa (coda lunga a sx) allora
\[\bar x\le  x_{0.5} \le x_{mo}\]
- Se la distribuzione è simmetrica allora
\[x_{mo}\approx x_{0.5}\approx \bar x\]
- Se la distribuzione presenta un'asimmetria positiva (coda lunga a dx) allora
\[x_{mo}\le x_{0.5}\le \bar x\]
::::

La figura \@ref(fig:mmm) ne offre una rappresentazione grafica.

```{r mmm,fig.cap="Relazione tra media mediana e moda",echo=FALSE}
op <- par(mfrow=c(1,3),mar=c(4,0,3,0)+.1)
a <-5
b <-2
mo <- (a-1)/(a+b-2) 
me <- qbeta(.5,a,b)-.02
xm <- a/(a+b)-.1
curve(dbeta(x,a,b),axes=F,xlab="",ylab="")
segments(c(mo,me,xm),0,c(mo,me,xm),dbeta(c(mo,me,xm),a,b),lty=2)
axis(1,at=c(mo,me,xm),labels = c("Moda","Mediana","Media"),las=2)
title("Asimmetria negativa")
a <-5
b <-5
mo <- (a-1)/(a+b-2) 
me <- qbeta(.5,a,b)
xm <- a/(a+b)
curve(dbeta(x,a,b),axes=F,xlab="",ylab="")
segments(c(mo,me,xm),0,c(mo,me,xm),dbeta(c(mo,me,xm),a,b),lty=2)
axis(1,at=0.5,labels = c("Moda=Mediana=Media"),las=1)
title("Simmetria")
a <-2
b <-5
mo <- (a-1)/(a+b-2) 
me <- qbeta(.5,a,b)+.02
xm <- a/(a+b)+.1
curve(dbeta(x,a,b),axes=F,xlab="",ylab="")
segments(c(mo,me,xm),0,c(mo,me,xm),dbeta(c(mo,me,xm),a,b),lty=2)
axis(1,at=c(mo,me,xm),labels = c("Moda","Mediana","Media"),las=2)
title("Asimmetria positiva")
par(op)
```


```{r 04-mediana-percentili-14,results='asis',echo=FALSE}
cat(knit_child("04a-Istogramma.Rmd",envir = environment(),quiet=T))
```

<!--chapter:end:04-mediana-percentili.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

\part{Probabilità}


```{r 05-Probabilita-1,echo=F,include=FALSE}
#rm(list = ls())


source("intro.R")
```


# Cenni di Teoria della probabilità

## Concetti di base 

La definizione più moderna che possiamo dare è che

> *La probabilità è una misura dell'incertezza*

che un osservatore *razionale* esprime sull'accadibilità di un evento.

Questa definizione consente di ampliare i campi di applicazione della
teoria della probabilità oltre il mero calcolo dei giochi d'azzardo. Ma
per arrivare a questa definizione molto ampia, matematici, scienziati e
filosofi discutono di caso, caos, disordine, frequenza, probabilità,
possibilità, verosimiglianza, ecc. da diversi secoli. La storia della
filosofia e del calcolo della probabilità è affascinate e complessa ed
esula dagli obiettivi di questi appunti. Rimando ai più curiosi il libro
di Costantini e Geymonat:

> D. Costantini e L. Geymonat, Filosofia della probabilità, Feltrinelli,
> Milano 1982.

### Eventi

Un evento $E$ è un *fatto* (potenzialmente verificabile), espresso nel
linguaggio comune, che non sappiamo se è vero o se è falso:

-   uscirà 6 dal lancio di un dado (perfetto).
-   uscirà testa dal lancio di una moneta non regolare.
-   Domani pioverà.
-   l'indice Dow Jones tra un'ora quoterà 35 000.

Dicendo che il *fatto* deve essere *potenzialmente verificabile*
escludiamo tutti quegli eventi che sono fuori dalla portata dei nostri
sensi. Ovvero speculare sull'esistenza di Dio o della vita dopo la morte
o di scenari ipotetici nel passato non è compito della probabilità.

### Algebra degli eventi

Usiamo gli operatori dell'insiemistica per combinare gli eventi tra di
loro, trasformando le operazioni sintattiche del linguaggio comune in
unioni ed intersezioni di eventi.

::: {.info data-latex=""}
::: {.definition name="Unione tra Eventi"}
Siano $A$ e $B$ due eventi, l'espressione $$
A\cup B
$$ è vera se **almeno uno dei due** è vero.
:::
:::

::: {.example}
\begin{eqnarray*}
    A&=&  \text{Domani pioverà}\\
    B&=&  \text{Domani ci sarà traffico}
\end{eqnarray*} $$
A\cup B\qquad\text{sarà vera se domani pioverà }\textbf{o}\text{ ci sarà traffico }\textbf{o}\text{ ci sarà pioggia e traffico.}
$$
:::

::: {.info data-latex=""}
::: {.definition name="Intersezione tra Eventi"}
Siano $A$ e $B$ due eventi, l'espressione $$
A\cap B\qquad\text{è vera se è vero $A$ }\textbf{e}\text{  è vero $B$.}
$$
:::
:::

::: {.example}
\begin{eqnarray*}
    A&=&  \text{Domani pioverà}\\
    B&=&  \text{Domani ci sarà traffico}
\end{eqnarray*} 
$$
A\cap B\qquad\text{sarà vera se domani pioverà }\textbf{e}\text{ ci sarà traffico.}
$$
:::

::: {.info data-latex=""}
::: {.definition name="Evento Complementare"}
Sia $A$ un evento, si definisce $\bar A$ l'evento complementare di $A$
:::
:::

::: {.example}
\begin{eqnarray*}
    A&=&  \text{Domani pioverà}\\
    \bar A&=&  \text{Domani }\textbf{non}\text{ pioverà}
\end{eqnarray*}
:::

\[
\text{domani}\textbf{ non}\text{ pioverà}
\]

::: {.info data-latex=""}
::: {.definition name="Evento Certo"}
Sia $A$ un evento, si definisce l'evento certo $\Omega$ l'evento:
\begin{eqnarray*}
\Omega &=& A\cup\bar A
\end{eqnarray*}
:::
:::

::: {.example}

\begin{eqnarray*}
    A&=&  \text{Domani pioverà}\\
    \bar A&=&  \text{Domani }\textbf{non}\text{ pioverà}\\
    \Omega &=& \text{Domani o pioverà o non pioverà}
\end{eqnarray*}

:::

::: {.info data-latex=""}
::: {.definition name="Evento Impossibile"}
Sia $A$ un evento, si definisce l'evento certo $\emptyset$ l'evento:
\begin{eqnarray*}
\emptyset &=& A\cap\bar A
\end{eqnarray*}
:::
:::

::: {.example}

\begin{eqnarray*}
    A&=&  \text{Domani pioverà}\\
    \bar A&=&  \text{Domani }\textbf{non}\text{ pioverà}\\
    \emptyset &=& \text{Domani pioverà \bf e non pioverà}
\end{eqnarray*}

:::

### Operazioni su insieme

::: {.nota data-latex=""}
Gli operatori Unione $\cup$ e Intersezione $\cap$ si comportano sugli
insiemi come somme $+$ e moltiplicazione $\times$ si comportano sui
numeri. In particolare \begin{align*}
  A\cup B &=  B\cup A ,&&\text{prorietà commutativa}\\
  A\cap B &=  B\cap A ,&&\text{prorietà commutativa}\\
  (A\cup B)\cup C &=   A\cup (B\cup C),       &&\text{prorietà associativa}\\
  (A\cap B)\cap C &=   A\cap (B\cap C),       &&\text{prorietà associativa}\\
  (A\cup B)\cap C &=  (A\cap C)\cup(B\cap C), &&\text{prorietà distributiva}\\
  A\cup B\cap C   &=   A\cup (B\cap C),       &&\text{l'intersezione ha priorità sull'unione...}\\ 
  (A\cup B)\cap C &\ne A\cup (B\cap C),       &&\text{a meno di opportune parentesi}
\end{align*}
:::

### La probabilità è una funzione

La probabilità assegna ad ogni evento il grado di credibilità $$
P(A)
$$ indica la probabilità che l'evento $A$ sia vero. In particolare $$
P(A)=P(\Omega)=1
$$ se $A$ è un evento **certo** 
$$
P(A)=P(\emptyset)=0
$$ 

se $A$ è un evento **impossibile** e in generale 

$$
0\le P(A)\le 1
$$

### Definizioni di probabilità

-   approccio classico (Laplace)
-   approccio frequentista
-   approccio soggettivista

::: {.info data-latex=""}
::: {.definition name="Approccio Classico (Laplace)"}
la probabilità è il rapporto tra il numero dei casi favorevoli e il
numero dei casi possibili, posto che gli eventi siano tutti
equiprobabili.

$$
P(A)=\frac{\#(\text{casi favorevoli ad $A$} )}{\#(\text{casi totali} )}
$$
:::
:::

::: {.example}
Un'urna contiene 5 sfere Rosse, 3 sfere Blu e 2 Nere. La probabilità
dell'evento 
$$
R =\text{Estraggo una Rossa}
$$ 
è data da 
$$
P(R)=\frac{5}{5+3+2}=0.5
$$
:::

::: {.info data-latex=""}
::: {.definition name="Approccio Frequentista"}
**Postulato empirico del caso.**

In un gruppo di prove ripetute più volte *nelle stesse condizioni*,
ciascuno degli eventi possibili si presenta con una frequenza relativa
che tende alla probabilità all'aumentare del numero di prove; ossia

$$
P(A)=\frac{n_A}{n}+\epsilon_n
\quad\mbox{dove}\quad \epsilon_{n}\to 0 \quad\mbox{per}\quad n\to\infty .
$$
:::
:::

::: {.example}
Viene lanciata 200 vole una moneta (che non sappiamo se è bilanciata
oppure no), Si è osservato che 136 su 200 lanci sono TESTA La
probabilità dell'evento $$
T =\text{“Esce Testa''}
$$ è data da 
$$
P(T)\approx\frac{136}{200}=`r 136/200`
$$

Viene lanciata 2000 vole una moneta (che non sappiamo se è bilanciata
oppure no), Si è osservato che 1360 su 2000 lanci sono TESTA La
probabilità dell'evento 
$$
T =\text{“Esce Testa''}
$$ 
è data da 
$$
P(T)\approx\frac{1360}{2000}=`r 1360/2000`
$$

Ma saremo più *sicuri* che avendo lanciato solo 200 volte.
:::

::: {.definition name="Approccio Soggettivista"}
<!-- In questo approccio la probabilità è la rappresentazione formale dell'ignoranza -->

<!-- sulla previsione.  -->

L'approccio soggettivista alla probabilità, sviluppato da Bruno de
Finetti, si differenzia dagli approcci classico e frequentista. Secondo
de Finetti, la probabilità non è una proprietà intrinseca degli eventi,
ma rappresenta il grado di credenza o fiducia che un individuo assegna
all'accadimento di un certo evento. In questo contesto, la probabilità è
personale e varia da soggetto a soggetto, riflettendo l'informazione,
l'esperienza e il giudizio personale.

La probabilità soggettiva si esprime attraverso le scommesse. Per
esempio, dire che un evento ha probabilità del 60% equivale a dire che
si è disposti a scommettere con un rapporto di 3:2 in favore
dell'evento, sia come scommettitore che come allibratore. 
Questa visione mette in luce come le probabilità siano
strettamente legate alle decisioni e alle aspettative personali, e come
possano essere aggiornate in base a nuove informazioni (teoria
Bayesiana). 

L'approccio soggettivistico è particolarmente utile in situazioni dove i
dati sono limitati o dove è difficile definire una frequenza a lungo
termine, come nelle previsioni meteorologiche o nelle valutazioni di
rischio in ambiti finanziari o assicurativi.
:::

## Teoria di Kolmogorov

La teoria di Kolmogorov è una teoria matematica, estremamente
formalizzata, che non si preoccupa di assegnare le probabilità agli
eventi ma alle regole formali per assegnarla senza cadere in
contraddizioni. La teoria muove da 3 assiomi che andremo ad elencare tra
poco, di estremo *buon senso* e muove per definizioni e teoremi verso
strumenti di grande aiuto nella soluzione di problemi concreti.

::: definition
Sia $\Omega$ l'evento certo, Si definisce uno spazio probabilizzato
$\{\Omega,\mathscr{A},P\}$, dove $\mathscr{A}$ è un'algebra costruita su
$\Omega$ e $P$ è una misura di probabilità.
:::

### Algebra degli Eventi

L'algebra degli eventi è una particolare collezione di sottoinsiemi di
$\Omega$ che siamo interessati a probabilizzare. È uno spazio astratto
che etichetta tutte le i possibili eventi che possiamo costruire a
partire da alcuni eventi di partenza. Un'algebra degli eventi viene
solitamente indicata con una lettera calligrafica corsiva. In queste
pagine useremo il carattere tipografico $\mathscr{A}$.

$\mathscr{A}$: un insieme di sotto insiemi di $\Omega$, che contiene
l'insieme vuoto, tutto $\Omega$ ed è chiuso rispetto alle unioni e alle
intersezioni e al passaggio al complementare: se $A\in\mathscr{A}$ e
$B\in\mathscr{A}$ allora


\begin{eqnarray*}
  A\cup B &\in& \mathscr{A}\\
  A\cap B &\in& \mathscr{A}\\
  \bar A &\in& \mathscr{A}
  \end{eqnarray*}


::: {.example}
Per illustrare l'idea di un'algebra degli eventi con un esempio finito, consideriamo un insieme $\Omega$ che rappresenta lo spazio campionario di un semplice lancio di due monete. $\Omega$ è dato da tutte le possibili combinazioni dei risultati di due lanci di moneta: $\Omega = {\text{(T, T)}, \text{(T, C)}, \text{(C, T)}, \text{(C, C)}}$.

Certo, consideriamo ora il lancio di due monete. In questo caso, lo spazio campionario $\Omega$ è dato da tutte le possibili combinazioni dei risultati di due lanci di moneta: $\Omega = \{\text{(T, T)}, \text{(T, C)}, \text{(C, T)}, \text{(C, C)}\}$.

Costruiamo un'algebra degli eventi $\mathscr{A}$ per questo esperimento. Questa algebra potrebbe includere i seguenti sottoinsiemi di $\Omega$:

1. L'insieme vuoto $\emptyset$, che rappresenta l'evento impossibile.
2. L'intero insieme $\Omega$, che rappresenta l'evento certo.
3. Singoli elementi come $\{\text{(T, T)}\}$, $\{\text{(T, C)}\}$, $\{\text{(C, T)}\}$, $\{\text{(C, C)}\}$, che rappresentano gli eventi di ottenere specifiche combinazioni.
4. Combinazioni di questi eventi, come $\{\text{(T, T)}, \text{(T, C)}\}$, che rappresenta l'evento in cui la prima moneta mostra T, indipendentemente dal risultato della seconda moneta.

Questi sottoinsiemi rispettano le proprietà di un'algebra degli eventi:

- Contengono l'insieme vuoto e l'intero insieme $\Omega$.
- Sono chiusi rispetto alle operazioni di unione, intersezione e passaggio al complementare. Ad esempio:
   - Unione: $\{\text{(T, T)}\} \cup \{\text{(C, C)}\} = \{\text{(T, T)}, \text{(C, C)}\} \in \mathscr{A}$.
   - Intersezione: $\{\text{(T, T)}, \text{(T, C)}\} \cap \{\text{(T, T)}, \text{(C, T)}\} = \{\text{(T, T)}\} \in \mathscr{A}$.
   - Complementare: $\overline{\{\text{(T, T)}\}} = \{\text{(T, C)}, \text{(C, T)}, \text{(C, C)}\} \in \mathscr{A}$.

In questo modo, l'algebra degli eventi $\mathscr{A}$ cattura tutte le possibili combinazioni di eventi che possono verificarsi nel contesto di due lanci di moneta.

In definitiva

\begin{eqnarray*}
\mathscr{A}   &=&  \{\emptyset, \\
              && \text{(T, T)}, \text{(T, C)}, \text{(C, T)}, \text{(C, C)} \\
              && \{\text{(T, T)}\cup \text{(T, C)}\}, \{\text{(T, T)}\cup \text{(C, T)}\} , \{\text{(T, T)}\cup \text{(C, C)}\},\\
              && \{\text{(T, C)}\cup \text{(C, T)}\}, \{\text{(T, C)}\cup \text{(C, C)}\} , \{\text{(C, T)}\cup \text{(C, C)}\},\\
              && \{\text{(T, T)}\cup \text{(T, C)}\cup\text{(C,T)}\}, \{\text{(T, T)}\cup \text{(T, C)}\cup \text{(C, C)}\},\\
              && \{\text{(T, T)}\cup \text{(C, T)}\cup\text{(C,C)}\}, \{\text{(T, C)}\cup \text{(C, T)}\cup \text{(C, C)}\},\\
              && \Omega\}
\end{eqnarray*}
  



:::

### Assiomi di Kolmogorov

::: {.info data-latex=""}
La probabilità $P$ è una funzione che trasforma ogni evento $A$ di
$\mathscr{A}$ in un numero reale
$$P:\mathscr{A}\to\mathbb{R},~~\forall A\in\mathscr{A}$$

Tale che

$~~~~i.\phantom{i}\phantom{i}~$ $P(A)\ge 0,~\forall A\in\mathscr{A}$

$~~~~ii.\phantom{i}~$ $P(\Omega)=1$

$~~~~iii.~$
$\forall A,B\in\mathscr{A}:A\cap B=\emptyset, P(A\cup B)=P(A)+P(B)$
:::

La forza della teoria consiste nel ricavare tutti i risultati partendo
da questi 3 assiomi.

### Proprietà di $P$

Dagli assiomi precedenti possiamo ricavare diverse proprietà
interessanti di $P$ che non sono scritte in modo esplicito negli assiomi
ma si ricavano per dimostrazione.

::: {.info data-latex=""}
::: {.proposition name="Proprietà Principali di $P$"}
Tra le tante enunciamo le più immediate ed utili:

1.  $0\le P(A) \le 1,~\forall A\in\mathscr{A}$

2.  $P(\emptyset)=0$

3.  $P(A)=1-P(\bar A)$

4.  $P(A\cap B)=P(A)-P(A\cap \bar B)$

5.  $P(A\cup B)=P(A)+P(B)-P(A\cap B)$
:::
:::

::: {.proof}
Nell'ordine

1.  Dall'assioma *i* sappiamo che $$P(A)>0$$ e dall'assioma *ii* che
    $$P(\Omega)=1$$ Siccome $$A\cup\bar A=\Omega$$ in virtù dell'assioma
    *iii* otteniamo \begin{eqnarray*}
    P(A\cup\bar A)&=&  P(\Omega)\\
    P(A)+P(\bar A)&=& 1\\
    P(A)&\le& 1
    \end{eqnarray*}

2.  Siccome il complementare di $\Omega$ è l'insieme vuoto $$
    \bar\Omega=\emptyset
    $$ dalla 1. sappiamo che la probabilità è compresa tra zero ed 1 e
    quindi \begin{eqnarray*}
    P(\Omega\cup\emptyset)&=&  P(\Omega)\\
    P(\Omega)+P(\emptyset)&=& 1\\
    P(\emptyset)= 0
    \end{eqnarray*}

3.  Siccome $$A\cup\bar A=\Omega$$ in virtù dell'assioma *iii* otteniamo
    \begin{eqnarray*}
    P(A\cup\bar A)&=&  P(\Omega)\\
    P(A)+P(\bar A)&=& 1\\
    P(A)&=& 1-P(\bar A)
    \end{eqnarray*}

4.  Osserviamo che $$A=(A\cap B)\cup(A\cap\bar B)$$ e quindi
    \begin{eqnarray*}
    P(A)&=&  P((A\cap B)\cup(A\cap\bar B))\\
    P(A)&=&  P(A\cap B)+P(A\cap\bar B)\\
    P(A\cap B)&=& P(\bar A)-P(A\cap\bar B)\\
    P(A\cap\bar B)&=& P(\bar A)-P(A\cap B)   
    \end{eqnarray*}

5.  Notiamo che $$A\cup B=(A\cap\bar B)\cup(B\cap\bar A)\cup(A\cap B)$$
    cioè $A\cup B$ si può riscrivere come l'unione di tre eventi
    disgiunti $(A\cap\bar B)$, $(B\cap\bar A)$ e $\cup(A\cap B)$. e
    quindi \begin{eqnarray*}
    P(A\cup B)&=&P((A\cap\bar B)\cup(B\cap\bar A)\cup(A\cap B))\\
    P(A\cup B)&=&P(A\cap\bar B)+P(B\cap\bar A)+P(A\cap B)\\
    P(A\cup B)&=&P(A)-P(A\cap B)+P(B)-P(B\cap A)+P(A\cap B)\\
    P(A\cup B)&=&P(A)+P(B)-P(B\cap A)
    \end{eqnarray*} in figura \@ref(fig:demorgan) una rappresentazione
    grafica.
:::

```{r 05-Probabilita-3}
fig.def(3.5)
```

```{r demorgan,fig.cap="Regola di De Morgan per due insiemi: la probabilità dell'unione è la somma della probilità di tre eventi disgiunti",echo=FALSE}

plot(c(0,10),c(0,10),axes = F,xlab="",ylab="",type="n",asp=.6)
rect(0,2,10,9)
text(9,8,expression(Omega),cex=3)
ellisse(3,5,2.8,2.1,col = ared,density=60,lwd=.1)
ellisse(7,5,2.8,2.1,col = iblue,density=60,angle=-45,lwd=.1)
text(2,5,expression(paste(A,intersect(bar(B)))),cex=2)
text(5,5,expression(paste(A,intersect(B))),cex=2)
text(8,5,expression(paste(B,intersect(bar(A)))),cex=2)
```

::: {.info data-latex=""}
::: {.definition name="Eventi Incompatibili"}
$A$ e $B$ si dicono **incompatibili** se e solo se
$$A\cap B = \emptyset$$ in figura \@ref(fig:incomp) una rappresentazione
grafica.
:::
:::

```{r incomp,fig.cap="Regola di De Morgan per due insiemi: la probabilità dell'unione è la somma della probaiblità di tre eventi disgiunti",echo=FALSE}
plot(c(0,10),c(0,10),axes = F,xlab="",ylab="",type="n",asp=.8)
rect(0,2,10,9)
text(9,8,expression(Omega),cex=3)
ellisse(3,5,1.4,1.1,col = ared,density=60,lwd=.1)
ellisse(7,5,1.5,1.1,col = iblue,density=60,angle=-45,lwd=.1)
text(3,5,expression(A),cex=2)
text(7,5,expression(B),cex=2)

fig.def()


```

::: {.example}
Un'urna ha 8 palline bianche numerate da 1 a 8 e 5 palline nere numerate
da 5 a 9. L'evento $$A=\text{esce un numero inferiore a 4},$$ e l'evento
$$B=\text{esce una pallina nera},$$ Sono chiaramente incompatibili.
:::

## Probabilità Condizionata

La probabilità di un evento $A$ condizionata ad un evento $B$ risponde
alla domanda

> *Se* $B$ fosse vero, con quale probabilità sarebbe vero $A$?

::: {.info data-latex=""}
::: {#pcond .definition name="Probabilità Condizionata"}
Si definisce probabilità di $A$ condizionata a $B$ (probabilità di $A$
dato $B$) la quantità $$
P(A|B)=\frac{P(A\cap B)}{P(B)}
$$
:::
:::

::: {.example}
Un'urna ha 8 palline bianche numerate da 1 a 8 e 5 palline nere numerate
da 5 a 9. Si considerino l'evento
$$A=\text{esce un numero maggiore o uguale a 6},$$ e l'evento
$$B=\text{esce una pallina nera}.$$

Ovviamente \begin{eqnarray*}
   P(A)&=&  \frac{7}{13}=`r 7/13`\\
   P(B)&=&  \frac{5}{13}=`r 5/13`\\
   P(A\cap B)&=&  \frac{4}{13}=`r 4/13`
\end{eqnarray*} e infine \begin{eqnarray*}
   P(A|B)&=&  \frac{P(A\cap B)}{P(B)}\\
   &=&  \frac{\frac{4}{13}}{\frac{5}{13}}=`r 4/5`\\
   P(B|A)&=&  \frac{P(A\cap B)}{P(A)}\\
   &=&  \frac{\frac{4}{13}}{\frac{7}{13}}=`r 4/7`
\end{eqnarray*}
:::

La probabilità condizionata ci consente di esprimere la probabilità
dell'intersezione come prodotto di probabilità condizionate. Infatti in
alcune circostanze è più facile ricavare una probabilità condizionata
invece della probabilità dell'intersezione. La *Chain Rule* è il modo di
esprimere probabilità dell'intersezione come prodotto di condizionate.

Versione a coppie $$P(A\cap B)=P(A)P(B|A)=P(B)P(A|B)$$

la dimostrazione deriva direttamente dalla definizione \@ref(def:pcond).

Versione a triple $$P(A\cap B\cap C)=P(A)P(B|A)P(C|A\cap B)$$

Per mostrare la versione a triple basta analizzare prima l'intersezione
di due eventi $A$ e $B\cap C$, ovvero


\begin{eqnarray*}
   P(A\cap B\cap C)&=&  P(A\cap (B\cap C))\\
   &=&  P(A)P(B\cap C|A)\\
   &=&  P(A)P(B|A)P(C|A\cap B)
\end{eqnarray*}

Versione generale
$$P(A_1\cap A_2\cap A_3 \cap...\cap A_n)=P(A_1)P(A_2|A_1)P(A_3|_1\cap A_2)...P(A_{n}|A_1\cap A_2\cap ...\cap A_{n-1})$$

Per mostrare la versione generale basta iterare il ragionamento in
triplette.

### Indipendenza tra Eventi

L'**indipendenza** è un concetto estremamente importante in probabilità.
Se due eventi $A$ e $B$ sono *indipendenti* significa che l'accadere o
meno dell'uno non altera in alcun modo la probabilità dell'altro.

::: {.info data-latex=""}
::: {.definition name="Indipendenza tra Eventi"}
Due eventi $A$ e $B$ si dicono **indipendenti** se e solo se
\begin{eqnarray*}
P(A|B)&=&P(A)\\
P(B|A)&=&P(B)
\end{eqnarray*}
:::
:::

e quindi, se $A$ e $B$ sono indipendenti 
$$
P(A\cap B)=P(A)P(B)
$$ 
Infatti 
$$
P(A\cap B)=P(A)P(B|A)=P(A)P(B)
$$

### Indipendenza e Incompatibilità

Se $A$ e $B$ sono **incompatibili** allora **non** sono
**indipendenti**, vice versa se $A$ e $B$ sono **indipendenti** allora
**non** sono **incompatibili**:

::: {.att data-latex=""}
-   Se $A\ne \emptyset$ e $B\ne \emptyset$ sono incompatibili, allora
    $P(A\cap B)=0\ne P(A)P(B)$
-   Se $A$ e $B$ sono indipendenti, allora $P(A\cap B)=P(A)P(B)\ne 0$
:::

### Partizioni di $\Omega$

Un *partizione* di $\Omega$ è una collezione di due o più eventi
disgiunti che uniti insieme restituiscono $\Omega$. Se $A$ è un evento e
$\bar A$ è il suo complementare è immediato che la coppia $\{A,\bar A\}$
è una *partizione* in quanto $A\cup\bar A=\Omega$. In questo caso
parleremo di *bipartizione*.

::: {.definition name="Bipartizione"}
Sia $A$ è un evento e $\bar A$ è il suo complementare, allora la coppia
$\{A,\bar A\}$ è una *bipartizione* di $\Omega$.
:::

![](img/2-part.png)

Più in generale definiamo una *partizione finita*:

::: {.definition name="Partizione finita"}
Sia $\{A_1,...,A_n\}$, $n<+\infty$ una collezione di eventi di $\Omega$
tali che

1.  $A_i\cap A_j=\emptyset, ~\forall i\ne j$
2.  $\bigcup_{i=1}^n A_i=\Omega$

Allora $\{A_1,...,A_n\}$ è detta una *partizione* (finita) di $\Omega$
:::

![](img/n-part.png)

### Teorema delle probabilità totali

Il teorema dell probabilità totali permette di esprime la probabilità un
evento come somma della probabilità delle intersezioni che lo
compongono.

::: {.info data-latex=""}
::: {.theorem name="Probabilità Totali versione a coppie"}
Siano $A$ e $B$ due eventi diversi dal vuoto, allora \begin{eqnarray*}
P(B)&=&P(A)P(B|A)+P(\bar A)P(B|\bar A)
\end{eqnarray*}
:::
:::

![](img/2x2.png)

::: {.proof}
Si parte considerando l'identità insiemistica 
$$
B = (A\cap B) \cup (\bar A \cap B)
$$ 
e quindi 
\begin{eqnarray*}
  P(B)&=&P(A\cap B)+P(\bar A\cap B)\\
  &=& P(A)P(B|A)+P(\bar A)P(B|\bar A), \qquad \text{per la chain rule}
\end{eqnarray*}
:::

::: {.example}
Quando piove Giulio arriva in ritardo con probabilità 0.18, mentre se
non piove arriva in ritardo con probabilità 0.01. Le previsioni del
tempo dicono che domani pioverà con probabilità 0.85. Calcolare la
probabilità che Giulio, domani, arrivi in ritardo.

**Soluzione.** Sia $B=\text{“Domani Giulio arriverà in ritardo''}$ e sia
$A=\text{“Domani pioverà''}$. Sappiamo dalle previsioni che $P(A)=0.85$,
mentre $P(\bar A)=1-0.85=0.15$. Inoltre sappiamo che $P(B|A)=0.18$
mentre $P(B|\bar A)=0.01$ Inoltre osserviamo che $B$ si può dividere
nell'intersezione in due insiemi disgiunti: piove *ed* arriva in ritardo
*oppure* non piove *ed* arriva in ritardo, in simboli

$$
B = (A\cap B) \cup (\bar A \cap B)
$$

Dal teorema dell probabilità totali

\begin{eqnarray*}
P(B)&=&P(A)P(B|A)+P(\bar A)P(B|\bar A)\\
&=& 0.85\times 0.18 + 0.15\times 0.01\\
&=& `r .85*.18+.15*.01`
\end{eqnarray*}
:::

Il teorema può essere esteoso alla versione a triple: Siano $\{A_1,A_2,A_3\}$ e $\{B_1,B_2,B_3\}$ due partizioni di $\Omega$:
$A_1\cap A_2=\emptyset, ~A_1\cap A_3=\emptyset,~A_2\cap A_3=\emptyset$ e
$\Omega=B_1\cup B_2\cup B_3$, e $B_1\cap B_2=\emptyset, ~B_1\cap B_3=\emptyset,~B_2\cap B_3=\emptyset$ e $\Omega=B_1\cup B_2\cup B_3$. Allora


\begin{eqnarray*}
P(B_1)&=&P(A_1)P(B_1|A_1)+P(A_2)P(B_1| A_2)+(A_3)P(B_1| A_3)\\
P(B_2)&=&P(A_1)P(B_2|A_1)+P(A_2)P(B_2| A_2)+(A_3)P(B_2| A_3)\\
P(B_3)&=&P(A_1)P(B_3|A_1)+P(A_2)P(B_3| A_2)+(A_3)P(B_3| A_3)\\
\end{eqnarray*}

::: {.info data-latex=""}
::: {.theorem name="Probabilità Totali versione Generale"}
Siano $\{A_1,...,A_n\}$ e $\{B_1,...,B_m\}$ due partizioni di $\Omega$, ovvero
$A_i\cap A_j=\emptyset, ~\forall i\ne j$ e $\Omega=\bigcup_{i=1}^n A_i$ e 
$B_i\cap B_j=\emptyset, ~\forall i\ne j$ e $\Omega=\bigcup_{i=1}^n B_i$
Allora 

$$
P(B_j)=\sum_{i=1}^nP(A_i)P(B_j|A_i),\qquad j=1,...,m
$$
:::
:::

![](img/nxm.png)

::: {.proof}
Si parte considerando l'identità insiemistica

$$
B_j = (A_1\cap B_j) \cup ( A_2 \cap B_j) \cup ... \cup ( A_n \cap B_j)
$$

e quindi


\begin{eqnarray*}
P(B_j)&=&P(A_1\cap B_j)+P( A_2\cap B_j)+...+P( A_n\cap B_j)\\
&=& P(A_1)P(B_j|A_1)+P(A_2)P(B_j|A_2)+...+P(A_n)P(B_j|A_n), \qquad \text{per la chian rule}\\
&=& \sum_{i=1}^nP(A_i)P(B_j|A_i)
\end{eqnarray*}

:::

::: {.example}
Un'urna contiene 10 palline: due etichettate con $A$, tre etichettate
con $B$ e le rimanenti cinque etichettate con $C$. Se si estrae la
pallina $A$ si estrae da l'urna $\mathcal{A}$ che contiene 3 palline
vincenti e una perdente, se si estrae la pallina $B$ si estrae da l'urna
$\mathcal{B}$ che contiene due palline vincenti e una perdente e se si
estrae la pallina $C$ si estrae da l'urna $\mathcal{C}$ che contiene 1
pallina vincente e due perdenti. Qual è la probabilità di vincere?

**Soluzione.** Anzitutto notiamo che

\begin{eqnarray*}
P(A)&=&\frac 2{10}\\
P(B)&=&\frac 3{10}\\
P(C)&=&\frac 5{10}
\end{eqnarray*} Sia $V=\text{“Vincere''}$, dai dati abbiamo che


\begin{eqnarray*}
P(V)&=&P(A)P(V|A)+P(B)P(V| B)+(C)P(V| C)\\
&=& \frac 2{10} \times \frac 34 + \frac 3{10} \times \frac 23 + \frac 5{10} \times \frac 13\\
&=& `r 2/10*3/4+3/10*2/3+5/10*1/3`
\end{eqnarray*}

:::

### Il Teorema di Bayes

Il teorema di Bayes è un risultato probabilitistico che consente di
esprimere le probabilità condizionate ed è utilizzato come base per
quella che è nota come teoria statistica bayesiana, di cui non entreremo
nel dettaglio.

::: {.info data-latex=""}
::: {.theorem name="Teorema di Bayes versione a coppie."}
Si considerino due eventi $A$ e $B$ di cui sono note $P(A)$, $P(B|A)$ e
$P(B|\bar A)$, allora

$$
P(A|B)=\frac{P(A)P(B|A)}{P(A)P(B|A)+P(\bar A)P(B|\bar A)}
$$
:::
:::

::: {.proof}

\begin{align*}
P(A|B)&=\frac{P(A\cap B)}{P(B)}, &&\text{per definizione}\\
&= \frac{P(A)P(B|A)}{P(B)},&& \text{per la chain rule}\\
&= \frac{P(A)P(B|A)}{P(A)P(B|A)+P(\bar A)P(B|\bar A)},&& \text{per il teorema delle prob. tot.}
\end{align*}

:::

::: {.example}
Quando piove Giulio arriva in ritardo con probabilità 0.18, mentre se
non piove arriva in ritardo con probabilità 0.01. Le previsioni del
tempo dicono che domani pioverà con probabilità 0.85. Se il giorno dopo
Giulio entrasse in ritardo con qual probabilità avrebbe piovuto?

**Soluzione.** Sia $B=\text{“Domani Giulio arriverà in ritardo''}$ e sia
$A=\text{“Domani pioverà''}$. Sappiamo dalle previsioni che $P(A)=0.85$,
mentre $P(\bar A)=1-0.85=0.15$. Inoltre sappiamo che $P(B|A)=0.18$
mentre $P(B|\bar A)=0.01$ Inoltre osserviamo che $B$ si può dividere
nell'intersezione in due insiemi disgiunti: piove *ed* arriva in ritardo
*oppure* non piove *ed* arriva in ritardo, in simboli

$$
B = (A\cap B) \cup (\bar A \cap B)
$$

Dal teorema dell probabilità totali


\begin{eqnarray*}
P(B)&=&P(A)P(B|A)+P(\bar A)P(B|\bar A)\\
&=& 0.85\times 0.18 + 0.15\times 0.01\\
&=& `r .85*.18+.15*.01`
\end{eqnarray*}

Dal teorema di Bayes


\begin{eqnarray*}
P(B|A)&=&\frac{P(A)P(B|A)}{P(A)P(B|A)+P(\bar A)P(B|\bar A)}\\
&=& \frac{0.85\times 0.18}{0.85\times 0.18 + 0.15\times 0.01}\\
&=& `r .85*.18/(.85*.18+.15*.01)`
\end{eqnarray*}

:::

::: {.info data-latex=""}
::: {.theorem name="Teorema di Bayes versione Generale"}
Siano $\{A_1,...,A_n\}$ e $\{B_1,...,B_n\}$ due partizioni di $\Omega$,
di cui sono note $P(A_i), \forall i$ e $P(B_j|A_i), \forall i,j$, allora
$$
P(A_i|B_j)=\frac{P(A_i)P(B_j|A_i)} {\sum_{i=1}^nP(A_i)P(B|A_i)}
$$
:::
:::

::: {.example}
Un'urna contiene 10 palline: due etichettate con $A$, tre etichettate
con $B$ e le rimanenti cinque etichettate con $C$. Se si estrae la
pallina $A$ si estrae da l'urna $\mathcal{A}$ che contiene 3 palline
vincenti e una perdente, se si estrae la pallina $B$ si estrae da l'urna
$\mathcal{B}$ che contiene due palline vincenti e una perdente e se si
estrae la pallina $C$ si estrae da l'urna $\mathcal{C}$ che contiene 1
pallina vincente e due perdenti. Giulio ha appena giocato e ha vinto,
qual è la probabilità che sia uscita una pallina etichettata con $C$?

**Soluzione.** Anzitutto notiamo che

\begin{eqnarray*}
P(A)&=&\frac 2{10}\\
P(B)&=&\frac 3{10}\\
P(C)&=&\frac 5{10}
\end{eqnarray*} Sia $V=\text{“Vincere''}$, dai dati abbiamo che

\begin{eqnarray*}
P(V)&=&P(A)P(V|A)+P(B)P(V| B)+(C)P(V| C)\\
&=& \frac 2{10} \times \frac 34 + \frac 3{10} \times \frac 23 + \frac 5{10} \times \frac 13\\
&=& `r 2/10*3/4+3/10*2/3+5/10*1/3`
\end{eqnarray*} In virtù del teorema di Bayes


\begin{eqnarray*}
P(C|V)&=&\frac{P(C)P(V| C)}{P(A)P(V|A)+P(B)P(V| B)+P(C)P(V| C)}\\
&=& \frac{\frac 5{10} \times \frac 13}{\frac 2{10} \times \frac 34 + \frac 3{10} \times \frac 23 + \frac 5{10} \times \frac 13}\\
&=& `r 5/10*1/3/(2/10*3/4+3/10*2/3+5/10*1/3)`
\end{eqnarray*}

:::

\clearpage

## Specchietto finale utile per gli esercizi elementari

::: {.info2 data-latex=""}
<div style="font-size:0.8em;">
```{r 05-Probabilita-2, echo=F,results='asis'}
# 
# if ( html){ f1 <- " "; f2 <- " "}   # se html usa questi
# if (!html) {f1 <- "\\bf"; f2 <- "\\rm"} # se pdf usa questi
# 
# r1 <- c( "$0\\le P(A) \\le 1,~\\forall A\\in\\mathscr{A}$", " ", "la probabilità è compresa tra 0 e 1.")
# r2 <- c(" $P(\\Omega)=1$","","la prob. dell'evento certo è 1,")
# r21 <-  c("$P(\\emptyset)=0$,", " ", "la prob. dell'insieme vuoto è zero.")
# r3 <-  c("$P(A)=1-P(\\bar A)$", " ","regola del complementare")
# r4 <-  c("$P(A\\cup B)=P(A)+P(B)-P(A\\cap B)$", " ", "regola della somma (de Morgan)")
# r5 <-  c("$P(A\\cup B)=P(A)+P(B)$", " ", paste(f1," se e solo se $A$ e $B$ sono incompatibili:",f2))
# r51 <-  c(       "","","terzo assima di Kolmogorov")
# r6 <-  c("$P(A\\cap B)=P(A)P(B|A)=P(B)P(A|B)$", " ", "regola del prodotto (chain rule)")
# r7 <-  c("$P(A\\cap B)=P(A)P(B)$", " ", paste(f1,"se e solo se $A$ e $B$ sono indipendenti",f2))
# r8 <-  c("$P(B)=P(A)P(B|A)+P(\\bar A)P(B|\\bar A)$", " ", "Teorema delle probabilità totali")
# r9 <- c("","","")
# 
# spc <- data.frame(rbind(r1,r2,r21,r3,r4,r5,r51,r6,r7,r8))
# if(html) kable(spc,booktabs = T,escape = F,row.names = F,col.names = NULL,longtable = F,linesep = "")

r1 <- c( "0&\\le P(A) \\le 1,~\\forall A\\in\\mathscr{A}", "&&\\text{la probabilità è compresa tra 0 e 1.}")
r2 <- c(" P(\\Omega)&=1","&&\\text{la prob. dell'evento certo è 1,}")
r21 <-  c("P(\\emptyset)&=0", "&&\\text{la prob. dell'insieme vuoto è zero.}")
r3 <-  c("P(A)&=1-P(\\bar A)","&&\\text{regola del complementare}")
r4 <-  c("P(A\\cup B)&=P(A)+P(B)-P(A\\cap B)", "&&\\text{regola della somma (de Morgan)}")
r5 <-  c("P(A\\cup B)&=P(A)+P(B)", "&&\\textbf{se e solo se A e B sono incompatibili:}")
r51 <-  c(       "&","&&\\text{terzo assima di Kolmogorov}")
r6 <-  c("P(A\\cap B)&=P(A)P(B|A)=P(B)P(A|B)", "&&\\text{regola del prodotto (chain rule)}")
r7 <-  c("P(A\\cap B)&=P(A)P(B)", "&&\\text{se e solo se A e B sono indipendenti}")
r8 <-  c("P(B)&=P(A)P(B|A)+P(\\bar A)P(B|\\bar A)", "&&\\text{Teorema delle probabilità totali}")

spc <- data.frame(rbind(r1,r2,r21,r3,r4,r5,r51,r6,r7,r8))

{
  cat("\\begin{align*}\n")
  for (i in 1:(nrow(spc)-1)) cat(paste(spc[i,]),"\\\\ \n")
  cat(paste(spc[nrow(spc),]),"\n")
  cat("\\end{align*}\n")
}
#if (!html)

```
</div>
:::

\normalsize

<!-- $\phantom{-}$ -->

<!--chapter:end:05-Probabilita.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r s06, echo=FALSE}



source("intro.R")

```

# Variabili Casuali

## Definizione formale di una VC discreta

Si consideri un partizione finita o al più numerabile dell'evento certo: 

$$
\{E_1,...,E_k\},~E_i\cap E_j,~\forall i\ne j, ~E_1\cup...\cup E_k=\Omega
$$

Una VC $X$ è una funzione che mappa la partizione sulla retta reale $$X:\Omega\to\mathbb{R}$$. $$X(E_j)=x_j,~\text{ad ogni evento $E_j$ viene assegnato un numero}$$

Si definisce $S_X$ il **supporto** della VC $X$, ed è l'insieme dei valori che la VC è suscettibile di assumere.

Resta definita la **funzione di probabilità** di $X$ che è data da: 

$$
f(x)=P(X=x)=P(E_j:X(E_j)=x)
$$


*Una variabile casuale* $X$ è un numero che ancora non sappiamo quanto varrà, potrà assumere uno qualunque dei valori $x$ del supporto $S_X$ e lo assumerà con una data probabilità $f(x)=P(X=x)$.

:::: {.example}
Consideriamo il lancio di due monete identiche, la partizione associata è

- $E_1=\{T,T\}$, la prima e la seconda moneta mostrano Testa.
- $E_2=\{T,C\}$, la prima moneta mostra Testa e la seconda Croce.
- $E_3=\{C,T\}$, la prima moneta mostra Croce e la seconda Testa.
- $E_4=\{C,C\}$, la prima e la seconda moneta mostrano Croce.

Si considerino le tre variabili nella tabella qui sotto

 $j$ | $E_j$   |  $X(E_j)$ | $Y(E_j)$ | $W(E_j)$ 
 ----|---------|-----------|----------|----------
 1   |$E_1=\{T,T\}$ | $X(E_1)=2$ | $Y(E_1)=0$ | $W(E_1)=1$ 
 2   |$E_2=\{T,C\}$ | $X(E_2)=1$ | $Y(E_2)=1$ | $W(E_2)=0$ 
 3   |$E_3=\{C,T\}$ | $X(E_3)=1$ | $Y(E_3)=1$ | $W(E_3)=0$ 
 4   |$E_4=\{C,C\}$ | $X(E_4)=0$ | $Y(E_4)=2$ | $W(E_4)=1$ 

$X$ conta il numero di Teste (0, 1 o 2), $Y$ conta il numero di Croci (0, 1 o 2),
$W$ vale uno se i due lanci sono identici e vale zero altrimenti.

Se la moneta è perfetta ($P(T)=P(C)=0.5$) allora:

\begin{eqnarray*} 
P(E_1) &=& P(\{C,C\}) \\ 
    &=& P(C)P(C), \qquad \text{in virtù dell'indipendenza}\\
    &=& \frac 12 \cdot \frac 12  = \frac 14 \\
P(E_2)  &=& P(\{T,C\})\\
    &=& P(T)P(C), \qquad \text{in virtù dell'indipendenza}\\
    &=& \frac 12 \cdot \frac 12  = \frac 14 \\
P(E_3)  &=& P(\{C,T\})\\
    &=& P(C)P(T), \qquad \text{in virtù dell'indipendenza}\\
    &=& \frac 12 \cdot \frac 12  = \frac 14 \\
P(E_4)  &=& P(\{T,T\})\\
    &=& P(T)P(T), \qquad \text{in virtù dell'indipendenza}\\
    &=& \frac 12 \cdot \frac 12  = \frac 14 
\end{eqnarray*}

e quindi:

\begin{eqnarray*} 
P(X=0) &=& P(E_1) \\ 
    &=&\frac 14\\
P(X=1) &=& P(E_2\cup E_3)\\
    &=& P(E_2)+P(E_3)\\
    &=& \frac 14+\frac 14 = \frac 24\\
P(X=2) &=& P(E_4) \\ 
    &=&\frac 14.
\end{eqnarray*}
::::

### Descrizione di una VC

Un $X$ VC ho molte analogie con una VS, $X$ ha un _supporto_
ovvero l'insieme dei valori che $X$ può assumere, una funzione d probabilità
che produce numeri compresi tra zero e uno che sommano ad uno (come le frequenze relative),
la Funzione di Ripartizione che, anziché cumulare le frequenze, cumula le probabilità,
il Valore Atteso che è l'analogo della media e la Varianza che è l'analogo della varianza
descrittiva. In sintesi

:::: {.nota data-latex=""}
Una VC discreta è **identificata** in maniera univoca da

- Il suo supporto $S_X$.
- La sua funzione di probabilità $f(x)$ o la su funzione di ripartizione $F$.
- Il suo valore atteso $E(X)$.
- La sua varianza $V(X).$
::::


:::: {.info data-latex=""}
:::: {.definition name="Supporto"} 
Sia $X$ una VC, si definisce $S_X$ il supporto di $X$, l'insieme di tutti i possibili valori che $X$ è suscettibile di assumere.
::::
::::

:::: {.example}
Lancio una moneta due volte e definisco $X$ la VC che _conta il numero di volte che osservo Testa in due lanci_. La VC $X$
potrà assumete solo 3 valori: 0 (zero volte Testa in due lanci), 1 (una volta Testa in due lanci), e 2 (2 volte Testa in due lanci). E quindi
\[
S_X=\{0,1,2\}
\]
::::

:::: {.info data-latex=""}
:::: {.definition name="Funzione di Probabilità"} 
Sia $X$ una VC con supporto $S_X$, si definisce $f$ la funzione di probabilità: 
è la probabilità che la VC $X$ assuma esattamente il valore $x$
\[
f(x)=P(X=x), ~x\in S_X
\]
::::
::::

:::: {.example}
Si consideri un'urna che contiene 4 palline: $$E_1=\fbox{1},E_2=\fbox{2},E_3=\fbox{2},E_4=\fbox{3}$$ Sia $X$ la VC che rappresenta il numero sulla pallina: 

\begin{eqnarray*}
X(E_1) &=& 1\\
X(E_2) &=& 2\\
X(E_3) &=& 2\\
X(E_4) &=& 3
\end{eqnarray*} 

ha come supporto: $$S_X = \{1,2,3\}$$ e come funzione di probabilità: 
\begin{eqnarray*}
f(1)&=&P(X=1)=P(E_1)=\frac 1 4\\
f(2)&=&P(X=2)=P(E_2\cup E_3)=\frac 1 4+\frac 1 4=\frac 2 4\\
f(3)&=&P(X=3)=P(E_3)=\frac 1 4
\end{eqnarray*} 
::::

La funzione di probabilità $f$ si comporta come le frequenze relative, 
la $f$ è compresa tra zero e uno, per ogni $x$ e 
la somma della $f$ calcolata su ogni $x$ dà uno.

:::: {.info data-latex=""}
:::: {.proposition name="Funzione di probabilità"} 
Sia $X$ una VC con supporto $S_X$ e funzione di probabilità $f$, allora

- $0\le f(x)\le1,\forall x\in S_X$
- $\sum_{x\in S_X} f(x) = 1.$
::::
::::

La Funzione di Ripartizione $F$ di una VC $X$ cumula tutta la probabilità fino ad $x$:

:::: {.info data-latex=""}
:::: {.definition name="Funzione di Ripartizione di una VC"}
$$
F(x)=P(X\le x)=\sum_{x^*\le x} f(x^*)
$$
::::
::::

:::: {.example name="Continua"} 
Continuando l'esempio precedente:
\begin{eqnarray*}
F(x)&=&P(X\le x)\\
    &=&\begin{cases}
    0, &x<1\\
    \frac 1 4, &1\le x<2\\
    \frac 3 4 &2\le x< 3\\
    \frac 4 4 & x\ge 3
    \end{cases}
\end{eqnarray*}
::::

La funzione di ripartizione gode di diverse proprietà. Una, in particolare sarà utilissima
per il calcolo delle probabilità della VC continua che vedremo più avanti.

:::: {.info data-latex=""}
:::: {.proposition name="Funzione di Ripartizione"} 
 La funzione di ripartizione $F$ di una VC $X$ è, per definizione:
 \[F(x)=P(X\leq x)\]
$F$ gode delle seguenti proprietà:

1.  Non decrescente, ossia $x_{1}<x_{2} \Rightarrow F(x_{1}) \le F(x_{2})$
2. $\lim_{x\to -\infty} F(x) = 0$, \qquad $\lim_{x\to\infty} F(x) = 1$.
3. Continua a destra, ossia $\lim_{x\to x_{0}^{+}} F(x) = F(x_{0})$.
4. $P(a < X \le b) = F(b) - F(a)$.
::::
::::

La funzione di ripartizione di una VC si comporta la funzione di ripartizione di
una VS. 
Altre caratteristiche si possono calcolare, come i **percentili** di ordine $p$:
*Il percentile* $x_p$ è quel valore che divide la distribuzione di $X$ in due: 
$P(X\le x_p)=p,~~~P(X>x_p)=1-p$


:::: {.info data-latex=""}
:::: {.definition name="Percentie di una VC"}
Sia $X$ una VC con support $S_X$ e con Fuzione di Ripartizione $F$, si definisce il $p$-esimo percentile di $X$, il vaolre $x_p$, tale che:
$$
x_p:F(x_p)=p
$$
::::
:::: 

La distribuzione può essere rappresentata graficamente con un istogramma di densità.
Esattamente come per la VC quantitative discrete una VC può essere rappresentata 
in intervalli di ampiezza uno costruiti intorno ai dati del supporto.
Nel nostro esempio di prima:

```{r 06-Variabili-Casuali-4}
fig.def()
```

```{r 06-Variabili-Casuali-1,echo=F}

#par(mfrow=c(2,1))
par(cex=.65)
barplot(c(1/4,2/4,1/4),space = 0,col=ablue)
axis(1,at=0:2+.5,labels = 1:3)
title("Istogramma di Densità")
text(1:3-.5,.1,c("0.25","0.50","0.25"),cex=2.5)

```

Analogamente possiamo costruire la funzione di ripartizione cumulando le probabilità.
Nell'esempio di sopra otterremmo:

```{r 06-Variabili-Casuali-2,echo=F}
plot(function(x) 0*(x<1) + .25*(x>=1 & x<2)+.75*(x>=2 & x <3)+1*(x>=3),.5,3.5,n=10001,
     axes=F,ylab="F(x)",type="n")
axis(1,at=0:4)
axis(2,c(0,.25,.75,1))
segments(1:3,0,1:3,c(.25,.75,1),lty=2)
segments(c(0,0),c(.25,.75),c(3,2),lty=2)
segments(c(-.5,1,2,3),c(0,.25,.75,1),c(1,2,3,3.5),c(0,.25,.75,1),lwd=2,col=ared)
points(1:3,c(0,.25,.75),col=ared,cex=1.5)
points(1:3,c(.25,.75,1),col=ared,cex=1.5,pch=16)
abline(h=1,lty=2)
arrows(1:3-.1,c(0,.25,.75),1:3-.1,c(.25,.75,1),length = .10,code = 3)
arrows(2-.1,0,2-.1,.25,length = .10,code = 3)
arrows(3-.1,0,3-.1,.25,length = .10,code = 3)
arrows(3-.1,.25,3-.1,.75,length = .10,code = 3)
text((1:3)-.4,c(.28,.78,.95),c("F(1)=f(1)=.25",c("F(2)=f(1)+f(2)=.75","F(3)=F(2)+f(3)=1")))
text(1-.2,.125,.25,pos=2,srt=90)
text(2-.2,.125,.25,pos=2,srt=90)
text(2-.2,.5,.5,pos=2,srt=90)
text(3-.2,.125,.25,pos=2,srt=90)
text(3-.2,.5,.5,pos=2,srt=90)
text(3-.2,1.75/2,.25,pos=2,srt=90)
title("Funzione di Ripartizione")

```

### Operazioni tra VC

Le VC sono numeri che non sappiamo in anticipo che valore assumeranno. Ma siccome 
diventeranno numeri li potremo sommare, sottrarre, moltiplicare, ecc. sia con numeri
costanti che con altre VC.

:::: {.example}
Sia $X$ la VC con supporto $S_X=\{-1,0,+1\}$ e con funzione di probabilità
\begin{eqnarray*}
   f(-1)&=&  \frac 15\\
   f(0)&=&\frac 35\\
   f(+1)&=&\frac 15
\end{eqnarray*}
  e sia $Y$ la VC con supporto $S_Y=\{0,+1\}$ e con funzione di probabilità
\begin{eqnarray*}
   f(0)&=&\frac 12\\
   f(+1)&=&\frac 12
\end{eqnarray*}

Se poniamo
\[
S=X+Y
\]
$S$ è la VC che rappresenta la somma di due VC: casuale è $X$, casuale è $Y$, casuale sarà la 
somma tra $X$ ed $Y$.

```{r 06-Variabili-Casuali-5}
sx <- c(-1,0,1)
sy <- c(0,1)
fx <- c(1/5,3/5,1/5)
nx <- c(1,3,1)
fy <- c(.5,.5)
ny <- c(1,1)

#out <- outer(sx,sy,function(x,y) paste("$",x+y,";\\frac 1{12}$",sep=""))

out <- outer(1:3,1:2,function(x,y) paste("$",sx[x]+sy[y],";\\frac {",nx[x],"\\times",ny[y],"}{5\\times 2}$",sep=""))
row.names(out) <- paste("$X=",sx,"$",sep="")
colnames(out) <- paste("$Y=",sy,"$",sep="")

#kable(out,booktabs=T,escape = F,linesep="")

res <- two_way(S_1 = sx,S_2 = sy,num1 = c(1,3,1),num2 = c(1,1),op = `+`,EV = F,vnam = "S")
```
::::

## Valore Atteso, e Varianza di una VC

Il **valore atteso** di una VC $X$ è l'analogo della media aritmetica di una VC,
le modalità di $X$ vengono pesate con le probabilità invece che con le frequenze.

:::: {.info data-latex=""}
:::: {.definition name="Valore Atteso di una VC discreta"} 
Si definisce $E(X)$ il valore atteso della VC $X$ con supporto $S_X$ e funzione di probabilita $f$:
$$
E(X)=\sum_{x\in S_x}xf(x)
$$
::::
::::


:::: {.att data-latex=""}
Il valore atteso di una VC è un numero.
::::

La **varianza** di una VC è del tutto analoga alla varianza di una VS.

:::: {.info data-latex=""}
::::{.definition name="Varianza di una VC discreta"}
Si definisce $V(X)$ la varianza della VC $X$ con supporto $S_X$ e funzione di probabilita $f$:
\begin{eqnarray*} 
V(X)&=&E\left(\big(X-E(X)\big)^2\right)\\ 
    &=&\sum_{x\in S_x}(x-E(X))^2f(x),\qquad\text{oppure equivalentemente}\\
    &=& E(X^2)-E^2(X)\\
    &=&\sum_{x\in S_X}x^2f(x)-E^2(X). 
\end{eqnarray*}
::::
::::

così come la sua standard deviation.

:::: {.info data-latex=""}
:::: {.definition name="Standard Deviation di una VC discreta"}
Si definisce $SD(X)$ la Standard Deviation della VC $X$ con supporto $S_X$ e funzione di probabilita $f$,
la radice della sua varianza
\begin{eqnarray*} 
SD(X)&=&\sqrt{V(X)}
\end{eqnarray*}
::::
::::

:::: {.example name="Continua"}
Continuiamo l'esempio precedente e otteniamo, il valore atteso: 
$$
E(X)=1\cdot\frac 1 4 +2\cdot\frac 2 4+3\cdot\frac 1 4=2
$$ 
e la varianza di $X$
$$
V(X)=1^2\cdot\frac 1 4 +2^2\cdot\frac 2 4+3^2\cdot\frac 1 4 - 2^2= 0.5
$$
::::

Le proprietà di valore atteso e varianza di una VC sono del tutto analoghe alle rispettive di una VS.

:::: {.info data-latex=""}
:::: {.proposition name="Proprietà del Valore Atteso di una VC"}
Le proprietà del valore atteso, $E(X)$ sono:

1. $x_{\min} \leq E(X) \leq x_{\max}, \quad x_{\min},\ x_{\max}\in S_{X}$,
2. $E\Big(X - E(X)\Big) = 0$,
3. $E\Big(X - E(X)\Big)^{2} < E(X - d)^{2} \quad\forall\ d \ne E(X)$,
4.  $E(a + b X) = a + b\ E(X)$.
5. $E(aX+bY)=aE(X)+bE(Y)$
::::
::::


Esiste anche l'analogo della proprietà di associatività per il valore atteso, ma richiede
alcuni risultati che esulano dallo scopo di questi appunti e non verrà riportata.


Anche per la varianza di una VC valgono le stesse proprietà della varianza di una VS. In particolare

:::: {.info data-latex=""}
:::: {.proposition name="Proprietà della Varianza di una VC"}
Le proprietà della Varianza, $V(X)$ sono:

1.  $V(X)\geq 0$,
2.  $V(X)=0$ se e solo se $P(X=x)=1$
3.  $$V(a+bX)=b^2V(X)$$
4.  Se $X$ e $Y$ sono **indipendenti**, allora 
$$V(aX+bY)=V(aX-bY)=a^2V(X)+b^2V(Y), \forall~a,b\in\mathbb{R}$$
::::
::::

Notiamo che nel caso di una VC $X$ la sua varianza vale zero se e solo se la VC
assume un solo valore con probabilità uno. Quindi una VC che non varia, ovvero una costante.

:::: {.att data-latex=""}
Se $a=1$ e $b=1$ allora $$V(X+Y)=V(X-Y)=V(X)+V(Y)$$
le varianze non si sottraggono mai.
:::

:::: {.info data-latex=""}
:::: {.proposition name="Proprietà della SD di una VC"}
Le proprietà della Standard Deviation di $X$, $SD(X)$ sono:

1.  $SD(X)\geq 0$,
2.  $SD(X)=0$ se e solo se $P(X=x)=1$
3.  $$SD(a+bX)=|b|V(X)$$
4.  Se $X$ e $Y$ sono **indipendenti**, allora $$SD(aX+bY)=SD(aX-bY)=\sqrt{a^2V(X)+b^2V(Y)}, \forall~a,b\in\mathbb{R}$$
::::
::::

:::: {.att data-latex=""}
Se $a=1$ e $b=1$ allora $$SD(X+Y)=SD(X-Y)=\sqrt{V(X)+V(Y)}$$
la SD di una somma non si esprime come la comma delle SD degli addendi.
::::

## Indipendenza tra VC

In generale due VC $X$ e $Y$ si dicono indipendenti se e solo se: $$P(X\in A\cap Y\in B)=P(X\in A)\cdot P(Y\in B),~~\forall A \subset S_X,\forall B \subset S_Y$$ Per le VC discrete la relazione di indipendenza si può scrivere: $$P(X=x\cap Y=y)=P(X=x)\cdot P(Y=y),~~~\forall x \in S_X,\forall y \in S_Y$$

## VC condizionate (complementi)

La probabilità condizionata di $X$ dato $Y$ si scrive $$
P(X\in A|Y\in B)=\frac{P(X\in A\cap Y\in B)}{P(Y\in B)}
$$ Se $X$ e $Y$ sono discrete si può scrivere 
\begin{eqnarray*} 
f(x|y) &=& P(X=x|Y=y)\\
       &=&\frac{P(X=x\cap Y=y)}{P(Y=y)}
\end{eqnarray*} 
e si legge: *la probabilità che* $X$ assuma il valore $x$ dato che $Y$ ha assunto il valore $y$ è $f(x|y)$.

### Valore atteso e varianza condizionata (complementi)

Sia $X$ una VC discreta con supporto $S_X$. Si definisce il valore atteso di $X$ condizionato ad $Y=y$, la quantità $$
E(X|Y=y)=\sum_{x\in S_X}xf(x|y)
$$ Si definisce varianza di $X$ condizionato ad $Y=y$, la quantità 

$$
V(X|Y=y)=\sum_{x\in S_X}(x-E(X|Y=y))^2f(x|y)
$$


### Esempio di indipendenza tra VC

Sia $Y$ una VC con supporto $S_Y=\{-1,+1\}$ e con funzione di probabilità \begin{eqnarray*}
f_Y(-1) &=& \frac 1 2\\
f_Y(+1) &=& \frac 1 2
\end{eqnarray*} 
la Y al piede della f serve a non confondere la funzione di probabilità della Y con quella della X. 
Se $X$ ed $Y$ sono **indipendenti** allora: \begin{eqnarray*}
P(X=x\cap Y=y) &=& f(x,y)\\
               &=& f_X(x)f_Y(y)
\end{eqnarray*}

```{r 06-Variabili-Casuali-6}
px <- c(.25,.5,.25)
py <- c(.5,.5)
out <- outer(py,px)
rownames(out) <- c("$Y=-1$","$Y=+1$")
```

| $\phantom{.}$       | $f_X(1)=\frac 1 4$      | $f_X(2)=\frac 2 4$      | $f_X(3)=\frac 1 4$      |
|---------------------|-------------------------|-------------------------|-------------------------|
| $f_Y(-1)=\frac 1 2$ | $f(1,-1)=`r out [1,1]`$ | $f(2,-1)=`r out [1,2]`$ | $f(3,-1)=`r out [1,3]`$ |
| $f_Y(+1)=\frac 1 2$ | $f(1,+1)=`r out [2,1]`$ | $f(2,+1)=`r out [2,2]`$ | $f(3,+1)=`r out [2,3]`$ |

\clearpage

## Specchietto finale per le VC discrete

\scriptsize
\vspace{10pt}
:::: {.info2 data-latex=""}
<div style="font-size:0.75em;">
```{r 06-Variabili-Casuali-3,results='asis'}
rig1 <- c(" S_X&", "\\text{il supporto della VC $X$:}")
rig11 <- c(" &", "\\text{l'insieme di tutti i possibili valori che la VC può assumere. }")
rig2 <- c("  &     ", "\\text{Se $X$ è una VD discreta il suo supporto ha:}")
rig3 <- c("S_X&=\\{x_1,...,x_k\\}", "\\text{un numero finito, }")
rig4 <- c("S_X&=\\{x_1,x_2,x_3,...\\}", " \\text{o al più numerabile di elementi.}")
rig5 <- c(" f(x)&=P(X=x),~x\\in S_X", "\\text{$f$ è la funzione di probabilità,}")
rig51 <- c(" &", "\\text{indica la probabilità che la VC $X$ assuma esattamente il valore $x$.}")
r6 <- c("E(X)&=\\sum_{x\\in S_X}xf(x)", "\\text{Valore atteso, l'analogo del concetto di media ma per la VS $X$}" )
rig7 <- c("E(a+bX)&=a+bE(X)", "\\text{linearità}")
rig8 <- c(" E(aX+bY)&=aE(X)+bE(Y)", "")
rig9 <- c(" V(X)&=E\\left(\\big(X-E(X)\\big)^2\\right)", "\\text{Varianza della VC $X$}" )
rig91 <- c("&=\\sum_{x\\in S_X}x^2f(x)-E^2(X) ","" )
riga <- c("V(a+bX)&=b^2V(X)", "" )
rigb <- c("SD(X)&=\\sqrt{V(X)}", "\\text{Standard Deviation della VC $X$}")
rigc <- c("SD(a+bX)&=|b| SD(X)", "")
rigd <- c("\\text{Indipendenza tra VC}&", "")
rige <- c("P(X\\in A\\cap Y\\in B)&=P(X\\in A)\\cdot P(Y\\in B)", "\\forall A \\subset S_X,\\forall B \\subset S_Y")
rigf <- c("P(X=x\\cap Y=y)&=P(X=x)\\cdot P(Y=y)", "\\forall x \\in S_X,\\forall y \\in S_Y")
rigg <- c("V(aX+bY)&=a^2V(X)+b^2V(Y)", "\\text{se e solo se $X$ e $Y$ sono indipendenti}")
righ <- c("SD(aX+bY)&=\\sqrt{a^2V(X)+b^2V(Y)}", "\\text{se e solo se $X$ e $Y$ sono indipendenti. }")
righ1 <- c("&","\\text{n.b. la SD di una somma non  }")
righ2 <- c("&","\\text{si può esprimere con la somma delle SD.}")


nig <- ls()[grep("^rig",ls())]
n <- length(nig)

tab <- matrix(nrow = n,ncol = 2)
#for (i in 1:length(rig)){
for (i in 1:n){
     tab[i,] <- get(nig[i])
}
#kable(tab,escape = F,booktab=T)
cat("\\begin{align*}\n")
for (i in 1:(n-1)){
cat(get(nig[i])[1],"& &",get(nig[i])[2],"\\\\ \n")
}
cat(get(nig[n])[1],"& &",get(nig[n])[2]," \n")
cat("\\end{align*}\n")
```
</div>
::::

\normalsize

## Le VC continue

Le VC che hanno un supporto più che numerabile, $S_X\subseteq\mathbb{R}$,
cioè un sottoinsieme della retta reale o la retta reale stessa vengono chiamate
VC continue. Siccome un intervallo di numeri reali contiene una quantità più che numerabile
di punti è impossibile probabilizzare tutti i punti dell'intervallo. Invece di probabilizzare 
i singoli numeri vengono probabilizzati gli intervalli.

Concettualmente si definiscono immaginando di mandare la precisione di una misura 
all'infinito. Ogni misura è un conteggio, una lunghezza si può misurare in _quanti_ metri, 
o in _quanti centimetri_ centimetri, o in _quanti millimetri_, ecc.

L'idea è di dividere il supporto in classi e costruire un istogramma di densità
tale che l'area sottesa ad una classe si la probabilità della classe stessa.
Se per esempio dividiamo l'intervallo in 11 intervalli, otteniamo, graficamente

```{r 06-Variabili-Casuali-7}
fig.def()
```

```{r 06-Variabili-Casuali-8}
#iblue <- rgb(0.0235294117647059,0.282352941176471,0.47843137254902)

a <- 3
b <- 7
m <- 10

x <- seq(0-1/m/2,1+1/m/2,by=1/m)
n <- length(x)
y <- dbeta(x,a,b)

# par(mfrow=c(2,1))
par(cex=.55)
plot(x,y,type="s",axes=F,ylab="f(x)")
segments(x[2:(m+2)],0,x[2:(m+2)],y[2:(m+2)],lty=2)
axis(1,c(0,x[c(5,6)],1),c("",expression(x),expression(x+dx),""))
polygon(x[c(5,5,6,6)],c(0,y[5],y[5],0),density = 20,col=iblue)
text(.7,2,"Area = P(x<X< <+dx)",cex=1.5)

```

La relativa Funzione di Ripartizione $F$ sarà

```{r 06-Variabili-Casuali-9}
y <- pbeta(x,a,b)

plot(x,y,type="n",axes=F,ylab="F(x)")

points(x[1:(n-1)],y[1:(n-1)])
points(x[1:(n-1)],y[2:n],pch=16)
segments(x0 = x[1:(n-1)],y0 = y[2:n],x1 = x[2:n],y1 = y[2:n])
segments(x0 = x[1:(n-1)],y0 = y[1:(n-1)],x1 = x[1:(n-1)],y1 = y[2:n],lty=2)
axis(1,c(0,x[c(4,5)],1),c("",expression(x),expression(x+dx),""))

axis(2,c(0,y[5],y[6],1),c("0","F(x)","F(x+dx)",1),las=2)
segments(x0 =-2,y0 = y[5],x1 = x[4],y1 = y[5],lty=2)
segments(x0 =-2,y0 = y[6],x1 = x[5],y1 = y[6],lty=2)

segments(x0 =x[4],y0 = y[5],x1 = x[4],y1 = y[5],lty=2)
segments(x0 =x[5],y0 = 0,x1 = x[5],y1 = y[5],lty=2)

arrows(x0 = 0,y0 = y[5],x1 = 0,y1 = y[6],length = .1,code = 3)
text(0.2,mean(y[5:6]),"P(x < X < x + dx)=f(x)dx")
```

Mandare $dx$ a zero significa farlo diventare progressivamente sempre più piccolo

```{r 06-Variabili-Casuali-10}
a <- 3
b <- 7
m <- 30
j <- m/2
x <- seq(0-1/m/2,1+1/m/2,by=1/m)
y <- dbeta(x,a,b)
plot(x,y,type="s",axes=F,ylab="f(x)")
segments(x[2:(m+2)],0,x[2:(m+2)],y[2:(m+2)],lty=2)
axis(1,c(0,x[c(j,j+1)],1),c("",expression(x),expression(x+dx),""),hadj = -.0)
polygon(x[c(j,j,j+1,j+1)],c(0,y[j],y[j],0),density = 20,col=iblue)
text(x[j+1],y[j+1]+.5,"Area = P(x<X< x+dx) = f(x) dx",cex=1.5,pos = 4)


```

La relativa Funzione di Ripartizione $F$ sarà

```{r 06-Variabili-Casuali-11}
y <- pbeta(x,a,b)
n <- length(x)

plot(x,y,type="n",axes=F,ylab="F(x)")

points(x[2:n],y[2:n],cex=.5)
points(x[1:(n-1)],y[2:n],pch=16,cex=.5)
segments(x0 = x[1:(n-1)],y0 = y[2:n],x1 = x[2:n],y1 = y[2:n])

axis(1,c(0,x[c(j,j+1)],1),c("",expression(x),expression(x+dx),""),hadj = -.0)

axis(2,c(0,y[j+1],y[j+2],1),c("0","","",1),las=2,hadj = 0)
segments(x0 =-2,y0 = y[j+1],x1 = x[j],y1 = y[j+1],lty=2)
segments(x0 =-2,y0 = y[j+2],x1 = x[j+1],y1 = y[j+2],lty=2)

segments(x0 =x[j],y0 = 0,x1 = x[j],y1 = y[j+1],lty=2)
segments(x0 =x[j+1],y0 = 0,x1 = x[j+1],y1 = y[j+2],lty=2)

arrows(x0 = 0,y0 = y[j+2],x1 = 0,y1 = y[j+1],length = .01,code = 3)
#text(0.2,mean(y[5:6]),"P(x < X < x + dx)=f(x)dx")
```

Una  VC continua $X$ è caratterizzata dal supporto $S_X\subseteq\mathbb{R}$ e dalla 
funzione di densità $f$ la cui area sottostante a $S_X$ è uguale ad 1.
\[\int_{-\infty}^{+\infty}f(x)dx=1\]

```{r 06-Variabili-Casuali-12}
a <- 3
b <- 7

curve(dbeta(x,a,b),0,1,1001,axes=F,ylab="f(x)")
axis(1,c(0,1),c(expression(-infinity),expression(+infinity)))
text(.5,2,"Area = 1",cex=1.5)
xx <- c(0.,seq(0,1,length=51),1)
yy <- c(0,dbeta(xx[2:52],a,b),0)
polygon(xx,yy,density = 20,col=iblue)

```

La relativa Funzione di Ripartizione $F$ sarà

```{r 06-Variabili-Casuali-13}
y <- pbeta(x,a,b)

curve(pbeta(x,a,b),n=101,axes=F,ylab="F(x)")
axis(1,c(0,1),c(expression(-infinity),expression(+infinity)))
abline(h = 1,lty=2)
axis(2,0:1)

```

La probabilità di un intervallo qualunque $(a,b)$ è l'area sottesa ad $f$

\[P(a<X<b)=\int_{a}^{b}f(x)dx=F(b)-F(a)\]

```{r 06-Variabili-Casuali-14}
a <- 3
b <- 7

curve(dbeta(x,a,b),0,1,1001,axes=F,ylab="f(x)")
axis(1,c(0,.3,.5,1),c(expression(-infinity),"a","b",expression(+infinity)))
xx <- c(0.3,seq(0.3,0.5,length=51),.5)
yy <- c(0,dbeta(xx[2:52],a,b),0)
polygon(xx,yy,density = 20,col=iblue)
expr1 <- expression(x)
expr2 <- expression(X < x + dx)
#text(.6,2,expression(paste("Area =",P(X %in% (list(a,b))))),cex=1.5,pos=3)
text(.6,2,expression(paste("Area =", "P(a < X < b)")),cex=1.5)

y <- pbeta(x,a,b)

curve(pbeta(x,a,b),n=101,axes=F,ylab="F(x)")
axis(1,c(0,.3,.5,1),c(expression(-infinity),"a","b",expression(+infinity)))
segments(c(.3,.5),c(0,0),c(.3,.5),c(pbeta(.3,a,b),pbeta(.5,a,b)),lty=2)
segments(c(-1,-1),c(pbeta(.3,a,b),pbeta(.5,a,b)),c(.3,.5),c(pbeta(.3,a,b),pbeta(.5,a,b)),lty=2)
abline(h = 1,lty=2)
axis(2,c(0,pbeta(.3,a,b),pbeta(.5,a,b),1),c(0,"F(a)","F(b)",1),las=1)
arrows(x0 = .01,y0 = pbeta(.3,a,b),x1 = 0.01,y1 = pbeta(.5,a,b),length = .1,code = 3)
text(.03,(pbeta(.3,a,b)+pbeta(.5,a,b))/2,"F(b)-F(a)=P(a < X < b)",pos=4)

```

Questa applicazione esemplifica il passaggio dal discreto al continuo e il concetto di _modello_:
[VC continue](https://patrizio-frederic.shinyapps.io/discreto-continuo/).


### Valore Atesso e Varianza di una VC continua

Se $X$ è una VC con supporto $S_X=(x_{\min},x_{\max})$, con $-\infty \le x_{\min} < x_{\max} \le +\infty$, allora si definisce
\[
E(X)=\int_{x_{\min}}^{x_{\max}} x f(x)dx
\]
e
\[
V(X)=\int_{x_{\min}}^{x_{\max}} (x-E(X))^2 f(x)dx
\]

### La VC uniforme

La VC uniforme è utile per prendere confidenza con le VC continue ed è definita
ne seguente modo: $S_X=[0,1]$ e
$$
f(x)= 
\begin{cases}
  1,~~\text{se $0\le x \leq 1$}\\
  0,~~\text{altrimenti}
\end{cases}
$$
Si tratta di una funziona a gradino che vale uno nell'intervallo [0,1] e zero altrove.


```{r 06-unif1}
plot(c(-.1,1.1),c(0,1.1),axes=F,type="n",xlab="x",ylab="f(x)")
segments(0,1,1,1,lwd=2)
segments(c(0,1),0,c(0,1),1,lty=2)
segments(-1,0,0,0,lwd=2)
segments(1,0,2,0,lwd=2)
axis(1,c(-1,0,1,2))
arrows(0.05,0,0.05,1,code = 3,length = .1)
text(0.1,.5,1)
```

La sua funzione di ripartizione è
$$
F(x) = P(X\leq x) = \int_{-\infty}^{x} f(t)dt
$$
Usiamo la lettera $t$ invece della $x$ perché la $x$ è usata per definire $F$
essendo $f(x)=0$ per ogni $x<0$ allora $\int_{-\infty}^x f(t)dt=\int_0^x f(t)dt$ e dunque

$$
F(x)=\begin{cases}
0,~~\text{se $x<0$}\\
x,~~\text{se $0\leq x \leq 1$}\\
1,~~\text{se $x>1$}\\
\end{cases}
$$


```{r 06-unif2}
unif <- function(x) x*(x>=0&x<=1) + (x>1)
plot(c(-.1,1.1),c(0,1.1),axes=F,type="n",xlab="x",ylab="f(x)",asp=1)
axis(1,c(-1,0,0.3,1,2),c(-1,0,"x",1,2),pos = 0)
axis(2,c(-1,0,0.3,1,2),c(-1,0,"x",1,2),pos = 0,las=2)
curve(unif,n = 1001,add=T,col=2,lwd=1.5)
segments(.3,0,.3,.3,lty=2)
segments(0,.3,.3,.3,lty=2)
segments(1,0,1,1,lty=2)
segments(0,1,1,1,lty=2)
```

Ovvero la probabilità che $X\leq x$ è l'area tra zero ed $x$ della funzione a gradino.
Per esempio
$$
F(0.3)=P(X\leq 0.3) =(\text{base = 0.3})\times(\text{altezza = 1})=0.3
$$

```{r 06-unif3}
plot(c(-.1,1.1),c(0,1.1),axes=F,type="n",xlab="x",ylab="f(x)")
segments(0,1,1,1,lwd=2)
segments(c(0,1),0,c(0,1),1,lty=2)
segments(-1,0,0,0,lwd=2)
segments(1,0,2,0,lwd=2)
axis(1,c(-1,0,0.3,1,2),c(-1,0,0.3,1,2),pos = 0)
rect(0,0,.3,1,density = 20,col=ablue)
arrows(0.31,0,0.31,1,code = 3,length = .1)
arrows(0,0.01,0.3,0.01,code = 3,length = .1)
text(.15,.1,0.3)
text(0.35,.5,1)

```

Il valore atteso è

\begin{eqnarray*}
E(X) &=& \int_{-\infty}^{+\infty} xf(x)dx\\
&=& \int_{0}^{1} xdx\qquad\text{poiché $f$ vale 1 in [0,1] e 0 altrove}\\
&=& \left[\frac{x^2}2 \right]_0^1\\
&=& \frac {1^2}2 - \frac {0^2}2 \\
&=& \frac 12
\end{eqnarray*}


La varianza


\begin{eqnarray*}
V(X) &=& E(X^2)-E^2(X)\\
E(X^2) &=& \int_{-\infty}^{+\infty} x^2f(x)dx\\
  &=& \int_{0}^{1} x^2dx\qquad\text{poiché $f$ vale 1 in [0,1] e 0 altrove}\\
  &=& \left[\frac{x^3}3 \right]_0^1\\
  &=& \frac {1^3}3 - \frac {0^2}3 \\
  &=& \frac 13 \\
V(X) &=& \frac 13-\left(\frac 12\right)^2\\
  &=&  \frac 13-\frac 14\\
  &=& \frac{4-3}{12}\\
  &=& \frac 1{12}
\end{eqnarray*}


## Operazioni sulle VC

Le VC sono numeri che non sappiamo in anticipo che valore avranno, quindi siamo autorizzati
a fare operazioni. Sia $X$ una VC con supporto $S_X$ e funzione di probabilità $f$. Se
$g:\mathbb{R}\to\mathbb{R}$, posto
$$
 Y = g(X)
$$
Allora $Y$ è una VC.

:::: {.example}
Sia $X$ una VC con supporto $\{-1,0,1\}$ r con funzione di probabilità
$$
f(x)= \begin{cases}
\frac 14,~~\text{se $x=-1$}\\
\frac 24,~~\text{se $x=\phantom-0$}\\
\frac 14,~~\text{se $x=+1$}\\
\end{cases}
$$
posto
$$
Y=g(X)= 2 + 3X
$$
allora
$$
S_Y = \{-1, 2, 5\}
$$
ovvero se $x=-1$, $y=2+3\times(-1)=-1$, se $x=0$, $y=2+3\times0=2$, se $x=1$, $y=2+3\times1=5$ e quindi


$$
f_Y(y)= \begin{cases}
\frac 14,~~\text{se $y=-1$}\\
\frac 24,~~\text{se $x=+2$}\\
\frac 14,~~\text{se $x=+5$}\\
\end{cases}
$$
::::

:::: {.example}
Sia $X$ una VC con supporto $\{-1,0,1\}$ r con funzione di probabilità
$$
f(x)= \begin{cases}
\frac 14,~~\text{se $x=-1$}\\
\frac 24,~~\text{se $x=\phantom-0$}\\
\frac 14,~~\text{se $x=+1$}\\
\end{cases}
$$
posto
$$
Y=g(X)= X^2
$$
allora
$$
S_Y = \{0,1\}
$$
ovvero se $x=-1$, $y=(-1)^2=1$, se $x=0$, $y=0^2=0$, se $x=1$, $y=1^2=1$ e quindi
$$
P(Y=0) = P(X=0) = \frac 24 = \frac 12\\
P(Y=1) = P(X=-1 \cup X = 1) = P(X=-1) + P(X = 1) = \frac 14+\frac14=\frac12
$$

$$
f_Y(y)= \begin{cases}
\frac 12,~~\text{se $y=0$}\\
\frac 12,~~\text{se $y=1$}\\
\end{cases}
$$
::::

:::: {.example}
Sia $X$ una VC con supporto $\{-1,0,1\}$ r con funzione di probabilità
$$
f(x)= \begin{cases}
\frac 14,~~\text{se $x=-1$}\\
\frac 24,~~\text{se $x=\phantom-0$}\\
\frac 14,~~\text{se $x=+1$}\\
\end{cases}
$$
posto
$$
Y=g(X)= X^2
$$
allora
$$
S_Y = \{0,1\}
$$
ovvero se $x=-1$, $y=(-1)^2=1$, se $x=0$, $y=0^2=0$, se $x=1$, $y=1^2=1$ e quindi
$$
P(Y=0) = P(X=0) = \frac 24 = \frac 12\\
P(Y=1) = P(X=-1 \cup X = 1) = P(X=-1) + P(X = 1) = \frac 14+\frac14=\frac12
$$

$$
f_Y(y)= \begin{cases}
\frac 12,~~\text{se $y=0$}\\
\frac 12,~~\text{se $y=1$}\\
\end{cases}
$$
::::

:::: {.example}
Sia $X$ un VC continua con supporto $S_X=[0,1]$ e funzione di densità $f(x)$. Posto
$$
Y=0.5+1.5\cdot X
$$
allora
$$
S_Y=[0.5,2]
$$
infatti se $X=0$ allora $Y=0.5+1.5\cdot 0=0.5$, mentre se $X=1$ allora $Y=0.5+1.5\cdot 1=2$.
```{r 06-beta}
xmin <- 0
xmax <- 2.5
ymin <- -.9
ymax <- 6
plot(c(xmin,xmax),c(ymin,ymax),type = "n",xlab="",ylab="",axes=F)
arrows(xmin,0,xmax,0,length = .1)
arrows(xmin,ymax/2,xmax,ymax/2,length = .1)
xg <- seq(0,1,by=.001)
lines(xg,ymax/2+dbeta(xg,5,5))
text(0,2.8,0)
text(1,2.8,1)
text(2.44,2.8,"x")
xg2 <- seq(0.5,2,by=.001)
normal <- function(x) (x-min(x))/(max(x)-min(x))
lines(xg2,dbeta(normal(xg2),5,5)/(max(xg2)-min(xg2)))
text(.5,-.2,0.5)
text(2,-.2,2)
text(2.44,-.2,"y")
arrows(0+.01,ymax/2-.2,.5-.01,0+.1,length = .2,col = 2)
arrows(1+.01,ymax/2-.2,2-.01,0+.1,length = .2,col = 2)
```
::::

<!--chapter:end:06-Variabili-Casuali.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r s07, echo=FALSE}
source("intro.R")

```

# Variabili Casuali di particolare interesse

<!-- Le VC sono i mattoncini elementari su cui statistici e probabilisti -->
<!-- costruiscono un _modello statistico_. Alcune VC rivestono sono  -->
<!-- particolarmente adatte per descrivere ampie classi di problemi statistici e probabilistici.  -->
<!-- Queste VC sono di particolare interesse e vengono nominate con un nome convenzionale. -->
<!-- Si tratta di modelli probabilistici che sono completamente noti a meno di un _numero finito_ di parametri  -->
<!-- numerici da fissare. -->

<!-- Se $X$ è una VC nominata dalla sigla convenzionale $\mathscr{L}$ che dipende dal parametro $\theta$ che può variare in $\Theta$ e scriveremo: -->
<!-- \[ -->
<!-- X\sim\mathscr{L}(\theta) -->
<!-- \] -->
<!-- che si legge: -->

<!-- > _$X$ è distribuita come una VC $\mathscr{L}$ di parametro $\theta$_ -->

<!-- L'insieme $\Theta$ è chiamato lo _spazio dei parametri_ ed è l'insieme numerico -->
<!-- per cui $\theta$ ha senso probabilistico. -->

Le variabili casuali rappresentano i mattoncini fondamentali con cui si costruiscono i modelli probabilistici e statistici. Alcune variabili casuali, in particolare, emergono come strumenti privilegiati per descrivere classi molto ampie di problemi. La loro importanza deriva dalla semplicità e generalità con cui riescono a rappresentare fenomeni complessi, rendendole elementi centrali in numerose applicazioni.

Queste variabili casuali, che chiamiamo **di particolare interesse**, sono caratterizzate da una distribuzione nota e convenzionalmente identificata da un nome. Esse non sono semplicemente funzioni generiche, ma rappresentano **modelli probabilistici** completamente definiti, a eccezione di un **numero finito di parametri** che devono essere specificati. Questi parametri controllano aspetti fondamentali della distribuzione, come il centro, la dispersione o la forma.

Per rappresentare formalmente questa idea, adottiamo la seguente notazione: se \( X \) è una variabile casuale con distribuzione identificata dalla sigla \(\mathscr{L}\) e parametrizzata da un vettore di parametri \(\theta\), scriviamo:

\[
X \sim \mathscr{L}(\theta)
\]

Questa espressione si legge:

> _$X$ è distribuita come una variabile casuale di tipo \(\mathscr{L}\) con parametro \(\theta\)_.

La scelta del simbolo \(\theta\) è intenzionalmente astratta. In questo libro, useremo \(\theta\) esclusivamente per rappresentare un **parametro generico** o un vettore di parametri. Nessun modello specifico che vedremo avrà mai un parametro chiamato \(\theta\): i parametri dei modelli saranno sempre identificati da simboli più specifici, come \(\mu, \sigma^2, \lambda\), che rifletteranno il significato del parametro nel contesto della distribuzione. Questa scelta è importante per mantenere il rigore formale e per distinguere il livello generale della teoria da quello specifico delle applicazioni.

In ogni caso, l'insieme dei valori possibili per \(\theta\), chiamato **spazio dei parametri** e indicato con \(\Theta\), definisce il dominio di validità della distribuzione. È l'insieme dei valori per cui la distribuzione ha senso probabilistico. 

Questa notazione, benché astratta, è fondamentale per costruire un linguaggio che sia universale, sintetico e adattabile a contesti diversi. Essa ci permette di comunicare non solo che \( X \) segue una certa distribuzione, ma anche di precisare come i parametri controllano il comportamento della distribuzione stessa. Sarà utile sia nel contesto probabilistico, dove i parametri sono spesso fissati a priori, sia in quello statistico, dove i parametri devono essere stimati dai dati osservati.

```{r 07-vc-importanti-1,results='asis',echo=FALSE}
cat(knit_child("07a-Binomiale.Rmd",envir = environment(),quiet=T))
```

```{r 07-vc-importanti-2,results='asis',echo=FALSE}
cat(knit_child("07b-Poisson.Rmd",envir = environment(),quiet=T))
```

```{r 07-vc-importanti-3,results='asis',echo=FALSE}
cat(knit_child("07c-Normale.Rmd",envir = environment(),quiet=T))
```

```{r 07-vc-importanti-4,results='asis',echo=FALSE}
cat(knit_child("07d-Esercizi-Normale.Rmd",envir = environment(),quiet=T))
```

<!--chapter:end:07-vc-importanti.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setuptlc, include=FALSE}



source("intro.R")

```

# Il Teorema del Limite Centrale

## Successioni di VC

Una successione di variabili casuali è espressa da un numero infinito di
variabili casuali $\{X_1,X_2,...,X_n,...\}$.

::: example
Sia \begin{eqnarray*}
  X_1 &\sim& N\left(0,1\right)\\
  X_2 &\sim& N\left(0,\frac 12\right)\\
  X_3 &\sim& N\left(0,\frac 13\right)\\  
  ...\\
  X_n &\sim& N\left(0,\frac 1n\right)\\
  ...
\end{eqnarray*}
:::

::: example
Sia \begin{eqnarray*}
  X_1 &\sim& \text{Binom}\left(1,\pi\right)\\
  X_2 &\sim& \text{Binom}\left(2,\pi\right)\\
  ...\\
  X_n &\sim& \text{Binom}\left(n,\pi\right)\\
  ...
\end{eqnarray*}
:::

Siamo interessati a sapere se la successione converge ad una VC $X$. Ma
essendo VC e non numeri il concetto di convergenza è più complesso.
Esistono diversi tipi di convergenza e non entreremo nella trattazione
sistematica del tema. Mostreremo solo le convergenze che ci interessano
per sviluppare il resto della teoria.

::: {.definition name="Convergenza in Distribuzione"}
Si dice che la successione $\{X_1,X_2,...,X_n,...\}$ converge in
distribuzione alla VC X se e solo se
$$\lim_{n\to\infty}F_n(x)=F(x),~\forall x\in S_X$$ dove $F_n$ e $F$
rappresentano la funzione di ripartizione di $X_n$ e di $X$,
rispettivamente. E si scrive $$X_n\operatorname*{\rightarrow}_{d} X$$
:::

::: {.definition name="Convergenza in Probabilità"}
Si dice che la successione $\{X_1,X_2,...,X_n,...\}$ converge in
probabilità alla VC X se e solo se
$$\lim_{n\to\infty}P(|X_n-X|<\varepsilon)=1$$e si scrive
$$X_n\operatorname*{\rightarrow}_{P} X$$
:::

::: {.definition name="Convergenza in Media Quadratica"}
Si dice che la successione $\{X_1,X_2,...,X_n,...\}$ converge in media
quadratica alla VC X se e solo se
$$\lim_{n\to\infty}E\big((X_n-X)^2\big)=0$$e si scrive
$$X_n\operatorname*{\rightarrow}_{L^2} X$$
:::

## Somme e Medie di VC

Siano $X_1,...,X_n$ $n$ VC IID, tali che $E(X_i)=\mu$ e
$V(X_i)=\sigma^2$. Chiamiamo $S_n$ la somma delle $X_i$
$$S_n =X_1+...+X_n=\sum_{i=1}^nX_i$$ Dalle proprietà del valore atteso e
della varianza otteniamo \begin{eqnarray*}
   E(S_n) &=&  E(X_1+...+X_n)\\
   &=& E(X_1)+...+E(X_n)\\
   &=& \mu+...+\mu\\
   &=& n\mu\\
   V(S_n) &=&  V(X_1+...+X_n)\\
   &=& V(X_1)+...+V(X_n)\\
   &=& \sigma^2+...+\sigma^2\\
   &=& n\sigma^2
\end{eqnarray*}

Chiamiamo $\bar X$ la media delle $X_i$
$$\bar X= \frac{S_n}n =\frac{X_1+...+X_n}n=\frac 1n\sum_{i=1}^nX_i$$
Dalle proprietà del valore atteso e della varianza otteniamo
\begin{eqnarray*}
   E(\bar X) &=&  E\left(\frac{S_n}{n}\right)\\
   &=& \frac 1n E(S_n)\\
   &=& \mu\\
   V\left(\bar X\right) &=&  V\left(\frac{S_n}{n}\right)\\
   &=& \frac 1{n^2}V(S_n)\\
   &=& \frac {\sigma^2}n 
\end{eqnarray*}

::: {.att data-latex=""}
Le VC $X_1,...,X_n$ sono VC qualunque non necessariamente normali, il
fatto che chiamiamo $E(X_i)=\mu$ e $V(X_i)=\sigma^2$ è solo una
convenzione e non deve fare pensare ai parametri della normale.
:::

::: {.theorem name="Legge dei Grandi Numeri"}
Siano $X_1,...,X_n$ $n$ VC IID, tali che $E(X_i)=\mu$ e
$V(X_i)=\sigma^2$. Posto
$$\bar X= \frac{S_n}n =\frac{X_1+...+X_n}n=\frac 1n\sum_{i=1}^nX_i$$
allora $\bar X$ converge in media quadratica alla VC $X$ che assume il
valore $\mu$ con probabilità 1.
:::

::: proof
Dalla definizione di convergenza in media quadratica, dobbiamo studiare
il limite $$\lim_{n\to\infty}E\big((\bar X-X)^2\big)$$ Siccome $X$ è
tale che $P(X=\mu)=1$ allora
$$\lim_{n\to\infty}E\big((\bar X-\mu)^2\big)$$ ma essendo
$E\big((\bar X-\mu)^2\big)=V(\bar X)=\sigma^2/n$ otteniamo
$$\lim_{n\to\infty}\frac {\sigma^2}n=0$$
:::

Quindi la media di VC converge alla media dell'urna se il numero di VC
aumenta all'infinito. Ma mentre converge al punto della media, cosa
succede? A questa domanda rispondono i teoremi centrali del limite.

## Teoremi del Limite Centrale

I *Teoremi del Limite Centrale* (TLC), central limit theorems, sono una
famiglia di teoremi sul limite delle somme di VC. Occupano un posto
centrale nella teoria della probabilità e dell'inferenza statistica.
Esistono molti enunciati a seconda delle ipotesi di partenza. In questo
corso mostriamo il teorema più noto e lo decliniamo in tre casi
particolari.

I TLC riguardano la convergenza in distribuzione di una successione di
somme di una VC. La potenza del teorema è che, non importa quale sia la
distribuzione di partenza delle $X_i$, la loro somma, per $n$ abbastanza
grande, è approssimabile con una distribuzione normale. Enunciamo qui
tre diversi teoremi che in realtà sono tre diverse declinazioni dello
stesso. Iniziamo dal primo e più famoso.

::: {.info data-latex=""}
::: {.theorem name="TLC per la Somma"}
Siano $X_1....,X_n$, $n$ Variabili Casuali (VC) Indipendenti e
Identicamente Distribuite (IID), tali che $E(X_i)=\mu$,
$V(X_i)=\sigma^2$, $\forall i=1....,n$. Posto $$S_n=X_1+...+X_n,$$
allora $$S_n\operatorname*{\sim}_{a} N\left(n\mu,n\sigma^2\right)$$
:::
:::

Il TLC per la somma asserisce che la somma di VC IID, se il numero di
addendi è sufficientemente grande, si può approssimare con una normale
semplificando notevolmente il calcolo.

Siccome una media non è altro che una somma diviso $n$, valore atteso e
varianza le abbiamo già ricavate, otteniamo

::: {.info data-latex=""}
::: {.theorem name="TLC per la Media"}
Siano $X_1....,X_n$, $n$ Variabili Casuali (VC) Indipendenti e
Identicamente Distribuite (IID), tali che $E(X_i)=\mu$,
$V(X_i)=\sigma^2$, $\forall i=1....,n$. Posto
$$\bar X =\frac {S_n} n = \frac{X_1+...+X_n}n,$$ allora
$$\bar X\operatorname*{\sim}_{a} N\left(\mu,\frac{\sigma^2}n\right)$$
:::
:::

Il TLC per la media asserisce che la media di VC IID, se il numero di
elementi che la contengono è sufficientemente grande, si può
approssimare con una normale semplificando notevolmente il calcolo. La
varianza di questa normale $\sigma^2/n$ va a zero per $n$ che diverge, e
ci riporta alla legge dei grandi numeri.

Se consideriamo le $X_i$ tutte Bernoulli di parametro $\pi$ sappiamo che
$E(X_i)=\pi$ e $V(X_i)=\pi(1-\pi)$. Dedichiamo alla media di Bernoulli
un simbolo speciale che sarà più chiaro più avanti.
$$\hat \pi=\frac{X_1+...+X_n}{n}$$ sostituendo valore atteso e varianza
nel TLC della media otteniamo:

::: {.info data-latex=""}
::: {.theorem name="TLC per la Proporzione"}
Siano $X_1....,X_n$, $n$ Variabili Casuali (VC) Indipendenti e
Identicamente Distribuite (IID), tali che $X_i\sim\text{\rm Ber}(\pi)$,
$\forall i=1....,n$. Posto
$$\hat\pi =\frac {S_n} n = \frac{X_1+...+X_n}n,$$ allora
$$\hat\pi\operatorname*{\sim}_{a} N\left(\pi,\frac{\pi(1-\pi)}n\right)$$
:::
:::

Il TLC per la proporzione è sempre il TLC per la media ma per VC di
Bernoulli. Ci dice che la proporzione osservata su un campione di $n$ VC
di Bernoulli è normale con media $\pi$ e varianza che va a zero con $n$
che diverge.

::: {.nota data-latex=""}
La notazione $\operatorname*{\sim}_{a}$ non è una notazione standard ma
è diventata una prassi con i miei studenti per semplificare la notazione
completa che sarebbe più elaborata. Per esempio nel caso del TLC della
somma anziché scrivere $$
S_n\operatorname*{\sim}_{a} N(n\mu,n\sigma^2)
$$ avremmo dovuto scrivere $$
S_n\operatorname*{\rightarrow}_{d} X,~~~~ X\sim N(n\mu,n\sigma^2)
$$ che complica troppo la trattazione.
:::

### Esempio Somma

Siano $X_1, X_2,...,X_n$ $n$ VC IID con supporto $S_X=\{-1,0,+1\}$ e con
funzione di probabilità $$
\begin{cases}
P(X=-1)&=\frac 13\\
P(X=\phantom{-}0)&=\frac 13\\
P(X=+1)&=\frac 13
\end{cases}
$$ Consideriamo $$
S_2=X_1+X_2
$$ il supporto di $S_2$ sarà $$S_{S_2}=\{-2,-1,0,+1,+2 \} $$ $S_2=-2$ se
sia $X_1=-1$ e $X_2=-1$, $S_2=-1$ se sia $X_1+X_2=-1$ ecc. mettiamo in
tabella

```{r 08-tlc-1,results='asis'}
S_1 <- (-1):1
S_2 <- (-1):1
num1 <- rep(1,times=3)
num2 <- rep(1,times=3)
res <- two_way(S_1=S_1,S_2=S_2,num1=num1,num2=num2,EV = F,vnam="S_2")
S_3 <- res$S_3
num3 <- res$num3
```



Se siamo interessati ad $S_3$ 
$$
S_3=X_1+X_2+X_3=S_2+X_3
$$ 

lavoriamo come prima facendo la somma tra $S_2$ e $X_1$

```{r 08-tlc-2,results='asis'}
res <- two_way(S_1=S_1,S_2=S_3,num1=num1,num2=num3,EV = F,vnam="S_3")
S_4 <- res$S_3
num4 <- res$num3
```


Iteriamo il ragionamento per $S_4$ 

$$
S_4=S_3+X_4
$$

```{r 08-tlc-3,results='asis'}
res <- two_way(S_1=S_1,S_2=S_4,num1=num1,num2=num4,EV = F,vnam="S_4")
S_5 <- res$S_3
num5 <- res$num3
```

Osserviamo i grafici per $n=1,...,4$

```{r 08-tlc-7}
p1 <- 1/3
p2 <- num3/sum(num3)
p3 <- num4/sum(num4)
p4 <- num5/sum(num5)

par(mfrow=c(1,2),cex=cex)
n <- 1
S <- (-n):n
k <- length(S)
plot(c(S[1]-.6,S[3]+.6),
     c(0,1/3+.1),
     type="n",
     xlab=expression(S[1]),
     ylab=expression(P(S[1])),
     axes=F)
rect(xleft = S[]-.5,ybottom = 0,xright = S[]+.5,ytop = p1,col=ablue)
axis(1,S)
axis(2,c(0,1/3),c(0,"1/3"),las=2)
title("Variabile di Partenza")

n <- 2
S <- (-n):n
k <- length(S)
plot(c(S[1]-.6,S[5]+.6),
     c(0,1/3),
     type="n",
     xlab=expression(S[2]),
     ylab=expression(P(S[2])),
     axes=F)
rect(xleft = S[]-.5,ybottom = 0,xright = S[]+.5,ytop = p2,col=ablue)
axis(1,S)
axis(2,c(0,p2),c(0,round(p2,3)),las=2)
title("Distribuzione di S2")

```

```{r 08-tlc-8}
par(mfrow=c(1,2),cex=cex)
n <- 3
S <- (-n):n
k <- length(S)
plot(c(S[1]-.6,S[k]+.6),
     c(0,1/3),
     type="n",
     xlab=expression(S[3]),
     ylab=expression(P(S[3])),
     axes=F)
rect(xleft = S[]-.5,ybottom = 0,xright = S[]+.5,ytop = p3,col=ablue)
axis(1,S)
axis(2,c(0,p3),c(0,round(p3,3)),las=2)
title("Distribuzione di S3")

n <- 4
S <- (-n):n
k <- length(S)
plot(c(S[1]-.6,S[k]+.6),
     c(0,1/3),
     type="n",
     xlab=expression(S[4]),
     ylab=expression(P(S[4])),
     axes=F)
rect(xleft = S[]-.5,ybottom = 0,xright = S[]+.5,ytop = p4,col=ablue)
axis(1,S)
axis(2,c(0,p4),c(0,round(p4,3)),las=2)
title("Distribuzione di S4")
par(mfrow=c(1,1),cex=cex)
```

Calcolare $S_n$ se $n$ è un numero elevato è difficoltoso. Il TLC ci
viene incontro. Possiamo calcolare valore atteso e varianza della VC
della VC che ha generato il sistema.

```{=tex}
\begin{eqnarray*}
  E(X) &=&  -1 \frac 13+0\frac 13+1 \frac 13=0\\
  V(X) &=&  (-1)^2\frac 13 +0^2\frac 13+1^2\frac 13-0^2=\frac 23
\end{eqnarray*}
```
e quindi in virtù del TLC per la somma $$
S_n\operatorname*{\sim}_a N(n\cdot 0, n\cdot 2/3)
$$ Se per esempio $n=50$ $$
S_{50}\operatorname*{\sim}_a N(0,50\cdot 2/3)
$$ e quindi se volessi calcolare la probabilità che $S_{50}<3$ useremmo
la distribuzione normale

```{r 08-tlc-9}
### Calcola la probabilità di una Normale qualunque su un intervallo
##  Input
##  x1 limite inferiore
##  verso (">" o "<")
##  mm media
##  ss varianza
##  vnam nome X
##  mu nome media
##  sigma nome varianza
x1 <- 3
verso <- "<"
mm <- 0
ss <- 100/3
mu <- "E(S_n)"
sigma <- "V(S_n)"
vnam <- "S_n"
```

```{r 08-tlc-4, results='asis'}
cat(norm_int(x1 = x1,verso = "<",mm = mm,ss = ss,vnam = vnam,sigma = sigma))
```

### Roulette

```{r 08-tlc-10}
mumax <- 5
za2 <- round(qnorm(.975),2)
pg <- format(18/37,digits=4,nsmall=4)
xg <- format(-1+2*18/37,digits=4,nsmall=4)
xn <- -1+2*18/37
pn <- 18/37
vn <- 4*18/37*19/37
```

**Il gioco.** Una giocata dalla roulette equivale ad estrarre da un'urna
che contiene 37 bussolotti numerati da 0 a 36.

```{r roulette}
include_graphics("img/tavolo-roulette.png")
```

Si può puntare su diverse combinazioni: pari o dispari, rosso o nero, da
1 a 18 o da 19 a 36,... e altre combinazioni. Se puntiamo 1€, per
esempio, su Rosso la vincita/perdita sarà: $$
\begin{cases}
+1€, &\text{se esce Rosso}\\
-1€, &\text{se non esce Rosso}
\end{cases}
$$ Giocheremo, sempre puntando un euro alla volta, per $n$ volte. La VC
$R_i$ che descrive l'evento Rosso o non Rosso, nella giocata $i$, è una
Bernoulli di parametro $$
\pi=\frac{18}{37}=`r format(18/37,digits=4,nsmall=4)`
$$ Sia la VC $X_i$ che descrive la vincita/perdita $$X_i=-1+2R_i$$ Se
$R_i=1$ allora $X_i=-1+2\times 1=+1$, mentre se $R_i=0$ allora
$X_i=-1+2\times 0=-1$ La VC
$R_i\sim\text{Ber}(\pi=`r format(18/37,digits=4,nsmall=4)`)$ e quindi
\begin{eqnarray*}
E(R_i) &=& `r format(18/37,digits=4,nsmall=4)`\\
V(R_i) &=& `r format(18/37,digits=4,nsmall=4)`\times(1-`r format(18/37,digits=4,nsmall=4)`)
\end{eqnarray*} E quindi \begin{eqnarray*}
E(X_i) &=& -1+2\times `r format(18/37,digits=4,nsmall=4)`\\
&=&`r format(-1+2*18/37,digits=4,nsmall=4)`\\
V(X_i) &=& 2^2\times `r format(18/37,digits=4,nsmall=4)`\times(1-`r format(18/37,digits=4,nsmall=4)`)\\
&=& `r format(4*18/37*19/37,digits=4,nsmall=4)`
\end{eqnarray*} Le $X_i$ sono tutte tra di loro Indipendenti e
Identicamente Distribuite (IID). Se quindi giochiamo $n$ volte la VC che
conta il numero di euro vinti/persi è $$
S_n = X_1+...+X_n
$$ Riscrivendo in termini di $R_i$ \begin{eqnarray*}
S_n &=& -1 + 2R_1 -1 + 2R_2 +...+(-1)+2R_n\\
    &=& -n + 2(R_1+...+R_n)\\
    &=& -n + 2R
\end{eqnarray*}
$R=R_1+...+R_n\sim \text{Bin}(n;\pi=`r format(18/37,digits=4,nsmall=4)`)$
Se per esempio gioco due ($n=2$) volte $$
S_2 = X_1+X_2
$$ il supporto di $S_2$ è l'insieme $\{-2,0,+2\}$. \begin{eqnarray*}
P(S_2=-2)&=&P(R_1+R_2=0)= \binom{2}{0}`r pg`^0(1-`r pg`)^2=`r dbinom(0,2,18/37)`\\
P(S_2=0)&=&P(R_1+R_2=1)= \binom{2}{1}`r pg`^1(1-`r pg`)^1=`r dbinom(1,2,18/37)`\\
P(S_2=+2)&=&P(R_1+R_2=2)= \binom{2}{0}`r pg`^2(1-`r pg`)^0=`r dbinom(2,2,18/37)`
\end{eqnarray*}

```{r 08-tlc-11}
# n <- 2
# plot(c(-n-.6,n+.6),c(0,.5),type="n",axes=F,xlab="x",ylab="f(x)")
# rect(-n+2*(0:2)-.5,0,-n+2*(0:2)+.5,dbinom(0:2,n,pn))
# axis(1,-n+2*(0:2))
# axis(2)
```

Se per esempio gioco tre ($n=3$) volte $$
S_2 = X_1+X_2+X_3=S_2+X_3
$$ il supporto di $S_3$ è l'insieme $\{-3,-1,+1,+3\}$. \begin{eqnarray*}
P(S_3=-3)&=&P(R_1+R_2+R_3=0)= \binom{3}{0}`r pg`^0(1-`r pg`)^3=`r dbinom(0,3,18/37)`\\
P(S_3=-1)&=&P(R_1+R_2+R_3=1)= \binom{3}{1}`r pg`^1(1-`r pg`)^2=`r dbinom(1,3,18/37)`\\
P(S_3=+1)&=&P(R_1+R_2+R_3=2)= \binom{3}{2}`r pg`^2(1-`r pg`)^1=`r dbinom(2,3,18/37)`\\
P(S_3=+3)&=&P(R_1+R_2+R_3=3)= \binom{3}{3}`r pg`^3(1-`r pg`)^0=`r dbinom(3,3,18/37)`
\end{eqnarray*}

<!-- ```{r 08-tlc-12} -->
<!-- n <- 3 -->
<!-- plot(c(-n-.6,n+.6),c(0,.5),type="n",axes=F,xlab="x",ylab="f(x)") -->
<!-- rect(-n+2*(0:n)-.5,0,-n+2*(0:n)+.5,dbinom(0:n,n,pn)) -->
<!-- axis(1,-n+2*(0:n)) -->
<!-- axis(2) -->
<!-- ``` -->

Se per esempio gioco tre ($n=10$) volte 
$$
S_{10} = S_9+X_{10}
$$

Se per esempio gioco tre ($n=100$) volte 
$$
S_{100} = S_{99}+X_{100}
$$ 
La probabilità di non perdere è data da. 

\begin{eqnarray*}
P(S_{100}>0)&=& P(-100+2R>0)\\
 &=& P(R>100/2)\\
&=&P(R>50)\\
&=&P(R=51)+P(R=52)+...+P(R=100)\\
&=&\binom{100}{51}`r pg`^{51}(1-`r pg`)^{49}+\binom{100}{52}`r pg`^{52}(1-`r pg`)^{48}+...\\
&+& \binom{100}{100}`r pg`^{100}(1-`r pg`)^{0}\\
&=& `r sum(dbinom(51:100,100,pn))`
\end{eqnarray*}

```{r 08-tlc-13}
par(mfrow=c(1,2),cex=cex)
n <- 2
plot(c(-n-1.1,n+1.1),c(0,.25),type="n",axes=F,xlab="x",ylab="f(x)",main = "n = 2")
rect(-n+2*(0:2)-1,0,-n+2*(0:2)+1,dbinom(0:2,n,pn)/2,col=ablue)
axis(1,-n+2*(0:2))
axis(2)

n <- 3
plot(c(-n-1.1,n+1.1),c(0,.25),type="n",axes=F,xlab="x",ylab="f(x)",main = "n = 3")
rect(-n+2*(0:n)-1,0,-n+2*(0:n)+1,dbinom(0:n,n,pn)/2,col=ablue)
axis(1,-n+2*(0:n))
axis(2)
```

```{r 08-tlc-14}
par(mfrow=c(1,2),cex=cex)
n <- 10
plot(c(-n-1.1,n+1.1),c(0,.15),type="n",axes=F,xlab="x",ylab="f(x)",main = "n = 10")
rect(-n+2*(0:n)-1,0,-n+2*(0:n)+1,dbinom(0:n,n,pn)/2,col=ablue)
axis(1,-n+2*(0:n))
axis(2)

n <- 100
plot(c(-n-.6,n+.6),c(0,.045),type="n",axes=F,xlab="x",ylab="f(x)",main = "n = 100")
rect(-n+2*(0:n)-1,0,-n+2*(0:n)+1,dbinom(0:n,n,pn)/2,col=ablue,border = ablue)
axis(1)
axis(2)
par(mfrow=c(1,1),cex=cex)
```

**Approssimazione normale** 

\begin{eqnarray*}
\mu &=& E(X_i)\\
&=& -1+2\times `r pg`\\
&=&`r format(-1+2*18/37,digits=4,nsmall=4)`\\
\sigma^2 &=& V(X_i) \\
&=& 2^2\times `r format(18/37,digits=4,nsmall=4)`\times(1-`r format(18/37,digits=4,nsmall=4)`)\\
&=& `r format(4*18/37*19/37,digits=4,nsmall=4)`
\end{eqnarray*}

`r vg <- format(4*18/37*19/37,digits=4,nsmall=4)`
`r vx <- 4*18/37*19/37` posto 
$$
S_n = X_1+...+X_n
$$ 
e dunque

\begin{eqnarray*}
E(S_n) &=& E(X_1+...+X_n)\\
       &=& E(X_1)+...+E(X_n)\\
       &=& \mu+...+\mu\\
       &=& n\mu\\
       &=& `r n*xn`\\
V(S_n) &=& V(X_1+...+X_n)\\
       &=& V(X_1)+...+V(X_n)\\
       &=&\sigma^2+...+\sigma^2\\
       &=& n\sigma^2\\
       &=& n\times `r vg`
\end{eqnarray*}

Per $n=100$

\begin{eqnarray*}
E(S_n) &=& 100\times (`r xg`)\\
       &=& `r 100*xn`\\
V(S_n) &=& 100\times `r vg`\\
       &=& `r 100*vx`
\end{eqnarray*}

In virtù del teorema del limite centrale

\begin{eqnarray*}
S_{100}&&\operatorname*{\sim}_{a} N(100\times(`r xg`),100\times`r vg`)\\
&& \operatorname*{\sim}_{a}  N(`r 100*xn`,`r 100*vx`)
\end{eqnarray*}

E quindi

```{r 08-tlc-5,results='asis'}
vnam<-"S_n"
mm <- round(100*xn,4)
ss <- 100*vn
x0 <- 0
x1 <- 0
cat(norm_int(x1 = x1,verso = ">",mm = mm,ss = ss,vnam = vnam,mu = "E(S_n)",sigma = "\\sqrt{V(S_n)}"))
```

```{r 08-tlc-15}
curve(dnorm(x,mm,sqrt(ss)),mm-4*ss^.5,+4*ss^.5,axes=F,ylab=expression(f(S[n])),xlab = expression(S[n]))
curve(dnorm(x,mm,sqrt(ss)),1,+4*ss^.5,add=T,type="h",n=1001,col="gray")
n <- 100
rect(-n+2*(0:n)-1,0,-n+2*(0:n)+1,dbinom(0:n,n,pn)/2)
axis(1,-n+2*(0:n))
axis(2)

papp <- 1-pnorm(0,mm,sqrt(ss))
pexx <- sum(dbinom(51:100,100,pn))
```

::: {.nota data-latex=""}
I due valori di probabilità esatta `r pexx` e quello approssimato
`r papp` sono diversi nella seconda cifra decimale. Questo è dovuto al
fatto che la normale calcola anche parte dell'istogramma della binomiale
in zero. Per ovviare e migliorare l'approssimazione basta spostare sulla
fine del rettangolo dello zero il calcolo della normale. Senza entrare
nel dettaglio si ricava che il rettangolo finisce per $S_n=1$ e dunque

```{r 08-tlc-6, results='asis'}
vnam<-"S_n"
mm <- round(100*xn,4)
ss <- 100*vn
x1 <- 1
cat(norm_int(x1 = x1,verso = ">",mm = mm,ss = ss,vnam = vnam,mu = "E(S_n)",sigma = "\\sqrt{V(S_n)}"))
```

:::


<!--chapter:end:08-tlc.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup09, include=FALSE}



source("intro.R")

```

# Statistiche campionarie

## Risultati preliminari

Consideriamo $n$ variabili casuali, $X_1,...,X_n$, IID, con valore atteso e varianza, rispettivamente, $\mu$ e $\sigma^2$. Sono dati che stiamo per osservare e quindi sono casuali.


:::: {.info data-latex=""}
::: {.definition}
Una **statistica campionaria**, $S$, è una funzione dei dati $X_1,...,X_n$
\[S(X_1,...,X_n)=s\in\mathbb{R}\]
:::
::::

Come funzione di VC $S=S(X_1,...,X_n)$ è una VC. Ad esempio la _media_ dei dati è una statistica, il _25-esimo percentile_ è una statistica,
la _mediana_ dei dati, la _varianza_, ecc.
Ci possiamo porre alcune domande, per esmepio:

- come si distribuisce una media campionaria di valori casuali?
\[\hat \mu=\bar X \operatorname*{\sim}_a N\left(\mu,\frac{\sigma^2}n\right)\]
dove, $E(\hat\mu)=\mu$, $V(\hat\mu)=\frac{\sigma^2}n$
- Come si distribuisce la varianza campionaria di valori casuali?
\[\hat\sigma^2=\frac 1 n \sum_{i=1}^n (X_i-\hat \mu)^2 \sim ?\]

<!-- - Come si distribuisce la _standardizzazione_ di $\bar X$ dati rispetto a $\mu$? -->
<!-- \[\frac{\hat \mu-\mu}{{\hat\sigma}/{\sqrt{n-1}}}\sim ? \] -->

## La distribuzione Chi-quadro $\chi^2$

:::: {.info data-latex=""}
::: {.definition}
Siano $Z_1,...,Z_n$, $n$ VC, IID, $Z_i\sim N(0,1)$, posto,
\[Y=Z_1^2+...+Z_n^2, \qquad \text{allora} \qquad Y\sim \chi^2_n\]
La distribuzione della somma del quadrato di $n$ normali standard è distribuita come un chi-quadro con $n$ _gradi di libertà_
:::
::::

::: {.info data-latex=""}
La VC ha come supporto tutta la retta reale positiva:
\[S_Y=\{y>0\}=\mathbb{R}^+\]
Lo spazio dei parametri non ha interesse statistico
\[n\in\mathbb{Z}^+\]
:::

Funzione di probabilista o densità:
\[
f(x)=\dfrac {x^{{\frac {n}{2}}-1}e^{-{\frac {x}{2}}}}{2^{\frac {n}{2}}\Gamma \left({\frac {n}{2}}\right)}
\]
dove
\[
\Gamma(t)=\int_0^{+\infty}x^{t-1}e^{-x}dx
\]

::: {.info data-latex=""}

  - per $n = 1$ ha una forma iperbolica;
  - per $n>2$ e a forma campanulare con un'asimmetria positiva (coda lunga a dx);
  - in virtù del TLC se $n$ diverge allora $Y\stackrel{\sim}{a}N(n,2n)$.
\[E(Y)=n,\qquad V(Y)=2n\]
:::

In figura \@ref(fig:chi-quadro1) e \@ref(fig:chi-quadro1) la forma della densità al variare di $n$.

```{r chi-quadro1, echo=FALSE, results='asis',fig.cap="La densità della VC Chi-quadro per diversi valori di $n$"}
dof <- c(1,5,10,50)
par(mfrow=c(1,2),cex=cex)
for(i in dof[1:2]){
curve(dchisq(x,i),0,qchisq(.999,i),main=paste("Chi-quadro, gdl=",i),xlab=expression(chi^2),ylab=expression(f(chi^2)),axes=F)
  axis(1)
  axis(2)
}
```
```{r chi-quadro2, echo=FALSE, results='asis',fig.cap="La densità della VC Chi-quadro per diversi valori di $n$"}
dof <- c(1,5,10,50)
par(mfrow=c(1,2),cex=cex)
for(i in dof[3:4]){
curve(dchisq(x,i),0,qchisq(.999,i),main=paste("Chi-quadro, gdl=",i),xlab=expression(chi^2),ylab=expression(f(chi^2)),axes=F)
  axis(1)
  axis(2)
}
```


### Le tavole del $\chi^2$

Non c'è una sola distribuzione $\chi^2$ ma tante quante sono i possibili gradi di libertà.
Per comodità editoriale vengono mostrati solo alcuni valori delle code, per alcuni gradi di libertà.
Offrono il percentile della $\chi^2$ per diversi gradi di libertà e diversi valori di 
$\alpha$, ovvero
\[P(\chi^2_n>\chi^2_{n;\alpha})=\alpha\]
per alcuni valori di $n$ e di $\alpha$.
Le tavole si presentano in forma tabellare dove ogni riga è indicizzata dal grado di libertà e ogni colonna dal valore di probabilità $\alpha$.

```{r 09-Statistiche-Campionarie_3}
par(mfrow=c(1,2),cex=cex)
alp <- 0.1
gdl <- 1
curve(dchisq(x,gdl),0,13,axes=F,ylab=expression(f(chi^2)),xlab = expression(chi^2))
axis(1)
axis(2)
draw_dist(function(x)dchisq(x,gdl),qchisq(1-alp,gdl),30,col=ablue)
axis(1,qchisq(1-alp,gdl),expression(chi[0.1]^2))
title(paste("gdl=",gdl,", alpha=",alp))

alp <- 0.05
gdl <- 3
curve(dchisq(x,gdl),0,18,axes=F,ylab=expression(f(chi^2)),xlab = expression(chi^2))
axis(1)
axis(2)
draw_dist(function(x)dchisq(x,gdl),qchisq(1-alp,gdl),30,col=ablue)
axis(1,qchisq(1-alp,gdl),expression(chi[0.05]^2))
title(paste("gdl=",gdl,", alpha=",alp))
```
```{r 09-Statistiche-Campionarie_4}
par(mfrow=c(1,2),cex=cex)
alp <- 0.025
gdl <- 10
curve(dchisq(x,gdl),0,32,axes=F,ylab=expression(f(chi^2)),xlab = expression(chi^2))
axis(1,c(0,5,10,15,25,30))
axis(2)
#curve(dchisq(x,gdl),qchisq(1-alp,gdl),type="h",add=T,col=ablue,n = 501)
draw_dist(function(x)dchisq(x,gdl),qchisq(1-alp,gdl),30,col=ablue)
axis(1,qchisq(1-alp,gdl),expression(chi[0.025]^2))
title(paste("gdl=",gdl,", alpha=",alp))
alp <- 0.95
gdl <- 30
curve(dchisq(x,gdl),0,63,axes=F,ylab=expression(f(chi^2)),xlab = expression(chi^2))
axis(1,c(0,10,30,40,50,60))
axis(2)
draw_dist(function(x)dchisq(x,gdl),qchisq(1-alp,gdl),100,col=ablue)
axis(1,qchisq(1-alp,gdl),expression(chi[0.95]^2))
title(paste("gdl=",gdl,", alpha=",alp))
par(mfrow=c(1,1),cex=cex)

```

```{r chi1}
alp <- c(0.9995 , 0.999 , 0.995 , 0.99 , 0.975 , 0.95 , 0.9 , 0.1 , 0.05 , 0.025 , 0.01 , 0.005 , 0.001 , 0.0005)
gdl <- 1:8
GdL <- gdl

tab <- format(round(outer(gdl,alp, function(x,y)qchisq(1-y,x)),5),digits = 4,nsmall = 4,scipen=1e10)
tab <- cbind(gdl,tab)
colnames(tab)<- c("GdL",paste("$\\alpha=$",format(alp,digits = 5,nsmall = 5)))
rownames(tab)<- gdl
kable(tab[,1:8],booktabs=T,escape=F,caption = "Prime 8 righe delle tavole del $\\chi^2$ (1/2)",linesep="",row.names=F,align="r") %>%
  kable_styling(font_size = 7)
```


```{r chi2}
kable(cbind(GdL,tab[,9:15]),booktabs=T,escape=F,linesep="",caption = "Prime 8 righe delle tavole $\\chi^2$ (2/2)",row.names=F,align='r') %>%
  kable_styling(font_size = 7)
```


Quindi per esempio se sono interessato a sapere quale valore del $\chi_3^2$ lascia alla sua destra 
lo 0.05 dell'area dovrò cercare sulla terza riga in corrispondenza della colonna 0.05
e quindi
\[
\chi_{3;0.05}^2=`r qchisq(.95,3)`
\]

## La distribuzione $t$-di Student

:::: {.info data-latex=""}
::: {.definition}
Siano $Z\sim N(0,1)$ e $Y\sim\chi^2_n$, $Z$ e $Y$ indipendenti, posto,
\[T=\frac Z{\sqrt{Y/n}} \qquad \text{allora} \qquad T\sim t_n\]
Il rapporto tra una normale standard e un la radice di un chi-quadro diviso per i suoi gradi di libertà è distribuito come una $t$-Student con $n$ _gradi di libertà_
:::
::::

:::: {.info data-latex=""}
La VC ha come supporto tutta la retta reale:
\[S_T=\mathbb{R}\]
Lo spazio dei parametri non ha interesse statistico
\[n\in\mathbb{Z}^+\]

Funzione di probabilista o densità.

  - è a forma campanulare
  - è simmetrica rispetto a zero
  - all'aumentare di $n$ le code si abbassano
  - Se $n\to\infty$, allora $t_n\to N(0,1)$

\[E(Y)=0,\qquad V(Y)=\frac{n}{n-2}\]
::::

In figura \@ref(fig:t-student1) e \@ref(fig:t-student2) il confronto tra la $t$-di Student e la normale standard, per diversi valori di $n$.

```{r t-student1, echo=FALSE, results='asis',fig.cap="confronto tra la $t$-di student e la normale standard, per diversi valori di $n$"}
dof <- c(1,5,10,40)
par(mfrow=c(1,2),cex=cex*.5)
for(i in dof[1:2]){
curve(dt(x,i),-6,6,main=paste("t-student, gdl=",i),xlab=expression(t),ylab=expression(f(t)),ylim=c(0,.41),lwd=2)
curve(dnorm(x),col=ared,add=T)
}
```

```{r t-student2, echo=FALSE, results='asis',fig.cap="confronto tra la $t$-di student e la normale standard, per diversi valori di $n$"}
dof <- c(1,5,10,40)
par(mfrow=c(1,2),cex=cex*.5)
for(i in dof[3:4]){
curve(dt(x,i),-6,6,main=paste("t-student, gdl=",i),xlab=expression(t),ylab=expression(f(t)),ylim=c(0,.41),lwd=2)
curve(dnorm(x),col=ared,add=T)
}
```

### Le tavole della $t$

Non c'è una sola distribuzione $t$ ma tante quante sono i possibili gradi di libertà.
Per comodità editoriale vengono mostrati solo alcuni valori delle code, per alcuni gradi di libertà.
Sulle tavole leggiamo:
\[
P(T>t_{n;\alpha})=\alpha
\]
per alcuni valori di $n$ e di $\alpha$.
Le tavole si presentano in forma tabellare dove ogni riga è indicizzata dal grado di libertà e ogni colonna dal valore di probabilità $\alpha$.
Per conoscere quale valore $t_{6;0.025}$ della $t_6$ con 6 gradi di libertà, tale che
\[
P(T>t_{6;0.025})=0.025
\]

```{r 09-Statistiche-Campionarie-5}
options(digits = 5)

alp <- c(0.1,0.05,0.025,0.01,0.005,0.001,0.0005)
gdl <- c(1:50,60,70,80,90,100,+Inf)
nam <- c("GdL",paste("$\\alpha=",alp,"$",sep=""))
zap <- format(qnorm(1-alp),digits = 5)
pap <- 1-alp

# r1 <- paste(nam[1],paste(alp,collapse = " & ",sep="& "),sep=" & ")
# r2 <- paste(nam[2],paste(zap,collapse = " & ",sep="& "),sep=" & ")
# r3 <- paste(nam[3],paste(pap,collapse = " & ",sep="& "),sep=" & ")
# xx <- seq(0,.09,by=0.01)
# XX <- seq(0,4,by=0.1)
continua <- rep("...",times=length(alp)+1)
tab <- format(outer(gdl,alp,function(x,y)qt(1-y,x)))
tab <- cbind(gdl,tab)
dimnames(tab)[[2]] <- nam
kable(rbind(tab[1:10,],continua),row.names = F,booktabs = T, escape = F,linesep = "", digits = 5,align='r',
      caption = " $\\alpha=0.025$, con 6 gradi di libertà")%>%
  column_spec(4,color = ared,border_left = T,border_right = T)%>%
  row_spec(6,color = ared,hline_after = T)%>%
  row_spec(5,hline_after = T)%>%
  kable_styling(font_size = 7)


options(digits = 5)
```

<!-- ![](img/tavt1.png) -->

Dunque $t_{6;0.025}=`r qt(1-.025,6)`$

```{r 09-Statistiche-Campionarie_1,fig.align='center'}
curve(dt(x,6),-5,5, axes = F, xlab='t',ylab='f(t)')
axis(1,c(-5,0,round(qt(.975,6),4),5))
axis(2)
curve(dt(x,6),round(qt(.975,6),4),5,n = 501, add=T,type='h',col='grey')
text(0,.1,0.975)
text(3.5,.025,0.025)
```

Per conoscere quale valore $t_{15;0.005}$ della $t_{15}$ con 15 gradi di libertà, tale che
\[P(T>t_{15;0.005})=0.005\]

Dunque $t_{15;0.005}=`r qt(1-.005,15)`$

```{r 09-Statistiche-Campionarie-6}
kable(rbind(continua,tab[13:17,],continua),row.names = F,booktabs = T, escape = F,linesep = "", digits = 5,align='r',caption = "$\\alpha=0.005$, con 15 gradi di libertà")%>%
  column_spec(6,color = ared,border_left = T,border_right = T)%>%
  row_spec(4,color = ared,hline_after = T)%>%
  row_spec(3,hline_after = T)%>%
  kable_styling(font_size = 7)
```


```{r 09-Statistiche-Campionarie_2,fig.align='center'}
curve(dt(x,15),-5,5, axes = F, xlab='t',ylab='f(t)')
axis(1,c(-5,0,round(qt(.995,15),4),5))
axis(2)
curve(dt(x,15),round(qt(.995,15),4),5,n = 501, add=T,type='h',col='grey')
text(0,.1,0.995)
text(4,.015,0.005)
```

Per conoscere quale valore $t_{49;0.0005}$ della $t_{49}$ con 49 gradi di libertà, tale che
\[P(T>t_{49;0.001})=0.001\]

```{r 09-Statistiche-Campionarie-7}
kable(rbind(continua,tab[46:50,],continua),row.names = F,booktabs = T, escape = F,linesep = "", digits = 5,align='r',caption = "$\\alpha=0.0005$, con 49 gradi di libertà")%>%
  column_spec(7,color = ared,border_left = T,border_right = T)%>%
  row_spec(5,color = ared,hline_after = T)%>%
  row_spec(4,hline_after = T)%>%
  kable_styling(font_size = 7)
options(digits = 4)
```

Dunque $t_{49;0.001}=`r qt(1-.001,49)`$


:::: {.nota data-latex=""}
L'ultima riga della $t$, per un numero infinito di GdL, coincide con la tabella aggiuntiva della $Z$. Infatti
\scriptsize\vspace{10 pt}

::: {.center data-latex=""}
```{r 09-Statistiche-Campionarie_8}
options(digits = 4)
nam <- c("\\alpha","z_\\alpha","\\Phi(z_\\alpha)=1-\\alpha")
alp <- c(0.1,0.05,0.025,0.01,0.005,0.001,0.0005)
zap <- format(qnorm(1-alp),digits = 5)
pap <- 1-alp

r1 <- paste(nam[1],paste(alp,collapse = " & ",sep="& "),sep=" & ")
r2 <- paste(nam[2],paste(paste("& \\color{red}{",paste(zap,collapse = "} &\\color{red}{",sep="& ")),sep=" & "),"}")
r3 <- paste(nam[3],paste(pap,collapse = " & ",sep="& "),sep=" & ")
tab[56,1]<- "$\\infty$"
tabl(tab[53:56,]) %>%
  row_spec(4,color = ared)%>%
  kable_styling(font_size = 12)
```
:::
\vspace{10 pt}
\normalsize
La tabella dei percentili della normale è
\scriptsize

\[\begin{array}{lrrrrrrr}
\hline
`r r1` \\ 
`r r2` \\ 
`r r3` \\
\hline
\end{array}\]

::::

\normalsize
## La distribuzione di $\hat\sigma^2$

Siano $X_1,...,X_n$, $n$ VC IID, replicazioni della stessa $X\sim N(\mu,\sigma^2)$.
La si definisce la varianza campionaria

\[\hat\sigma^2=\frac 1 n \sum_{i=1}^n(X_i-\hat \mu)^2, \qquad \hat \mu=\bar X=\frac 1 n \sum_{i=1}^nX_i\]

allora

\[ \hat\sigma^2\sim\frac{\sigma^2}n\chi^2_{n-1}.\]

Osserviamo:

\begin{eqnarray*}
E(\hat\sigma^2) &=& \frac {\sigma^2}n E(\chi^2_{n-1})\\
                &=& \frac{n-1}n \sigma^2
\end{eqnarray*}

## La distribuzione della statistica standardizzata

Siano $X_1,...,X_n$, $n$ VC, IID, $X_i\sim N(\mu,\sigma^2)$. Si definisce _standardizzazione_ di $\bar X$ dati rispetto a $\mu$:
\[T=\frac{\hat \mu-\mu}{{\hat\sigma}/{\sqrt{n-1}}}\]
Allora
\[T\sim t_{n-1}. \]

<!--chapter:end:09-Statistiche-Campionarie.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
\part{Inferenza}

```{r setup10, include=FALSE}
source("intro.R")
```

# Inferenza: concetti introduttivi

<!-- Inferire è un termine che trova le sue radici nella filosofia e si estende in numerosi ambiti del sapere. Nella sua essenza, inferire significa trarre una conclusione a partire da premesse date, ma il modo in cui questo processo viene attuato varia in base al contesto. Sebbene l'inferenza abbia applicazioni in diversi campi, quando si parla comunemente di "inferenza", ci si riferisce spesso all'inferenza nel contesto statistico. -->

<!-- In statistica, inferire si traduce nell'arte di estrarre conclusioni significative e informazioni da dati e campioni, un processo cruciale nella ricerca e nell'analisi dei dati. La Treccani definisce l'atto di inferire come: -->

<!-- 2a Trarre, partendo da una determinata premessa o dalla constatazione di un fatto, una conseguenza, un giudizio, una conclusione. -->

<!-- Questa definizione si adatta perfettamente al contesto statistico, dove l'attenzione è focalizzata sull'interpretazione e sulle implicazioni dei dati raccolti. -->

Dalla Treccani si definisce l'atto di inferire come: 

> Trarre, partendo da una determinata premessa o dalla constatazione di un fatto, una conseguenza, un giudizio, una conclusione.

Inferire è un atto fondamentale non solo del pensiero umano, ma anche della vita stessa. Gran parte delle specie evolute possiede un istinto innato per trarre conclusioni a partire dall'osservazione dell'ambiente circostante. Che si tratti di un animale che associa il rumore di un predatore al pericolo o di una pianta che risponde agli stimoli luminosi per orientare la crescita, l'inferenza è una strategia essenziale per adattarsi e sopravvivere.

L'uomo, tuttavia, si distingue per un aspetto cruciale: la capacità di prendere coscienza di questo processo, di formalizzarlo e di costruirci sopra un linguaggio. La riflessione sull'inferenza, il tentativo di comprendere e replicare consapevolmente ciò che molte specie fanno istintivamente, ha dato origine a discipline come la logica, la matematica e, infine, la statistica. Studiando l'inferenza, l'uomo è riuscito a trasformare un'abilità naturale in un processo razionale, formale e comunicabile, che non solo migliora la comprensione del mondo, ma rende possibile la condivisione e la riproduzione di quel sapere.

Esistono diverse modalità di inferenza, che riflettono modi distinti di trarre conclusioni: **deduttiva**, **induttiva diretta** e **induttiva inversa**.

L'inferenza deduttiva, tipica della logica formale, parte da premesse certe per arrivare a conclusioni necessarie. È il tipo di ragionamento che governa la matematica: dati alcuni assunti iniziali, le conclusioni sono logicamente inevitabili e già contenute nelle premesse. Questo tipo di inferenza è deterministico e privo di incertezze.

L'inferenza induttiva diretta, invece, opera nel campo della probabilità. Partendo da un modello noto o da una distribuzione che descrive un fenomeno, permette di calcolare la probabilità di osservare determinati esiti. È il caso di un'urna dalla composizione nota: possiamo determinare con precisione la probabilità che un'estrazione casuale produca un certo risultato. Qui il ragionamento è incerto, ma il modello è dato a priori e costituisce la base delle conclusioni.

L'inferenza induttiva inversa, oggi conosciuta come **inferenza statistica**, ribalta questa prospettiva. Non ci si chiede quali osservazioni aspettarsi da un modello noto, ma quale modello, quali parametri o quali proprietà della popolazione possano spiegare i dati osservati. È il problema dell'urna dalla composizione incognita: data una serie di estrazioni, come possiamo inferire la composizione complessiva? Questo tipo di inferenza è intrinsecamente probabilistico e dipende fortemente dal contesto e dalle ipotesi di partenza.

Ciò che rende l'inferenza statistica così straordinaria è proprio la sua capacità di trasformare osservazioni parziali, influenzate dal caso, in una comprensione più ampia e generalizzabile. Ma ciò che la rende possibile è il linguaggio formale che l'uomo ha costruito per descriverla. Questo linguaggio non solo rende espliciti i passaggi impliciti del ragionamento, ma permette di comunicare e replicare i risultati, superando i limiti del contesto individuale.

In definitiva, l'inferenza statistica è l'espressione più raffinata di una capacità condivisa con molte specie, ma che l'uomo ha elevato a strumento consapevole e rigoroso. Essa rappresenta un ponte tra l'istinto naturale e la razionalità formale, dimostrando come un processo profondamente radicato nella biologia possa essere trasformato in un metodo universale per comprendere il mondo.

Per comprendere la differenza tra inferenza induttiva diretta e inversa, consideriamo il classico esempio di un'urna contenente bussolotti di due colori: rosso e blu.



## Inferenza da popolazioni finite

Le popolazioni finite rappresentano il contesto più vicino all'intuizione comune. Si tratta di insiemi chiusi e completamente enumerabili di unità, come le persone registrate in un censimento o gli studenti iscritti a un corso. In questi casi, ogni elemento della popolazione può essere associato a una lista e il campione rappresenta una frazione precisa della popolazione. Questo contesto è tipico delle **statistiche ufficiali**, come quelle prodotte dall'ISTAT.

Esempi concreti di domande a cui risponde l'inferenza da popolazioni finite includono:

- **Censimenti**: Quanti cittadini vivono in Italia? Qual è il numero medio di componenti per nucleo familiare? Quanti studenti frequentano le scuole secondarie?
- **Indagini campionarie**: Qual è il reddito medio delle famiglie italiane? Quale percentuale di famiglie ha accesso a Internet?
- **Sondaggi elettorali**: Qual è la proporzione degli elettori intenzionati a votare per un certo partito politico?

Queste analisi si basano su campioni estratti da una popolazione nota, che è completamente enumerata. Per esempio, nel caso di un'indagine ISTAT sul reddito medio, la popolazione di riferimento è il database delle famiglie italiane registrate. Il campione estratto può essere usato per stimare parametri come la media o la mediana del reddito, con metodi che tengono conto della numerosità finita della popolazione.


## Inferenza da popolazioni infinite

Quando la popolazione non è enumerabile o è concettualmente infinita, entriamo nel campo delle popolazioni infinite. Questo contesto è comune nelle applicazioni scientifiche, dove le osservazioni rappresentano un campione di una popolazione teoricamente infinita. Esempi tipici includono:

- **Processi fisici e naturali**: Qual è la probabilità che il livello di un fiume superi una certa soglia durante la stagione delle piogge? Qual è la concentrazione media di un inquinante nell'aria in una data area?
- **Processi industriali**: Qual è la proporzione di prodotti difettosi in una linea di produzione? Quanto dura mediamente un componente prima di rompersi?
- **Eventi ripetibili**: Qual è la probabilità di ottenere testa lanciando una moneta? Qual è la frequenza con cui un cliente accede a un servizio online?

In questi contesti, la popolazione non è un insieme fisicamente limitato, ma una rappresentazione teorica. Ad esempio, nel caso di una moneta, ogni lancio è considerato indipendente dagli altri, e la popolazione è rappresentata dalla distribuzione probabilistica dei possibili esiti.

## Inferenza distribution-free e da modello

Un'ulteriore distinzione riguarda il tipo di informazione disponibile sulla distribuzione dei dati:

- Nell'inferenza **distribution-free**, ci si limita a inferire proprietà aggregate come la media o la varianza, senza assumere nulla sulla forma della distribuzione.
- Nell'inferenza **da modello**, si ipotizza una distribuzione nota (ad esempio normale, binomiale o Poisson), a meno di alcuni parametri incogniti. Questo approccio permette non solo di stimare i parametri, ma anche di rispondere a domande più complesse.

Esempi concreti di domande per questi contesti includono:

- **Distribution-free**: Qual è la mediana del reddito? Qual è la varianza del tempo impiegato dai pendolari per raggiungere il lavoro?
- **Da modello**: Qual è la probabilità che un semaforo abbia più di 10 auto in fila a mezzogiorno, sapendo che il numero di auto segue una distribuzione di Poisson? Qual è la probabilità che il livello dell'acqua in un fiume superi 150 cm, assumendo che sia distribuito normalmente?

Nel caso di un modello, l'aggiunta di un'ipotesi probabilistica consente di rispondere non solo su parametri come media e varianza, ma sull'intera distribuzione del fenomeno.

## Sintesi dei contesti

Questa classificazione dei contesti di inferenza aiuta a chiarire il tipo di domande che lo statistico può affrontare. Mentre l'inferenza da popolazioni finite si concentra su contesti pratici e limitati, tipici delle statistiche ufficiali, l'inferenza da popolazioni infinite e da modelli probabilistici consente di affrontare problemi più astratti e complessi, con applicazioni che spaziano dalla scienza alla finanza.


### 1. Binomiale da urna con dimensione nota e estrazione senza reinserimento (popolazioni finite)
Immaginiamo un'urna contenente \(N = 100\) bussolotti, di cui \(60\) rossi e \(40\) blu. Supponiamo di estrarre \(n = 10\) bussolotti **senza reinserimento**. Ogni estrazione dipende da quelle precedenti, perché il numero di bussolotti rimasti nell'urna cambia a ogni passo. 

In questo caso, il numero di bussolotti rossi osservati segue una distribuzione ipergeometrica, ma per grandi \(N\) e piccoli \(n\), può essere ben approssimata da una **binomiale** con probabilità di successo \(\pi = 0.6\). Questo modello è tipico delle **popolazioni finite**, dove il campionamento avviene da un insieme noto e completamente enumerabile. L'inferenza potrebbe riguardare proporzioni (\(\pi\)) o altre caratteristiche della composizione dell'urna.

### 2. Binomiale da urna con dimensione incognita (popolazioni infinite)
Passiamo ora a un caso più astratto: un'urna di dimensione incognita che contiene bussolotti rossi e blu in proporzione \(\pi\). Supponiamo di estrarre \(n = 10\) bussolotti **con reinserimento**, cioè ogni estrazione è indipendente dalle altre e non altera la composizione dell'urna.

In questo contesto, trattiamo l'urna come una popolazione teoricamente infinita, in cui ogni estrazione è un evento Bernoulliano con probabilità di successo \(\pi\). Il numero di bussolotti rossi osservati segue una distribuzione binomiale con parametri \(n\) e \(\pi\). Questo modello è tipico delle **popolazioni infinite**, dove l'inferenza si concentra su proporzioni e probabilità, senza fare riferimento a una numerosità finita.

### 3. Urna con palline numerate, in numero finito, con distribuzione incognita (popolazioni finite, inferenza distribution free)
Immaginiamo un'urna contenente \(N = 100\) palline numerate con valori sconosciuti, ad esempio: \(x_1, x_2, \dots, x_{100}\). Estraiamo \(n = 10\) palline **senza reinserimento**, registrandone i valori numerici.

Qui non facciamo alcuna ipotesi sulla distribuzione dei numeri nell'urna, ma possiamo comunque fare inferenza su alcune proprietà aggregate della popolazione, come la media \(\mu\), la mediana o la varianza \(\sigma^2\). Questo tipo di analisi è detto **distribution free**, perché non assume alcuna forma specifica per la distribuzione sottostante. È tipico delle **popolazioni finite**, dove l'obiettivo è spesso stimare parametri descrittivi senza vincoli modellistici.

### 4. Urna con infinite palline numerate negli interi, con distribuzione Poisson, legata a \(\lambda\) (popolazioni infinite, inferenza da modello)
Immaginiamo un'urna teorica contenente un numero infinito di palline numerate con valori interi \(0, 1, 2, \dots\). La probabilità di estrarre una pallina con un dato numero \(k\) segue una distribuzione di Poisson con parametro \(\lambda\), che rappresenta la media (e varianza) del processo.

Questo modello è usato per descrivere fenomeni come il numero di eventi in un intervallo di tempo o spazio. Supponiamo di osservare \(n = 10\) estrazioni indipendenti: possiamo fare inferenza sul parametro \(\lambda\), che caratterizza completamente la distribuzione. Qui entriamo nel campo delle **popolazioni infinite**, dove l'inferenza si basa su un modello probabilistico noto.

### 5. Urna con infinite palline nel reale, con distribuzione normale, legata a \(\mu, \sigma^2\) (popolazioni infinite, inferenza da modello)
Infine, consideriamo un'urna teorica contenente un numero infinito di palline, ciascuna numerata con un valore reale. La probabilità di estrarre una pallina con un dato valore segue una distribuzione normale \(N(\mu, \sigma^2)\), dove \(\mu\) è la media e \(\sigma^2\) la varianza.

In questo caso, ogni estrazione è un'osservazione continua indipendente dalla precedente. Supponiamo di osservare \(n = 30\) estrazioni: l'obiettivo è fare inferenza su \(\mu\) e \(\sigma^2\), i parametri che caratterizzano la distribuzione. Questo modello, legato alle **popolazioni infinite**, è fondamentale in molte applicazioni statistiche, perché permette di fare inferenze non solo sui parametri, ma anche su eventi futuri.

Questi esempi mostrano come il modello dell'urna possa essere generalizzato, passando da popolazioni finite e concrete a contesti astratti e teorici, mantenendo un filo conduttore nell'inferenza che si desidera effettuare. Che ne pensi?

Nel proseguire, ci concentreremo sull'inferenza da popolazioni infinite e da modelli probabilistici, che sono centrali per lo studio dell'economia e della scienza dei dati. Tuttavia, l'approccio sarà sempre lo stesso: estrarre il massimo di informazioni dai dati osservati, bilanciando rigore e comprensibilità.

<!-- **Inferenza induttiva diretta** -->

<!-- Immaginiamo che l'urna contenga una proporzione nota di bussolotti rossi e blu: il 60% sono rossi (\(\pi = 0.6\)) e il 40% blu (\(1 - \pi = 0.4\)). Supponiamo di estrarre 5 bussolotti a caso con reinserimento, cioè ogni estrazione è indipendente dalle altre. Possiamo calcolare la probabilità di ottenere un certo numero di bussolotti rossi su 5 estrazioni. Ad esempio: -->

<!-- > Qual è la probabilità di ottenere esattamente 3 bussolotti rossi? -->

<!-- Sappiamo che le estrazioni seguono una **distribuzione binomiale** con \(n = 5\) e \(\pi = 0.6\). La probabilità di osservare \(k = 3\) rossi è data da: -->

<!-- \[ -->
<!-- P(X = 3) = \binom{5}{3} \pi^3 (1-\pi)^2 = \frac{5!}{3!(5-3)!} (0.6)^3 (0.4)^2 -->
<!-- \] -->

<!-- Calcolando, otteniamo: -->

<!-- \[ -->
<!-- P(X = 3) = 10 \cdot (0.6)^3 \cdot (0.4)^2 = 10 \cdot 0.216 \cdot 0.16 = 0.3456 -->
<!-- \] -->

<!-- Quindi, la probabilità di ottenere esattamente 3 bussolotti rossi è \(P(X = 3) = 0.3456\). Questo è un esempio di **inferenza induttiva diretta**: partendo da un modello noto (la composizione dell'urna), possiamo calcolare la probabilità di osservare un dato risultato. -->

<!-- **Inferenza induttiva inversa** -->

<!-- Consideriamo ora il caso inverso: supponiamo di non conoscere la composizione dell'urna, ma di sapere che ogni estrazione segue una distribuzione Bernoulli identica e indipendente. Dopo 5 estrazioni, osserviamo il seguente risultato: -->

<!-- \[ -->
<!-- \text{Rosso, Rosso, Blu, Rosso, Blu} -->
<!-- \] -->

<!-- In totale, abbiamo 3 bussolotti rossi su 5 estrazioni. Ora la domanda è: -->

<!-- > Qual è la proporzione di bussolotti rossi nell'urna (\(\pi\))? -->

<!-- Questo è un esempio di **inferenza induttiva inversa**, o inferenza statistica. Non conosciamo il parametro \(\pi\) (la proporzione di rossi), ma possiamo stimarlo dai dati. Una prima stima naturale è la **frequenza relativa osservata**: -->

<!-- \[ -->
<!-- \hat{\pi} = \frac{\text{numero di rossi osservati}}{\text{numero totale di estrazioni}} = \frac{3}{5} = 0.6 -->
<!-- \] -->

<!-- Questa stima suggerisce che la proporzione di bussolotti rossi nell'urna potrebbe essere circa il 60%. Tuttavia, questa è solo una stima basata sul campione osservato e non possiamo essere certi che il parametro vero \(\pi\) sia esattamente \(0.6\). -->

<!-- **Confronto tra i due approcci** -->

<!-- La differenza tra i due tipi di inferenza è fondamentale: -->

<!-- - **Induttiva diretta**: partiamo da un modello noto (\(\pi = 0.6\)) per calcolare la probabilità di osservare un certo risultato (\(X = 3\)). -->
<!-- - **Induttiva inversa**: partiamo da un risultato osservato (\(X = 3\)) per stimare il parametro \(\pi\) che descrive la composizione dell'urna. -->

<!-- Il primo approccio si basa su un modello già definito, mentre il secondo utilizza i dati per costruire o verificare il modello stesso. Questo secondo approccio, che è il cuore dell'inferenza statistica, ci consente di trarre conclusioni sulla popolazione a partire da campioni finiti. -->

<!-- ## Inferenza deduttiva, induttiva diretta e indiretta -->

<!-- La matematica è un processo di _inferenza deduttiva_: partendo da premesse accettate come vere, giunge a conclusioni logicamente inderogabili e univoche, con ogni conclusione già implicita nelle premesse. -->

<!-- Se disponiamo di un'urna con composizione **nota**, _calcolare la probabilità_ di una certa composizione di  -->
<!-- bussolotti è un processo di _inferenza induttiva diretta_.  In questo caso, pur non potendo predire con certezza l'esito di un singolo evento, possiamo valutare le probabilità relative a vari esiti. -->

<!-- Con un'urna dalla composizione **incognita** e un insieme di dati estratti da essa, entriamo nel campo dell'**inferenza induttiva inversa** meglio nota come **inferenza statistica**. Qui, il nostro obiettivo è speculare sulla composizione totale dell'urna utilizzando le informazioni ottenute da una parte limitata di essa (il **campione**). -->

<!-- ## Inferenza da Popolazioni finite, infinite, da modello e distribution free -->

<!-- Se si conosce il numero di unità di cui è composta la popolazione ed è possibile scegliere il campione da una lista che consenta di sorteggiare il numero della lista, allora parliamo di inferenza da **popolazioni finite**. -->

<!-- Se non si conosce il numero di unità di cui è composta la popolazione, allora parliamo di inferenza da **popolazioni infinite**. -->

<!-- Se siamo interessati solo ad alcuni aspetti statistici della distribuzione della popolazione, come la sua media, la sua varianza, la mediana, ecc. allora si parla di __distribution free__. -->

<!-- Se abbiamo un'ipotesi sulla distribuzione dei dati e possiamo adottare un __modello__ noto, a meno dei suoi parametri, allora parliamo di __inferenza da modello__. -->

<!-- ::: example -->
<!-- Il reddito medio annuo degli italiani ha una _distribuzione incognita_ $X_i\sim\mathcal{L}$, con -->
<!-- valore atteso $E(X_i)=\mu$ e $V(X_i)=\sigma^2$. Estraiamo _senza reinserimento_ $n=100$ italiani a caso e osserviamo -->
<!-- \[\bar x=25\text{mila €},\qquad \hat\sigma=5\text{mila €}\] -->

<!--   - è come estrarre $n=100$ bussolotti da un'urna che contiene un numero **noto** di bussolotti ognuno numerato col reddito del singolo italiano. -->
<!--   - Cosa possiamo dire su $\mu$ e $\sigma^2$? -->
<!--   - **Inferenza da Popolazioni Finite**: conosco il numero di individui di cui è composta la popolazione $N$, posseggo la lista degli individui e campiono direttamente dalla lista. Sono interessato solo a $\mu$, $\sigma^2$ (_distribution free_) -->

<!-- ::: -->

<!-- ::: example -->
<!-- Una moneta che **non** sappiamo se è bilanciata, dunque $X_i\sim\text{Ber}(\pi)$, dove $\pi$ è incognito: la lanciamo 10 volte e otteniamo -->
<!-- \[T,T,T,C,C,T,T,T,C,T\] -->

<!--   - è come aver estratto $n=10$ bussolotti da un'urna che contiene una quantità **incognita** di bussolotti $T$ e $C$ -->
<!--   - Cosa possiamo dire su $\pi$? -->
<!--   - **Inferenza da Popolazioni Infinite**: la popolazione dei lanci è chiaramente infinita, non ha senso parlare di lista. Le ripetizioni sono chiaramente IID, il modello Bernoulli è obbligato. -->

<!-- ::: -->

<!-- ::: example -->

<!-- Il numero di automobili in fila alle 12 di un giorno lavorativo ad un dato semaforo è distribuito come una Poisson di parametro $\lambda$ **incognito**, osserviamo le auto in fila per 10 giorni feriali alle ore 12 e otteniamo -->
<!-- \[5,10,3,6,6,7,4,7,9,3\] -->

<!--   - è come aver estratto $n=10$ bussolotti da un'urna che contiene una quantità **incognita** di bussolotti numerati con numeri interi -->
<!--   - Cosa possiamo dire su $\lambda$? -->
<!--   - **Inferenza da modello su Popolazioni Infinite**: la popolazione dei giorni feriali è infinita, non ha senso parlare di lista, aggiungiamo un'informazione: _sappiamo che il modello è Poisson_ -->

<!-- ::: -->

<!-- ::: example -->

<!-- Il livello dell'acqua $X$ di un fiume, il giorno $i$, è descritto bene da una normale di $X_i\sim N(\mu,\sigma^2)$. -->
<!-- I parametri $\mu$ e $\sigma^2$ sono incogniti. Dopo $n=30$ giorni di osservazione abbiamo osservato -->
<!-- \[\hat\mu=122\text{cm},\qquad \hat\sigma=11.3\text{cm}\] -->

<!-- - Cosa possiamo dire su -->
<!-- \[P(X_{n+1}>150)=?\] -->
<!-- - **Inferenza da modello su Popolazioni Infinite**: la popolazione delle possibili misure del livello dell'acqua è infinita, non ha senso parlare di lista, aggiungiamo un'informazione: _sappiamo che il modello è Normale_, siamo interessati non solo ai parametri ma all'intera distribuzione. -->

<!-- ::: -->


## Statistica Classica

In questo libro adotteremo il paradigma della **statistica classica**, un approccio che si fonda sull'idea che i dati osservati siano manifestazioni imperfette di una realtà ideale descritta da modelli probabilistici. Pur essendo un sostenitore della prospettiva bayesiana, mi concentrerò su questo approccio tradizionale per il suo valore didattico e per la sua centralità in molti contesti applicativi.

Alla base della statistica classica vi è un'assunzione fondamentale: i parametri di un modello, come \(\mu\), \(\sigma^2\) o \(\lambda\), **esistono** in un senso ideale. Questi parametri non sono osservabili, ma rappresentano la struttura profonda e immutabile della popolazione. Ogni osservazione \(x_i\) e ogni statistica campionaria, come \(\bar{x}\) o \(s^2\), non sono altro che **ombre** di questa verità ideale, un'emanazione del parametro inosservabile \(\theta\). 

La distinzione tra ciò che è ideale e ciò che è osservabile richiama direttamente la metafora platonica della caverna. I parametri, rappresentati da lettere greche, abitano l'**iperuranio**, il regno delle verità perfette e immutabili. In questo regno, esiste una media ideale, una varianza ideale, una probabilità ideale. I dati osservati e le statistiche campionarie, invece, sono come le **ombre proiettate sulle pareti della caverna**: imperfette, corrotte dalla variabilità del campionamento, ma comunque legate a quella verità profonda che tentiamo di comprendere.

Ad esempio:

- Una singola osservazione \(x_i\) è un'ombra della struttura imposta dal parametro \(\theta\), ma la sua forma concreta è influenzata dalla variabilità intrinseca del processo probabilistico.
- La media campionaria \(\bar{x}\) è un'approssimazione imperfetta della media ideale \(\mu\), che abita l'iperuranio.
- La frequenza relativa osservata \(m/n\) riflette solo parzialmente la probabilità ideale \(\pi\) in una distribuzione Bernoulli.

Questo approccio funziona perfettamente come un **gioco mentale rigoroso**, a condizione che le assunzioni del modello siano rispettate. In particolare:

1. Le forme distributive delle variabili casuali devono essere corrette. Se assumiamo che una variabile segua una distribuzione normale, tale ipotesi deve essere giustificata dai dati o dal contesto.
2. I legami di dipendenza o indipendenza tra le osservazioni devono essere realistici. Ad esempio, l'assunzione che i dati siano indipendenti e identicamente distribuiti (IID) è cruciale in molti modelli, ma deve essere verificata.

Queste assunzioni creano il legame tra il mondo ideale e quello osservato. Senza di esse, la connessione tra le ombre (\(x_i\) e le statistiche campionarie) e la verità (\(\theta\)) si spezza, e le inferenze statistiche rischiano di essere prive di fondamento.

Ecco una revisione della chiosa, che sottolinea il carattere filosofico e potenzialmente contestabile di questa impostazione, in relazione alle radici empiriste della statistica:

L'idea che \(\theta\) esista in un regno ideale, come verità perfetta e inosservabile, riflette una visione filosofica di derivazione platonica, che può sembrare in contrasto con le radici empiriste su cui si fonda la statistica moderna. La disciplina, infatti, nasce dalla necessità di descrivere e comprendere il mondo reale attraverso dati osservabili, più vicina all'approccio britannico della conoscenza basata sull'esperienza.

Questa apparente tensione tra la ricerca della verità ideale e l'attenzione ai dati concreti non è priva di contraddizioni. La statistica classica si sviluppa su una sottile linea di compromesso: accetta l'esistenza di parametri teorici per costruire modelli potenti e rigorosi, ma dipende integralmente dalle osservazioni empiriche per avvicinarsi a quelle verità astratte. È un modello mentale che funziona all'interno delle sue assunzioni, ma che rimane soggetto a critica e revisione, specialmente quando queste assunzioni vengono meno.

Nel nostro percorso, accetteremo questo approccio come base metodologica, riconoscendo che esso rappresenta un sistema coerente, anche se non l'unica via possibile per fare inferenza. L'idea di parametri perfetti che governano i dati può essere vista come un artificio concettuale, utile per l'analisi e la modellazione, ma non necessariamente una descrizione della realtà ultima. In questo senso, la statistica classica non è solo uno strumento matematico, ma anche un prodotto di una specifica eredità filosofica, che merita di essere compresa e, quando necessario, interrogata. 

<!--chapter:end:10-Inferenza.Rmd-->

---
editor_options:
  chunk_output_type: console
output: pdf_document
---

```{r setupstim, include=FALSE}
source("intro.R")

```

# Elementi di Teoria della Stima

Stimare, in statistica, significa scegliere un valore o un insieme dei parametri di popolazione
alla luce di un campione. 

## Campionamento



Un campione casuale è un certo numero di $n$ di osservazioni prese a caso dalla popolazione $\mathscr{P}$


Un campione casuale è la migliore garanzia che abbiamo affinché la scelta dell'unità da rilevare non sia guidata da criteri che possano alterare i risultati.


Alcune informazioni possono essere usate per suddividere la Popolazione di partenza in sottogruppi prima di estrarre campioni. La _Teoria Dei Campioni_ è quella disciplina della statistica che studia come costruire un campione che sia, al tempo stesso sufficientemente piccolo e sufficientemente rappresentativo.


In questo corso guarderemo una sola forma di campionamento declinata in due modi

  - Campionamento Casuale Semplice Senza reintroduzione (equivalente delle estrazioni SR)
  - Campionamento Casuale Semplice Con reintroduzione (equivalente delle estrazioni CR)

### Lessico

Un campione in essere (prima di essere osservato) è una sequenza di VC
\[X_1,...,X_n\]

un campione osservato è un insieme di numeri
\[\mathbf{x}=(x_1,...,x_n)\]

Lo _Spazio dei Campioni_ $\mathcal{S}$ è il supporto della VC multipla $X_1,...,X_n$
è l'insieme di tutti i possibili campioni di ampiezza $n$: $(x_1,...,x_n)$ che possiamo osservare.

### Esempio al finito


Si dispone di un'urna contente 4 bussolotti
\[\{0,1,3,7\}\]

Se estraiamo SR, lo spazio dei campioni è di dimensione $\#\mathcal{S}=4(\times 6)$ ed è descritto in tabella (\@ref(tab:csr)).
Anziché tutti i 24 possibili campioni la tabella mostra solo i 4 campioni riordinati, perché il campione (0,1,3), ai nostri fini
è identico al campione (3,1,0) e al campione (0,3,1), ecc.

```{r 11-Stima-1,results="asis"}
xp <- c(0,1,3,7)

camp <- combn(xp,3)

dimnames(camp)[[1]]<- c("$x_{(1)}$","$x_{(2)}$","$x_{(3)}$")
dimnames(camp)[[2]] <- paste("C.",1:ncol(camp))
mu<-mean(xp) 
sig<-mean(xp^2)-mean(xp)^2
kable((camp),booktabs=T,escape=F,caption = "I 4 campioni riordinati nell'ipotesi di CCS SR",label = "csr") %>%
column_spec (1:ncol(camp),border_left = F, border_right = T) 
```

Se invece estraiamo CR, lo spazio dei campioni è di dimensione $\#\mathcal{S}=64$ ed è descritto in tabella (\@ref(tab:ccr)).

```{r 11-Stima-2,results="asis"}
xp <- c(0,1,3,7)
  
xxx <- expand.grid(xp,xp,xp)
xxx <- xxx[order(xxx[,1],xxx[,2],xxx[,3]),]
xord <- t(apply(xxx,1,sort))[,3:1]
cxord <- apply(xord,1,function(x)paste(x,collapse = ",") )
# cxord <- apply(xord,1,function(x)paste(x,collapse = "") )
nr <- tapply(rep(1,64),cxord,sum)
pr <- nr/sum(nr)
# 

camp <- unique(xord)

camp <- data.frame(camp)
camp$nr <- factor(nr)
camp$pr <- pr
names(camp)<- c("$x_{(1)}$","$x_{(2)}$","$x_{(3)}$","$\\#$ c.","$P(C.)$")
camp <- data.frame(t(camp))
dimnames(camp)[[2]] <- paste("C.",1:ncol(camp))
mu<-mean(xp) 
sig<-mean(xp^2)-mean(xp)^2
kable(t(camp),booktabs=T,escape=F,caption = "I 64 campioni riordinati estratti senza reinserimento.",label = "ccr") %>%
row_spec (1:ncol(camp)) %>%
#  column_spec(column = 4,background = "grey80",bold=F,col=iblue,align = 'c') %>%
kable_styling(font_size = 8)
```


### Lessico


Siano $X_1,..,X_n$ $n$ VC IID, replicazioni della stessa variabile $X\sim\mathscr{L}(\theta)$, $\theta\in\Theta$


Lettere greche per i parametri (incogniti) della popolazione e lettere latine per le osservazioni. 


 - Bernoulli: $X_1,...,X_n$ VC IID, $X_i\sim \text{Ber}(\pi)$, $\theta\equiv\pi$, $\Theta\equiv[0,1]$.
 - Poisson: $X_1,...,X_n$ VC IID, $X_i\sim \text{Pois}(\lambda)$, $\theta\equiv\lambda$, $\Theta\equiv\mathbb{R}^+$
 - Normale: $X_1,...,X_n$ VC IID, $X_i\sim N(\mu,\sigma^2)$, $\theta\equiv(\mu,\sigma^2)$, $\Theta\equiv\mathbb{R}\times\mathbb{R}^+$
 - Generico: $X_1,..,X_n$ $n$ VC IID, replicazioni della stessa variabile $X\sim\mathscr{L}(\theta)$, $\theta\in\Theta$


Nel _paradigma classico_ la probabilità si assegna alle $X_i$, in quanto risultato di un sorteggio casuale
\[P(X_1,...,X_n;\theta)\]


Ma non è consentito trattare con lo strumento della probabilità l'incertezza sul parametro $\theta$ che governa la popolazione. Perché $\theta$ è incognito ma non è il frutto di una selezione casuale.


Nel _paradigma Bayesiano_ l'incertezza sul parametro viene trattata con gli stessi strumenti dell'incertezza sui dati, dando vita ad un teoria coerente e molto utile per alcune applicazioni particolari.

## Gli stimatori

_Stimare_, in statistica, significa scegliere un punto (stima puntuale) o una regione (stima intervallare) dello spazio dei parametri $\Theta$ alla luce dei dati $x_1,...,x_n$.

:::: {.info data-latex=""}
::: {.definiton}
Uno **stimatore** puntuale (point estimator) è una statistica $h$ che trasforma il campione $X_1,...,X_n$ in un punto dello spazio dei parametri:
\[h:\mathcal{S}\to\Theta\]
:::
::::

Il campione $X_1,...,X_n$ casuale viene trasformato attraverso $h$ in un punto specifico di $\Theta$
\[h(X_1,...,X_n)=\hat\theta\in\Theta\]

Uno **stimatore** è una variabile casuale in quanto funzione di valori casuali.

:::: {.example}
Da una popolazione che ha $E(X_i)=\mu$ incognita, potremmo proporre di _stimare_ $\mu$ con la media dei dati che _otterremo_:
\[h(X_1,...,X_n)=\frac 1 n \sum_{i=1}^n X_i=\bar X=\hat \mu\]
::::

:::: {.example} 
Da una popolazione di Poisson che ha $X_i\sim\text{Poiss}(\lambda)$, $\lambda$ incognita, potremmo proporre di _stimare_ $\lambda$ con la mediana dei dati che _otterremo_:
\[h(X_1,...,X_n)=X_{0,5}=\hat\lambda\]
::::

### Stimatori e Stime

::: {.nota data-latex=""}
- Uno **Stimatore**: $h(X_1,...,X_n)$ è funzione di $X_1,...,X_n$ è una **VC**
- Una **Stima**: $h(x_1,...,x_n)$ è funzione di $x_1,...,x_n$ e dunque è un **numero**
:::

:::: {.example}
Da una popolazione che ha $E(X_i)=\mu$ incognita, potremmo proporre di _stimare_ $\mu$ con la media dei dati che _otterremo_:
\[h(X_1,...,X_n)=\frac 1 n \sum_{i=1}^n X_i=\hat \mu\]

Estraiamo $n=5$ individui dalla popolazione $x_1=2.1$, $x_2=2.4$, $x_3=3.2$, $x_4=1.7$, $x_5=3.0$,  
```{r 11-Stima-7}
xs <- c(2.1,2.4,3.2,7.1,3.0)
```

Per ottenere $\hat\mu$ la stima di $\mu$ applichiamo $h$ ai dati e otteniamo
\[\hat\mu = \frac 1 5\sum_{i=1}^5x_i=\frac 1 5 `r sum(xs)`=`r mean(xs)`\]
::::

:::: {.example} 
Da una popolazione di Poisson che ha $X_i\sim\text{Poiss}(\lambda)$, $\lambda$ incognita, potremmo proporre di _stimare_ $\lambda$ con la mediana dei dati che _otterremo_:
\[h(X_1,...,X_n)=X_{0,5}=\hat\lambda\]


Osserviamo $n=5$ valori (già riordinati)<br/> 
$x_{(1)}=0$, $x_{(2)}=0$, $x_{(3)}=2$, $x_{(4)}=2$, $x_{(5)}=3$, $x_{(6)}=4$, $x_{(7)}=7$,    


Per ottenere $\hat\lambda$ la stima di $\lambda$ applichiamo $h$ ai dati e otteniamo
\[\hat\lambda = x_{0.5}=x_{(4)}=2\]
::::

### Come scegliere uno stimatore


La definizione non offre un criterio per la scelta. 
Gli stimatori vengono costruiti per avere la miglior _precisione possibile_.
La precisione non si può valutare sulla singola _stima_ ma studiando, 
prima di osservare i dati, le proprietà probabilistiche dello _stimatore_.

Le proprietà auspicabili per uno stimatore sono di due tipi

- Esatte (per $n$ finito)
- Asintotiche (per $n$ che diverge)

### Proprietà Auspicabili di uno stimatore (per $n$ finito)


:::: {.info data-latex=""}
::: {.definition name="Correttezza di uno stimatore"}
Siano $X_1,...,X_n$, $n$ VC, IID, replicazioni della stessa $X\sim\mathscr{L}(\theta)$, sia $h$ uno stimatore per $\theta$.
Lo stimatore $h$ si dice **corretto** se
\[E(h(X_1,...,X_n))=E(h)=\theta\]
:::
::::

:::: {.info data-latex=""}
::: {.definition name="Mean Squared Error di uno stimatore"}
Si definisce **Errore Quadratico Medio** (_Mean Squared Error_) la quantità
\[MSE(h)=E((h-\theta)^2)=V(h)+B^2(h)\]
dove
\[B(h)=|E(h)-\theta|\]
:::
::::

:::: {.nota}
se $h$ è corretto allora
\[MSE(h)=V(h)\]
::::

:::: {.info data-latex=""}
::: {.definition name="Efficienza di uno stimatore"}
Siano $h_1$ e $h_2$ due stimatori per $\theta$, si dice che $h_1$ è **più efficiente** di $h_2$ se e solo se
\[MSE(h_1)<MSE(h_2)\]
:::
::::

:::: {.nota data-latex=""}
Se $h_1$ e $h_2$ sono entrambi corretti, allora, $h_1$ è **più efficiente** di $h_2$ se e solo se
\[V(h_1)<V(h_2)\]

L'**errore** di uno stimatore è l'_inverso_ della sua **precisione**.
::::

### Media aritmetica e varianza campionaria caso IID

Siano $X_1,...,X_n$, $n$ VC, IID, replicazioni della stessa $X$ tale che $E(X)=\mu$ e $V(X)=\sigma^2$, sia $h\equiv\hat \mu$ uno stimatore per $\mu$
\[
\hat \mu=\bar X=\frac 1 n \sum_{i=1}^n X_i
\]


Dai risultati che già conosciamo sappiamo che
\[
E(\hat \mu)=\mu
\]
e dunque $\hat \mu$ è sempre uno stimatore corretto per $\mu$.
Essendo $\hat \mu$ corretto per $\mu$ allora
\[MSE(\hat \mu)=V(\hat \mu)=\frac{\sigma^2}n\]

Si consideri la varianza campionaria:
\[
\hat\sigma^2=\frac 1 n \sum_{i=1}^n(X_i-\hat \mu)^2
\]

Si può dimostrare che

:::: {.info data-latex=""}
\[
E(\hat \sigma^2)=\frac {n-1}n \sigma^2<\sigma^2
\]
::::

### Media aritmetica campionamento SR (popolazioni finite)

Siano $X_1,...,X_n$, $n$ VC, osservazioni estratte SR da una popolazione $X$ di $N$ individui, tale che $E(X)=\mu$ e $V(X)=\sigma^2$, sia $h\equiv\hat \mu$ uno stimatore per $\mu$
\[
\hat \mu=\frac 1 n \sum_{i=1}^n X_i
\]
Allora
\[
E(\hat \mu)=\mu
\]
e dunque $\hat \mu$ è sempre uno stimatore corretto per $\mu$.
Essendo $\hat \mu$ corretto per $\mu$ allora
\[MSE(\hat \mu)=V(\hat \mu)\]

Per calcolare $V(\hat \mu)$ dobbiamo tenere conto della _frazione di campionamento_ $n/N$

:::: {.info data-latex=""}
\[
MSE(\hat \mu)=\frac{N-n}{N-1}\frac{\sigma^2} n
\]
::::

dove $\frac{N-n}{N-1}$ è chiamato _coefficiente di correzione per popolazioni finite_.
Osserviamo che più alto è $n$ più il coefficiente tende ad 1.
Se $n = N$ il coefficiente diventa zero il campione è diventato l'intera popolazione  e 
l'incertezza sulla media è zero.
  
### Esempi

:::: {.example name="Popolazione finita, estrazioni SR (Stima)"}

```{r 11-Stima-3,results="asis"}
xp <- c(0,1,3,7)

camp <- combn(xp,3)

dimnames(camp)[[1]]<- c("$x_1$","$x_2$","$x_2$")
dimnames(camp)[[2]] <- paste("C.",1:ncol(camp))
mu<-mean(xp) 
sig2<-mean(xp^2)-mean(xp)^2
# kable((camp)) %>%
# column_spec (1:ncol(camp),border_left = F, border_right = T) %>%
# kable_styling()


```


Si dispone di un'urna contente 4 bussolotti
\[\{0,1,3,7\}\]

È immediato calcolare
\[\mu=`r mu`, \qquad \sigma^2=`r sig2` \]

Proponiamo due stime diverse di $\mu$
\begin{eqnarray*}
h_1 &=& \frac 1 3 \sum_{i=1}^3 x_i\\
h_2 &=& x_{0.5}
\end{eqnarray*}

Se estraiamo SR, $\#\mathcal{S}=4(\times 6)$, lo spazio dei campioni è

```{r stima-sr,results="asis"}
xp <- c(0,1,3,7)

camp <- combn(xp,3)
h1 <- apply(camp,2,mean)
h2 <- apply(camp,2,median)
camp <- rbind(camp,h1,h2)+.000001
dimnames(camp)[[1]]<- c("$x_{(1)}$","$x_{(2)}$","$x_{(3)}$","$h_1$","$h_2$")
dimnames(camp)[[2]] <- paste("C.",1:ncol(camp))
mu<-mean(xp) 
sig2<-mean(xp^2)-mean(xp)^2
 kable((camp),booktabs=T,escape=F,caption = "Media e mediana dei campioni") %>%
 row_spec(row = 4:5,hline_after = T,bold=F,col=iblue) %>%
 column_spec (1:ncol(camp),border_left = F, border_right = T) %>%
  kable_styling(latex_options = "hold_position",font_size = 8)

```


In tabella \@ref(tab:stima-sr) la $h_1$ e $h_2$ associati ad ogni campione. Avremo che
Otteniamo:
\[E(h_1)=\frac 1{4}(`r camp[4,1]`+`r camp[4,2]`+`r camp[4,3]`+`r camp[4,4]`)=`r mean(camp[4,])`=\mu\]
$h_1$ è **corretto** 
mentre
\[E(h_2)=\frac 1{4}(`r camp[5,1]`+`r camp[5,2]`+`r camp[5,3]`+`r camp[5,4]`)=`r mean(camp[5,])`\neq\mu\]
non lo è.
::::


:::: {.example name="Popolazione finita, estrazioni SR (Efficienza)"}
Ricordiamo che l'**Errore Quadratico Medio** (_Mean Squared Error_) per $\mu$ è
\[MSE(h)=E((h-\mu)^2)=V(h)+B^2(h)\]
dove
\[B(h)=|E(h)-\mu|\]

Osserviamo la precisione di $h_1$:
\[MSE(h_1)=\frac 1{4}\left((`r camp[4,1]`-`r mu`)^2+(`r camp[4,2]`-`r mu`)^2+(`r camp[4,3]`-`r mu`)^2+(`r camp[4,4]`-`r mu`)^2\right)=`r mean(camp[4,]^2)-mu^2`\]

Osserviamo la precisione di $h_2$:
\[MSE(h_2)=\frac 1{4}((`r camp[5,1]`-`r mu`)^2+(`r camp[5,2]`-`r mu`)^2+(`r camp[5,3]`-`r mu`)^2+(`r camp[5,4]`-`r mu`)^2)=`r mean((camp[5,]-mu)^2)`\]

E quindi essendo $MSE(h_1)=`r mean(camp[4,]^2)-mu^2`<`r mean(camp[5,]^2)-mean(camp[5,])^2 + (mean(camp[5,])-mu)^2`=MSE(h_2)$, e dunque $h_1$ è _più efficiente_ di $h_2$.
 
:::: {.nota data-latex=""}
\[V(h_1)=V(\hat \mu)=\frac{N-n}{N-1}\frac{\sigma^2}{n}=\frac{4-3}{4-1}\frac{`r sig2`}{3}\]
::::
::::

:::: {.example name="Popolazione finita, estrazioni SR (Stima della Varianza)"}
Proponiamo la varianza campionaria $\hat\sigma^2$ per stimare $\sigma^2=`r sig2`$
\[\hat\sigma^2=\frac 1 3\sum_{i=1}^3(x_i-\hat \mu)^2\]


```{r var-sr,results="asis"}
xp <- c(0,1,3,7)

camp <- combn(xp,3)
hv <- apply(camp,2,vvv)
camp <- rbind(camp,hv)
dimnames(camp)[[1]]<- c("$x_{(1)}$","$x_{(2)}$","$x_{(3)}$","$\\hat\\sigma^2$")
dimnames(camp)[[2]] <- paste("C.",1:ncol(camp))
mu<-mean(xp) 
sig2<-mean(xp^2)-mean(xp)^2
 kable((camp),booktabs=T,escape=F,caption = "Varianza dei campioni SR di ampiezza $n=3$") %>%
 row_spec(row = 4,hline_after = T,bold=F,col=iblue) %>%
 column_spec (1:ncol(camp),border_left = F, border_right = T)%>%
  kableExtra::kable_styling(font_size = 8,latex_options = "hold_position")


```


In tabella \@ref(tab:var-sr) la varianza associata ad ogni campione. Avremo che
\[E(\hat\sigma^2)=\frac 1 4 (`r hv[1]`+`r hv[2]`+`r hv[3]`+`r hv[4]`)=`r mean(hv)`\]

:::: {.nota data-latex=""}
$E(\hat\sigma^2)=`r mean(hv)`<`r sig2`=\sigma^2$, $\hat\sigma^2$ è distorto e, in media, sottostima la vera $\sigma^2$
::::
::::

:::: {.example name="Popolazione finita, estrazioni CR (Stima)"}
Se estraiamo CR, $\#\mathcal{S}=64$ lo spazio dei campioni è
```{r stima-cr,results="asis"}
xp <- c(0,1,3,7)
  
xxx <- expand.grid(xp,xp,xp)
xxx <- xxx[order(xxx[,1],xxx[,2],xxx[,3]),]
xord <- t(apply(xxx,1,sort))
cxord <- apply(xord,1,function(x)paste(x,collapse = ",") )
pr <- tapply(rep(1,64),cxord,sum)
pr <- pr/sum(pr)
# 

xxm <- apply(xxx, 1, mean)


camp <- unique(xord)

camp <- cbind(camp,pr)
h1 <- apply(camp[,1:3], 1, mean)
h2 <- apply(camp[,1:3], 1, median)

eh1 <- sum(pr*h1)
eh2 <- sum(pr*h2)
mse1 <- sum(pr*(h1-mu)^2)
mse2 <- sum(pr*(h2-mu)^2)

camp <- data.frame(unique(cxord),pr,h1,h2)
names(camp)<- c("Camp.","$P(C.)$","$h_1$","$h_2$")
#camp <- t(camp)
dimnames(camp)[[1]] <- paste("C.",1:nrow(camp))
mu<-mean(xp) 
sig<-mean(xp^2)-mean(xp)^2
kable((camp),booktabs=T,escape=F,caption = "Media e dediana dei campioni e probabilità associate") %>%
#column_spec (1:ncol(camp),border_left = F, border_right = T) %>%
  kableExtra::kable_styling(font_size = 8,latex_options = "hold_position")


```

:::: {.att data-latex=""}
**Attenzione** i campioni qui non hanno tutti la stessa probabilità, in tabella \@ref(tab:stima-cr) i campioni, le loro probabilità e le corrispettive varianze. Otterremo:
\[E(h)=h_\text{Camp. 1}P(\text{Camp. 1})+...+h_\text{Camp. 20}P(\text{Camp. 20})\]

E dunque:
\begin{eqnarray*}
  E(h_1)  &=& `r h1[1]`\times`r pr[1]`+`r h1[2]`\times`r pr[2]`+...+`r h1[20]`\times`r pr[20]` = `r eh1`=\mu\\
  E(h_2)  &=& `r h2[1]`\times`r pr[1]`+`r h2[2]`\times`r pr[2]`+...+`r h2[20]`\times`r pr[20]` = `r eh2`\neq\mu
\end{eqnarray*}
::::

Anche nel CCS CR $h_1$ è corretto per $\mu$ e $h_2$ **non** è corretto per $\mu$
::::

:::: {.example name="Popolazione finita, estrazioni CR (Efficienza)"}
Mentre l'errore
\[MSE(h)=(h_\text{Camp. 1}-\mu)^2P(\text{Camp. 1})+...+(h_\text{Camp. 20}-\mu)^2P(\text{Camp. 20})\]


E dunque
\begin{eqnarray*}
  MSE(h_1) &=& (`r h1[1]`-`r mu`)^2\times`r pr[1]`+(`r h1[2]`-`r mu`)^2\times`r pr[2]`+...+
    (`r h1[20]`-`r mu`)^2\times`r pr[20]`=`r mse1`\\
  MSE(h_2) &=& (`r h2[1]`-`r mu`)^2\times`r pr[1]`+(`r h2[2]`-`r mu`)^2\times`r pr[2]`+...+
    (`r h2[20]`-`r mu`)^2\times`r pr[20]`=`r mse2`
\end{eqnarray*}


Anche in questo caso, l'errore medio quadratico di $h_1$ è minore di quello di $h_2$, e quindi $h_1$ è più efficiente di $h_2$ ovvero più _preciso_.

:::: {.nota data-latex=""}
come definito dalla teoria abbiamo
\[MSE(h_1)=MSE(\hat \mu)=V(\hat \mu)=\frac{\sigma^2}n=\frac{`r sig2`}{3}=`r mse1`\]
::::
::::

:::: {.example name="Popolazione finita, estrazioni CR (Efficienza)"}

Proponiamo la varianza campionaria $\hat\sigma^2$ per stimare $\sigma^2=`r sig2`$
\[\hat\sigma^2=\frac 1 3\sum_{i=1}^3(x_i-\hat \mu)^2\]


```{r 11-Stima-4,results="asis"}
xp <- c(0,1,3,7)
  
xxx <- expand.grid(xp,xp,xp)
xxx <- xxx[order(xxx[,1],xxx[,2],xxx[,3]),]
xord <- t(apply(xxx,1,sort))
cxord <- apply(xord,1,function(x)paste(x,collapse = ",") )
pr <- tapply(rep(1,64),cxord,sum)
pr <- pr/sum(pr)
# 
n <- 3
xxm <- apply(xxx, 1, mean)

camp <- unique(xord)

camp <- cbind(camp,pr)
hv <- apply(camp[,1:3], 1, vvv)

ehv <- sum(pr*hv)

camp <- data.frame(unique(cxord),pr,hv)
names(camp)<- c("Camp.","$P(C.)$","$\\hat\\sigma^2$")
# camp <- t(camp)
dimnames(camp)[[1]] <- paste("C.",1:nrow(camp))
mu<-mean(xp) 
sig<-mean(xp^2)-mean(xp)^2
kable((camp),booktabs=T,escape=F,caption = "Varianza dei campioni estratti CR di ampiezza $n=3$") %>%
#column_spec (1:ncol(camp),border_left = F, border_right = T) %>%
kable_styling(font_size = 8)
   
```

E quindi
\[
  E(\hat\sigma^2)  = `r hv[1]`\times`r pr[1]`+`r hv[2]`\times`r pr[2]`+...+`r hv[20]`\times`r pr[20]` = `r ehv`<
  `r sig2`=\sigma^2
\]


Osserviamo che
\[
E(\hat\sigma^2)=\frac{n-1}n\sigma^2=\frac{`r n-1`}{`r n`}`r sig2`
\]

:::: 

### Distribuzione delle statistiche

```{r 11-Stima-5, echo=FALSE, results='asis'}

#curve(dgamma(x,9,.9),0,25)
set.seed(1)
xpop <- rgamma(10000,9,.9)
mu <- mean(xpop)
s2 <- vvv(xpop)
```


Estraggo campioni di ampiezza $n=5$ da una popolazione di $N=10000$ individui con media $\mu=`r mu`$ e varianza $\sigma^2=`r s2`$.
Osservo tre diversi stimatori per $\mu$: $h_1$, $h_2$ e $h_3$

```{r 11-Stima-6, echo=FALSE, results='asis'}

#curve(dgamma(x,9,.9),0,25)
xpop <- rgamma(10000,9,.9)
n <- 5
mu <- mean(xpop)
s2 <- vvv(xpop)
se <- sqrt(s2/n)

camp <- sapply(1:20, function(x)sample(xpop,n,T))
h1 <- apply(camp, 2, mean)
h2 <- h1+rnorm(20,0,1)
h3 <- h2+rnorm(20,0,2)
```

Se ripetessi l'estrazione un grande numero di volte potre vedere i tre stimatori nel grafico qui di seguito.
Lo stimatore in con la distribuzione in blu è il più efficiente dei tre: la probabilità che si avveri lontano dal vero parametro è minore che per gli altri due.
Mentre lo stimatore in con la distribuzione in verde è il meno efficiente dei tre: la probabilità che si avveri lontano dal vero parametro è ,aggiore che per gli altri due.


```{r 11-Stima-8}
par(mfrow=c(1,1),cex=cex)
curve(dnorm(x,mu,se),mu-4*se,mu+4*se,xlab="x",ylab="",axes=F,col=4)
axis(1,c(mu-4*se,mu,mu+4*se),c(round(mu-4*se,1),expression(mu),round(mu+4*se,2)))
axis(2)
points(h1[1:20],rep(0,times=20),pch=4,col=4)
points(h2[1:20],rep(0,times=20),pch=2,col=ared)
points(h3[1:20],rep(0,times=20),pch=5,col=3)
curve(dnorm(x,mu,se+.5),col=ared,add=T)
curve(dnorm(x,mu,se+1),col=3,add=T)
segments(mu,0,mu,dnorm(mu,mu,se),lty=2)

```


### Proprietà Auspicabili di uno stimatore (per $n\to\infty$)


Siano $X_1,...,X_n$, $n$ VC, IID, replicazioni della stessa $X\sim\mathscr{L}(\theta)$, sia $h$ uno stimatore per $\theta$

:::: {.info data-latex=""}
::: {.definition name="Correttezza Asintotica"}
Lo stimatore $h$ si dice **asintoticamente corretto** se
\[\lim_{n\to\infty}E(h(X_1,...,X_n))=E(h)=\theta\]
:::
::::
   
:::: {.example}
  \[\lim_{n\to\infty}E(\hat\sigma^2)=\lim_{n\to\infty}\frac{n-1}n\sigma^2=\sigma^2\]
::::

:::: {.info data-latex=""}
::: {.definition name="Correttezza Asintotica"}
Lo stimatore $h$ si dice **consistente** (in media quadratica) se e solo se
\[\lim_{n\to\infty}MSE(h(X_1,...,X_n))=\lim_{n\to\infty}MSE(h)=0\]

Essendo
\[MSE(h)=V(h)+B^2(h)\]
allora
\[\lim_{n\to\infty} MSE(h)=0, \text{ se e solo se} \lim_{n\to\infty} V(h)=0 \text{ e } \lim_{n\to\infty} B^2(h)=0\]
:::
::::

:::: {.example name="Consistenza"}
Siano $X_1,...,X_n$, $n$ VC, IID, replicazioni della stessa VC $X$ con $E(X)=\mu$ e $V(X)=\sigma^2$
Usiamo $\hat \mu$ per stimare $\mu$:
\[\hat \mu=\frac 1 n \sum_{i=1}^n X_i=\frac {S_n}n\]

Siccome $\hat \mu$ è stimatore corretto per $\mu$:
\[E(\hat \mu)=E\left(\frac{X_1+...+X_n}{n}\right)=\frac 1 n(E(X_1)+...+E(X_n))=\frac 1 n (\mu+...+\mu)=\mu\]


Allora
\[MSE(\hat \mu)=V(\hat \mu)=\frac {\sigma^2}n\]


Al divergere di $n$
\[\lim_{n\to \infty}MSE(\hat \mu)=\lim_{n\to\infty}\frac{\sigma^2}n=0\]

Lo stimatore $\hat \mu$ per $\mu$ è stimatore **corretto** e **consistente**.
::::

## La $SD$ e lo $SE$

:::: {.info data-latex=""}
La _standard deviation_ (SD) $\sigma$, rappresenta la dispersione degli individui dalla media, è un indicatore di *variabilità* della *popolazione*, per esempio in una popolazione finita di $N$ individui:
\[\sigma=\sqrt{\sigma^2}=\sqrt{\frac 1 N\sum_{i=1}^N(x_i-\mu)^2},\]
la _deviazione standard_ $\sigma$ è la radice della varianza della popolazione $\sigma^2$.

Lo _standard error_ $SE(h)$ di uno stimatore $h$ per $\theta$ è un indicatore della *variabilità* dello stimatore nello *spazio dei parametri*
\[SE(h)=\sqrt{V(h)}\]
Lo _standard error_ $SE(h)$ di uno stimatore $h$ per $\theta$ è la radice della varianza della VC $h$.

La _standard deviation stimata_ $\sigma$, rappresenta la dispersione degli individui _del campione_ dalla media _del campione_, è un indicatore di *variabilità* del *campione*:
\[\hat\sigma=\sqrt{\hat\sigma^2}=\sqrt{\frac 1 n\sum_{i=1}^n(x_i-\hat\mu)^2}\]
La _deviazione standard stimata_ $\hat\sigma$ è la radice della varianza del campione $\hat\sigma^2$.
::::

:::: {.example}
Lo standard error dello stimatore media aritmetica campionaria $\hat\mu$ per $\mu$
\[SE(\hat \mu)=\sqrt\frac{\sigma^2} n=\frac\sigma {\sqrt{n}}\]

L'errore che si commette nello stimare una media dipende da due fattori

  - la _standard deviation_ $\sigma$ che indica la variabilità degli individui tra di loro
  - $1/\sqrt n$ che è l'inverso dell'ampiezza del campione

Se $\sigma$ è incognito viene stimato da (come vedremo nel paragrafo (\@ref(vnorm)))
\[S=\sqrt{S^2}=\sqrt{\frac{n}{n-1}\hat\sigma^2}\]

Ottenendo
\[\widehat{SE(\hat\mu)}=\frac S {\sqrt n}\]
::::

<!--chapter:end:11-Stima.Rmd-->

---
editor_options: 
  chunk_output_type: console
---
```{r setup12, include=FALSE}
rm(list=ls())


source("intro.R")

```


# Teoria della Verosimiglianza



## Il Modello Statistico

Un modello statistico è l'insieme di un modello probabilistico $X_i\sim\mathscr{L}(\theta)$
e di un piano di campionamento dalla popolazione $\mathscr{P}$. 
In queste pagine considereremo come unico piano di campionamento il campionamento casuale semplice **con** reintroduzione, ovvero assumeremo sempre le ipotesi IID. 
Per esempio sono un modello statistico:

  - Le $X_1,...,X_n$ sono IID, replicazioni di $X\sim\text{Ber}(\pi)$, $\pi\in[0,1]$ .
  - Le $X_1,...,X_n$ sono IID, replicazioni di $X\sim\text{Pois}(\lambda)$, $\lambda\in\mathbb{R}^+$.
  - Le $X_1,...,X_n$ sono IID, replicazioni di $X\sim N(\mu,\sigma^2)$, $(\mu,\sigma^2)\in\mathbb{R}\times\mathbb{R}^+$.
  - Le $X_1,...,X_n$ sono IID, replicazioni di $X\sim \mathscr{L}(\theta)$, $\theta\in\Theta$-

### Esiste lo stimatore più efficiente?

Dipende dalle informazioni che abbiamo sulla Popolazione $\mathscr{P}$
In contesti _distribution free_ e ipotesi IID, senza alcuna ulteriore conoscenza della popolazione, non esistono procedure che possano essere dimostrate _ottimali_, il ricercatore valuta da caso a caso, previa un'attenta analisi descrittiva preliminare.
In ipotesi _distribution free_ l'intera distribuzione della variabile $X$ è incognita.
Se spostiamo l'attenzione all'_inferenza da modello_ ipotizziamo di conoscere la forma della distribuzione di probabilità delle $X$ a meno dei suoi parametri.

Sotto alcune condizioni di regolarità gli stimatori più efficienti sono gli stimatori di Massima Verosimiglianza.
Lo stimatore di massima verosimiglianza parte dell'assunto che tutta l'informazione che un campione 
porge nella comprensione della popolazione risieda in una misura chiamata _Verosimiglianza_.

## La Verosimiglianza

La _Verosimiglianza_ è una misura di incertezza __non__ sul risultato di un 
esperimento casuale, ma sui meccanismi che generano una sequenza casuale.
Nella teoria della verosimiglianza si può parlare di probabilità solo per il campione,
ma non per i meccanismi che lo hanno generato. Una volta osservati i dati la conoscenza
di questi meccanismi diventa più o meno _verosimile_ agli occhi del ricercatore alla
luce dell'osservazione.

Nella teoria della verosimiglianza, dunque, si usano due termini diversi: _probabilità_ 
per indicare la misura dell'incertezza sui risultati dell'estrazione del campione e 
_verosimiglianza_ per indicare la misura di incertezza sui meccanismi che hanno prodotto il
campione.

La _teoria della verosimiglianza_ presuppone la totale ignoranza del ricercatore 
che esplora un sistema casuale di cui sta cercando di comprendere i parametri. 

Se per esempio voglio conoscere la probabilità $\pi$ di una moneta truccata di 
porgere Testa, la teoria della verosimiglianza presuppone che per me, 
prima di osservare il campione, tutti i possibili valori di $\pi$ siano 
equamente verosimili, compresi quelli più estremi.

Questa totale ignoranza non è sempre giustificata e per allargare la teoria della
verosimiglianza rimando il lettore su testi di statistica Bayesiana che sfruttano 
il teorema di Bayes per costruire una misura alternativa (più ampia) della 
verosimiglianza, basata solo sul concetto allargato di probabilità.

> Donovan, T. M., and Mickey, R. M. (2019). Bayesian Statistics for Beginners: A Step-by-Step Approach. Oxford: Oxford University Press.

La _funzione di verosimiglianza_ è la una funzione di probabilità dei dati, 
fissata sul campione osservato, in cui la variabile è il parametro. La verosimiglianza è
indicata con la lettera $L$ (_Likelihood_) e si scrive

::: {.info data-latex=""}
\[
L(\theta;\text{Dati})\propto P(\text{Dati};\theta)
\]
:::

e si legge che la _verosimiglianza_  $L$ di $\theta$ è proporzionale $\propto$ alla
probabilità di osservare i dati osservati nell'ipotesi che $\theta$ sia vera.
Il simbolo proporzionale $\propto$ significa che:
\[
L(\theta;\text{Dati})=Const.\cdot P(\text{Dati};\theta)
\]
dove $Const.$ è una constante qualunque che non dipende da $\theta$.
Il valore di $L(\theta;\text{Dati})$ per un $\theta$ fissato, non ha alcune significato 
se non è confrontato con altri valori. Infatti $L(\theta;\text{Dati})$ **non** è una
probabilità, ma se
\[
L(\theta_1;\text{Dati})> L(\theta_2;\text{Dati})
\]
Significa che l'ipotesi che sia stato $\theta=\theta_1$ il valore del parametro del 
modello che ha generato i dati è più verosimile dell'ipotesi che sia 
stato $\theta=\theta_2$.

### La Verosimiglianza attraverso un esempio

Supponiamo di avere un'urna che ha solo $N=10$ bussolotti alcuni bianchi $B$ e i rimanenti non bianchi $\overline{B}=N-B$, ma non conosciamo $B$.
Il numero di bianchi $B$ potrà essere $0, 1,...,10$.
La VC $X$ che registra l'evento bianco o nero di una estrazione è chiaramente Bernoulli $X\sim\text{Ber}(\pi)$ di parametro:
\[\pi=\frac B {10}\]
$\pi$ è la proporzione di bussolotti bianchi nell'urna.
In questo specifico esempio:
\[\pi\in\left\{\frac 0{10}=0.0,\frac{1}{10}=0.1,...,\frac 9 {10}=0.9,\frac{10}{10}=1.0\right\}\]
Lo spazio dei parametri ha dimensione 11.
Uno stimatore ha il compito di scegliere uno di questi 11 valori.

Estraiamo $n=5$ bussolotti CR (IID) e otteniamo
\[x_1=0,x_2=1,x_3=1,x_4=0,x_5=1\]

```{r 12-Verosimiglianza-2}
xs <- c(0,1,1,0,1)
```

Se conoscessi $\pi$ attraverso il calcolo delle probabilità saprei calcolare la 
probabilità della sequenza (ordinata) 0,1,1,0,1 proveniente da 5 esperimenti di 
Bernoulli IID

\begin{multline*}
P(X_1=0\cap X_2=1 \cap X_3=1 \cap X_4=0\cap X_5=1;\pi) = \\
\begin{array}{ll}
  = &P(X_1=0;\pi)P(X_2=1;\pi)P(X_3=1;\pi)P(X_4=0;\pi)P(X_5=1;\pi)\\
  = &(1-\pi)\pi\pi(1-\pi)\pi\\
  = &\pi^3(1-\pi)^{5-3}
\end{array}
\end{multline*}

La posso calcolare per ogni possibile valore di $\pi\in\{0.0,0.1,...,1.0\}$.

### Se $\pi$ fosse...

A questo punto graduare decidere i valori di $\pi$ tra più e meno _verosimili_
alla luce dei dati $\mathbf{x}=(0,1,1,0,1)$. Questo si fa sostituendo $\pi$ con
i suoi possibili valori e calcolando la _ipotetica_ probabilità.


```{r 12-Verosimiglianza-1,results='asis'}
pp <- (0:10)/10
for (i in 1:11){
  cat("Se fosse $\\pi=$",pp[i]," con quale probabilità avrei osservato la sequenza $0,1,1,0,1$? \n")
  cat("\\[ \n")
  cat(pp[i],"^3\\cdot(1-",pp[i],")^2=",pp[i]^3*(1-pp[i])^2," \\qquad\\text{l'ipotesi $\\pi=",pp[i],"$ ha verosimiglianza proporzionale a",
      pp[i]^3*(1-pp[i])^2,"}\n")
  cat("\\]\n")
}
```

Definiamo la **funzione di verosimiglianza** (Likelihood), la funzione $L$ del parametro incognito $\pi$ alla luce dei dati $X_1=0,X_2=1,X_3=1,X_4=1,X_5=1$ osservati:
\begin{multline*}
  L(\pi;X_1=0,X_2=1,X_3=1,X_4=1,X_5=1) =  \\
  \begin{array}{ll}
     = &L(\pi)\\
     = &K\cdot P(X_1=0\cap X_2=1\cap X_3=1\cap X_4=1\cap X_5=1;\pi) \qquad \text{con $K> 0$}\\
     \propto & P(X_1=0\cap X_2=1\cap X_3=1\cap X_4=1\cap X_5=1;\pi)\\
     \propto & \pi^3(1-\pi)^2
  \end{array}
\end{multline*}
La verosimiglianza gradua quanto un certo valore di $\pi$ è compatibile con i dati
osservati. Per esempio l'ipotesi $\pi=0.5$ è più verosimile dell'ipotesi $\pi=0.4$, alla luce dei dati
$x_1=0,x_2=1,x_3=1,x_4=0,x_5=1$,
\[
L(0.5)=`r .5^3*(.5)^2`>L(0.4)=`r .4^3*.6^2`
\]
Se mettiamo $\pi$ in ascissa e $L(\pi)$ in ordinata, otteniamo il grafico della verosimiglianza di $\pi$, alla luce dei dati osservati.

```{r 12-Verosimiglianza-3}
xs <- c(0,1,1,0,1)
n <- length(xs)
sn <- sum(xs)
pp <- (0:10)/10
Lp <- data.frame(pp^3*(1-pp)^2)
rnm <-paste("$\\pi=",pp,"$", sep = "")
rownames(Lp) <- rnm
names(Lp) <- "$L(\\pi)$"
fig.def()
```

```{r 12-Verosimiglianza-4}
plot(pp,Lp[,1],axes=F,xlab=expression(pi),ylab=expression(L(pi)))
segments(pp,0,pp,Lp[,1] ,lty=2)
axis(1,pp)
axis(2,las=2)
fig.def(4)
```

### La verosimiglianza non è una probabilità

Notiamo che
\[\sum_{\pi\in\{0.0,0.1,...,1.0\}}L(\pi) = `r Lp[1,1]`+`r Lp[2,1]`+...+`r Lp[11,1]`=`r sum(Lp[,1])`\neq 1\]

La possiamo moltiplicare per un numero qualunque

```{r 12-Verosimiglianza-5}
par(mfrow=c(2,2),cex=cex)
plot(pp,Lp[,1],axes=F,xlab=expression(pi),ylab=expression(L(pi)), main = expression(P(x[1],...,x[n],pi)),type="h",lwd=2,col=ared)
axis(1,pp)
axis(2,las=2)

plot(pp,10*Lp[,1],axes=F,xlab=expression(pi),ylab=expression(L(pi)), main = expression(10 %*%P(x[1],...,x[n],pi)),type="h",lwd=2,col=ared)
axis(1,pp)
axis(2,las=2)

plot(pp,(1/2)*Lp[,1],axes=F,xlab=expression(pi),ylab=expression(L(pi)), main = expression(1/ 2 %*%P(x[1],...,x[n],pi)),type="h",lwd=2,col=ared)
axis(1,pp)
axis(2,las=2)

plot(pp,(1/0.035)*Lp[,1],axes=F,xlab=expression(pi),ylab=expression(L(pi)), main = expression(1/ .035 %*%P(x[1],...,x[n],pi)),type="h",lwd=2,col=ared)
axis(1,pp)
axis(2,las=2)

par(mfrow=c(1,1),cex=cex)
```

### La stima di massima verosimiglianza

$\hat\pi_{ML}=\hat\pi$ è 
\[\hat\pi\in\{0.0,0.1,0.2,...,1.0\}: L(\hat\pi)>L(\pi),\forall\pi\neq\hat\pi\]

E quindi:
\[\hat\pi=0.6=\frac 3 5\]

Consideriamo $\ell$, il logaritmo di $L$
\[\ell(\pi)=\log L(\pi)\]


```{r 12-Verosimiglianza-6}
xs <- c(0,1,1,0,1)
n <- length(xs)
sn <- sum(xs)
pp <- (0:10)/10
rnm <-paste("$\\pi=",pp,"$", sep = "")
Lp <- data.frame(pp,pp^3*(1-pp)^2,log(pp^3*(1-pp)^2))
rownames(Lp) <- rnm
names(Lp) <- c("$\\pi=$","$L(\\pi)$","$\\ell(\\pi)$")
kable(t(Lp),booktabs=T, escape = F,linesep = "", digits = 4,col.names = NULL)

fig.def()
```

```{r 12-Verosimiglianza-7}
par(mfrow=c(1,2),cex=cex)
plot(pp,Lp[,2],axes=F,xlab=expression(pi),ylab=expression(L(pi)),main = "L",pch=16,type='b')
segments(sn/n,0,sn/n,(sn/n)^sn*(1-sn/n)^(n-sn),lty=2)
axis(1,pp)
axis(2,las=2)
Lp[is.infinite(Lp[,2]),2]<- - 100
plot(pp,Lp[,3],axes=F,xlab=expression(pi),ylab=expression(L(pi)),main = "log L",pch=16,type='b',ylim=c(-8,-3))
segments(sn/n,-9,sn/n,log((sn/n)^sn*(1-sn/n)^(n-sn)),lty=2)
axis(1,pp)
axis(2,las=2)
```


### Esempio IID da popolazione finita (parte due)

Riprendiamo l'esempio di prima: un'urna che ha solo $N=10$ bussolotti alcuni bianchi $B$ altri neri $N$, ma non conosciamo $B$ ed $N$.
Il numero di bianchi $B$ potrà essere $0, 1,...,10$ e 
$\pi$ è la proporzione di bussolotti bianchi nell'urna
\[\pi=\frac B{10}\]

Estraiamo $n=5$ bussolotti CR (IID) e otteniamo 
3 _successi_ (bussolotto bianco) e 2 _insuccessi_ (bussolotto nero).
Non conosciamo l'ordine.
  
$X\sim\text{Binom}(5,\pi)$, il mio campione è un'estrazione dalla binomiale con $n=5$.
Speculiamo su $\pi$

```{r 12-Verosimiglianza-8}
xs <- c(0,1,1,0,1)
```

Se conoscessi $\pi$ attraverso il calcolo delle probabilità saprei calcolare la probabilità $P(X)=3$, con $X\sim\text{Binom}(5,\pi)$
\[
P(X=3;\pi) =   \binom{5}{3}\pi^3(1-\pi)^{5-3} =`r choose(5,3)`\cdot \pi^3(1-\pi)^2
\]

Se fosse $\pi=0$ ($B=0$) con quale probabilità avrei osservato la sequenza $X=3$?
\[\binom{5}{3}0^3\cdot(1-0)^2=0\]
  - l'ipotesi $\pi=0$ ha **verosimiglianza** proporzionale a zero

Se fosse $\pi=0.1$ ($B=1$) con quale probabilità avrei osservato la sequenza $X=3$?
\[\binom{5}{3}0.1^3\cdot(1-0.1)^2=`r dbinom(3,5,.1) `\]
  - l'ipotesi $\pi=0.1$ ha **verosimiglianza** proporzionale a `r dbinom(3,5,.1)`

...

Definiamo la funzione di verosimiglianza (Likelihood), la funzione $L$ del parametro incognito $\pi$ alla luce dei dati $x=3$ osservati:
\begin{eqnarray*}
  L(\pi;x=3) &=& L(\pi) \\
            &\propto& P(x=3;\pi)\\
            &=& \binom{5}{3}\pi^3(1-\pi)^2\\
            &\propto& \pi^3(1-\pi)^2
\end{eqnarray*}

La tabella

```{r two-column, results='asis', echo=FALSE, out.extra=''}
xs <- c(0,1,1,0,1)
n <- length(xs)
sn <- sum(xs)
pp <- (0:10)/10
Lp <- data.frame(pp,pp^3*(1-pp)^2,log(pp^3*(1-pp)^2))
rnm <-0:10
rownames(Lp) <- rnm
cn <- rep("",times=11)
names(Lp) <- c("$\\pi=$","$L(\\pi)$","$\\log(L(\\pi))$")

kable(t(Lp),col.names = NULL ,booktabs=T, escape = F,linesep = "", digits = 4)
```

e il grafico


```{r 12-Verosimiglianza-9}
par(mfrow=c(1,2),cex=cex)
plot(pp,Lp[,2],axes=F,xlab=expression(pi),ylab=expression(L(pi)),main = "L",pch=16,type='b')
segments(sn/n,0,sn/n,(sn/n)^sn*(1-sn/n)^(n-sn),lty=2)
axis(1,pp)
axis(2,las=2)
Lp[is.infinite(Lp[,2]),2]<- - 100
plot(pp,Lp[,3],axes=F,xlab=expression(pi),ylab=expression(L(pi)),main = "log L",pch=16,type='b',ylim=c(-8,-3))
segments(sn/n,-9,sn/n,log((sn/n)^sn*(1-sn/n)^(n-sn)),lty=2)
axis(1,pp)
axis(2,las=2)

fig.def(6)
```


### Abbiamo trovato il vero $\pi$?

Ovviamente $\hat\pi$ non è $\pi$ che non conosceremo mai, $\hat\pi=0.6$ è il valore _più verosimile_ tra tutti i possibili valori di $\pi$, ma non è $\pi$.
Ci possiamo chiedere se, per esempio, l'ipotesi $\pi=0.5$ è _"impossibile"_.
Anche in questo caso la risposta è negativa, $\pi=0.5$ è solo, alla luce dei dati, _meno verosimile_ dell'ipotesi $\pi=0.6$. E possiamo anche calcolare di di quanto:
\[\frac{L(\hat\pi=0.6)}{L(\pi=0.5)}=\frac{`r dbinom(3,5,.6)`}{`r dbinom(3,5,.5)`}=`r dbinom(3,5,.6)/dbinom(3,5,.5)`\]

Alla luce dei dati (3 successi su 5 estrazioni) il valore $\hat\pi=0.6$ è il $`r (dbinom(3,5,.6)/dbinom(3,5,.5)-1)*100`\%$ _più verosimile_ di $\pi=0.5$.
\[
  (`r dbinom(3,5,.6)/dbinom(3,5,.5)`-1)\times 100 \%= `r (dbinom(3,5,.6)/dbinom(3,5,.5)-1)*100`\%
\]


### Muoviamo anche $S_n$


```{r , results='asis', echo=FALSE, out.extra=''}
if (!html)  par(mfrow=c(3,2),cex=cex)

  n <- 5
  for (sn in 0:5){
  pp <- (0:10)/10
  Lp <- data.frame(pp^sn*(1-pp)^(n-sn))
  rnm <-paste("$\\pi=",pp,"$", sep = "")
  rownames(Lp) <- rnm
  names(Lp) <- "$L(\\pi)$   "
  plot(pp,Lp[,1],axes=F,xlab=expression(pi),ylab=expression(L(pi)))
  segments(sn/n,0,sn/n,(sn/n)^sn*(1-sn/n)^(n-sn),lty=2)
  axis(1,pp)
  axis(2,las=2)
  
  title(bquote(s[5]==.(sn)))
 
}
```

In sintesi, lo spazio $X\times\Theta$ è l'incrocio tra tutti i possibili $\pi$ e tutti i possibili $s_n$, ne esce una matrice con 10 righe e 10 colonne dove le righe rappresentano $s_n$ e le colonne $\pi$.

```{r 12-Verosimiglianza-10}
pig <- (0:10)/10
sn  <- (0:5)
LX  <- outer(pig,sn,function(p,x)dbinom(x,5,p))
dimnames(LX)[[1]] <- paste("$\\pi=",pig,"$",sep="")
dimnames(LX)[[2]] <- paste("$s_{5}=",sn,"$",sep="")

kable(LX,row.names = T,booktabs = T, escape = F,linesep = "", digits = 4)
```

questa tabella, letta per righe ci indica la probabilità, letta per colonne ci indica la _verosimiglianza_.

## La Funzione di Verosimiglianza 

:::: {.info data-latex=""}
::: {.definition name="Funzione di Verosimiglianza"}
Siano $x_1,...,x_n$ $n$ osservazioni di $X\sim \mathscr{L}(\theta)$, $\theta\in\Theta$, si definisce la verosimiglianza $L$ di $\theta$ la funzione:
\[L(\theta;x_1,...,x_n)=L(\theta)\propto P(X_1=x_1,...,X_n=x_n;\theta)\]
:::
::::

La funzione di verosimiglianza è una funzione in $\theta$ (la variabile) per 
$x_1,...,x_n$ fissi. Indica quanto un particolare valore di $\theta$ è supportato dai dati.
Più alta è la verosimiglianza più i valori di $\theta$ che la rendono alta sono supportati dall'evidenza campionaria.
Se $x_1,..,x_n$ sono osservazioni $IID$ otteniamo
\begin{eqnarray*}
L(\theta) &\propto& P(X_1=x_1;\theta)\cdot...\cdot P(X_n=x_n;\theta) \\
          &\propto& f(x_1;\theta)\cdot...\cdot f(x_n;\theta)\\
          &\propto& \prod_{i=1}^n f(x_i;\theta)
\end{eqnarray*}

:::: {.info data-latex=""}
::: {.definition name="Log Verosimiglianza"}
Si definisce la log-verosimiglianza $\ell$:
\begin{eqnarray*}
\ell(\theta) &=& \log L(\theta) \\
             &=& \log \prod_{i=1}^n f(x_i;\theta)\\
             &=& \sum_{i=1}^n \log f(x_i;\theta)
\end{eqnarray*}
:::
::::

## La Stimatore di massima Verosimiglianza 


:::: {.info data-latex=""}
::: {.definition name="Stimatore du Massima Verosimiglianza"}
Lo stimatore di *massima verosimiglianza* per $\theta$ è
\begin{eqnarray*}
\hat\theta &=& \operatorname*{\text{argmax}}_{\theta\in\Theta} L(\theta)\\
           &=& \operatorname*{\text{argmax}}_{\theta\in\Theta} \ell(\theta)
\end{eqnarray*}

\[\hat\theta:L(\hat\theta)>L(\theta), \forall\theta\neq\hat\theta, \qquad\ell(\hat\theta)>\ell(\theta), \forall\theta\neq\hat\theta\]
:::
::::

## Il Principio di Verosimiglianza

Secondo la teoria della verosimiglianza, dato un modello statistico 
tutta l'informazione che un campione 
$\mathbf{x}=(x_1,...,x_n)$ porge a $\theta$ è contenuta nella sua funzione di
verosimiglianza.

## Verosimiglianza e Statistiche Sufficienti

da scrivere

## Caso Bernoulli urna infinita.

Se l'urna è infinita $N\to\infty$, allora $\pi\in[0,1]$. 
Le variabili $X_1,...,X_n$ tutte replicazioni IID di $X\sim \text{Ber}(\pi)$, si realizzano in $x_1,...,x_n$.

__Esempio.__ $n=5$, $x_1=0,x_2=1,x_3=1,x_4=0,x_5=1$, 
La probabilità della singola estrazione è
\[P(X_i=x_i;\pi)=f(x_i;\pi)=\pi^{x_i}(1-\pi)^{1-x_i}\]

La verosimiglianza è
\begin{eqnarray*}
L(\pi)     &\propto& \prod_{i=1}^n f(x_i;\pi)\\
           &=& \prod_{i=1}^n \pi^{x_i}(1-\pi)^{1-x_i}\\
           &=& \pi^{x_1}(1-\pi)^{1-x_1} \pi^{x_2}(1-\pi)^{1-x_2} ... \pi^{x_n}(1-\pi)^{1-x_n}\\
           &=& \pi^{x_1}\pi^{x_2}...\pi^{x_n}\quad (1-\pi)^{1-x_1}(1-\pi)^{1-x_2}...(1-\pi)^{1-x_n}\\
           &=& \pi^{x_1+x_2+...+x_n}(1-\pi)^{1-x_1+1-x_2+...+1-x_n}\\
           &=& \pi^{\sum_{i=1}^n x_i}(1-\pi)^{n-\sum_{i=1}^n x_i}\\
           &=& \pi^{s_n}(1-\pi)^{n-s_n}, \qquad s_n=\sum_{i=1}^n x_i
\end{eqnarray*}

La statistica $s_n$ contiene **tutta** l'informazione del campione $x_1,...,x_n$.
La log-verosimiglianza è
\begin{eqnarray*}
\ell(\pi)  &=& \log L(\pi)\\
           &=& \log \pi^{s_n}(1-\pi)^{n-s_n}\\
           &=& \log \pi^{s_n} + \log (1-\pi)^{n-s_n}\\
           &=& s_n \log \pi + (n-s_n) \log (1-\pi)
\end{eqnarray*}

Per derivare il $\pi$ che rende massima la verosimiglianza si deve derivare la funzione $\ell$ ed uguagliare a zero la derivata prima:
\[\ell'(\pi)= \frac{s_n}{\pi}+(-1)\frac{n-s_n}{1-\pi}=\frac{s_n}{\pi}-\frac{n-s_n}{1-\pi}\]

$\hat\pi$ è dunque quel valore tale che
\[\ell'(\hat\pi)=0\]

Eguagliamo a zero la derivata prima della log verosimiglianza:
\begin{eqnarray*}
\ell'(\pi)  &=& 0 \\
\frac{s_n}{\pi}-\frac{n-s_n}{1-\pi}          &=& 0 \\
\frac{s_n(1-\pi)-(n-s_n)\pi}{\pi(1-\pi)}     &=& 0\qquad \text{il denominatore è ininfluente} \\
s_n - s_n \pi - n \pi + s_n \pi              &=& 0 \\
s_n- n \pi                                   &=& 0\\
n\pi                                         &=& s_n \\
\hat\pi                                      &=& \frac{s_n}n\\
                                             &=& \frac{\sum_{i=1}^n x_i}n
\end{eqnarray*}

Se $n=5$, $s_5=3$ allora:
\[\hat\pi=\frac{3}{5}=0.6\]

$L(\pi;s_5=3)$, $\ell(\pi;s_5=3)$.

```{r 12-Verosimiglianza-11}
fig.def()
```

```{r 12-Verosimiglianza-12}
sn <- 3; n<-5
par(mfrow=c(1,2),cex=cex)
curve(x^3*(1-x)^2,0,1,axes=F,xlab=expression(pi),ylab=expression(L(pi)))
axis(1,pp)
axis(2,las=2)
segments(sn/n,0,sn/n,(sn/n)^sn*(1-sn/n)^(n-sn),lty=2)

curve(log(x^3*(1-x)^2),0,1,axes=F,xlab=expression(pi),ylab=expression(log~L(pi)))
axis(1,pp)
axis(2,las=2)
segments(sn/n,-15,sn/n,log((sn/n)^sn*(1-sn/n)^(n-sn)),lty=2)
par(cex=cex)

```

### Calcolo delle proprietà di $\hat\pi$

Dunque

:::: {.info data-latex=""}
Siano $X_1,...X_n$ $n$ VC IID, tali che $X_i\sim\text{Ber}(\pi)$ lo stimatore
di massima verosimiglianza per $\pi$ è
\[\hat \pi=\frac 1n \sum_{i=1}^nX_i\]
::::

Il _vero_ valore di $\pi$ è incognito ma sappiamo che:

:::: {.info data-latex=""}
$\hat\pi$ è corretto per $\pi$, infatti
\[E(\hat\pi)=E\left(\frac{1}n\sum_{i=1}^n X_i\right)=\frac{1}n\sum_{i=1}^nE(X_i)=\frac{\pi+...+\pi}{n}=\frac n n\pi=\pi\]

E quindi
\[MSE(\hat\pi)=V(\hat\pi)=\frac{\pi(1-\pi)}{n}\]
che è ancora funzione di $\pi$.
::::

:::: {.info data-latex=""}
Lo stimatore $\hat\pi$ per $\pi$ è _consistente_, infatti
\[\lim_{n\to +\infty}MSE(\hat\pi)=\lim_{n\to +\infty}\frac{\pi(1-\pi)}{n}=0\]

$\hat\pi$ è _corretto_ e _consistente_ per $\pi$.
::::

Osserviamo che:

:::: {.info data-latex=""}
\[SE(\hat\pi)=\sqrt{\frac{\pi(1-\pi)}{n}}\]
::::

È un risultato teorico che dipende dal _vero_ $\pi$, che non conosciamo.

:::: {.info data-latex=""}
L'errore di stima si stima sostituendo a $\pi$ la sua stima $\hat\pi$
\[\widehat{SE(\hat\pi)}=\sqrt{\frac{\hat\pi(1-\hat\pi)}{n}}\]
::::

Se $\hat\pi=0.6$ e $n=5$
\[\widehat{SE(\hat\pi)}=\sqrt{\frac{0.6(1-0.6)}{5}}=`r sqrt(.6*(1-.6)/5)`\]

:::: {.nota data-latex=""}
Lo Standard Error è l'ordine di grandezza dell'errore commesso.
::::

### Se $n$ aumenta e $\hat\pi=0.6$

Se $n=10$ e $s_{10}=6$, allora anche in questo caso
\[\hat\pi=\frac 6{10}=0.6\]

ma
\[\widehat{SE(\hat\pi)}=\sqrt{\frac{0.6(1-0.6)}{10}}=`r sqrt(.6*(1-.6)/10)`.\]

Se $n=20$ e $s_{10}=12$, allora anche in questo caso
\[\hat\pi=\frac {12}{20}=0.6\]

e
\[\widehat{SE(\hat\pi)}=\sqrt{\frac{0.6(1-0.6)}{20}}=`r sqrt(.6*(1-.6)/20)`.\]

Se $n=100$ e $s_{100}=60$, allora anche in questo caso
\[\hat\pi=\frac {60}{100}=0.6\]

e
\[\widehat{SE(\hat\pi)}=\sqrt{\frac{0.6(1-0.6)}{100}}=`r sqrt(.6*(1-.6)/100)`.\]

Se $n=1000$ e $s_{10}=600$, allora anche in questo caso
\[\hat\pi=\frac {600}{1~000}=0.6\]

e
\[\widehat{SE(\hat\pi)}=\sqrt{\frac{0.6(1-0.6)}{1000}}=`r sqrt(.6*(1-.6)/1000)`.\]


Osserviamo nel grafico $L(\pi;s_n=0.6\cdot n)$ e $\ell(\pi;s_n=0.6\cdot n)$ per $n=5,10,20,100,1~000$

```{r 12-Verosimiglianza-13}
sn<-3; n <- 5
L <- function(x) x^(sn)*(1-x)^(n-sn)
l <- function(x) log(L(x))
par(mfrow=c(1,2),cex=cex)
curve(L(x)/L(sn/n),0,1,axes=F,xlab=expression(pi),ylab=expression(L(pi)))

sn <-6; n <- 10
curve(L(x)/L(sn/n),add=T,col=ared)

sn <-12; n <- 20
curve(L(x)/L(sn/n),add=T,col=3)

sn <-60; n <- 100
curve(L(x)/L(sn/n),add=T,n=1001,col=4)

axis(1,pp)
axis(2,las=2)

sn <-600; n <- 1000
curve(L(x)/L(sn/n),add=T,n=1001,col=5)

axis(1,pp)
axis(2,las=2)
segments(sn/n,0,sn/n,1,lty=2)

sn<-3; n <- 5

curve(l(x)-l(.6),0,1,axes=F,xlab=expression(pi),ylab=expression(log~L(pi)))

sn <-6; n <- 10
curve(l(x)-l(.6),add=T,col=ared)

sn <-12; n <- 20
curve(l(x)-l(.6),add=T,col=3)

sn <-60; n <- 100
curve(l(x)-l(.6),add=T,col=4)

axis(1,pp)
axis(2,las=2)
segments(sn/n,-10,sn/n,0,lty=2)

sn <-600; n <- 1000
curve(l(x)-l(.6),add=T,col=5)

axis(1,pp)
axis(2,las=2)
segments(sn/n,-10,sn/n,0,lty=2)

par(cex=cex)

```

### L'ipotesi $\pi=0.5$

```{r 12-Verosimiglianza-14}
n  <- 5
sn <- .6*n
```

Se $n=`r n`$, $s_n=`r sn`$ il valore $\hat\pi=0.6$ è il $`r (dbinom(sn,n,.6)/dbinom(sn,n,.5)-1)*100`\%$ _più verosimile_ di $\pi=0.5$.
\[\frac{L(0.6;s_5=3)}{L(0.5;s_5=3)}=\frac{0.6^{`r sn`}(1-0.6)^{`r n-sn`}}{0.5^{`r sn`}(1-0.5)^{`r n-sn`}}=`r dbinom(sn,n,.6)/dbinom(sn,n,.5)`\]

```{r 12-Verosimiglianza-15}
n  <- 10
sn <- .6*n
```

Se $n=`r n`$, $s_n=`r sn`$ il valore $\hat\pi=0.6$ è il $`r (dbinom(sn,n,.6)/dbinom(sn,n,.5)-1)*100`\%$ _più verosimile_ di $\pi=0.5$.
\[\frac{L(0.6;s_5=3)}{L(0.5;s_5=3)}=\frac{0.6^{`r sn`}(1-0.6)^{`r n-sn`}}{0.5^{`r sn`}(1-0.5)^{`r n-sn`}}=`r dbinom(sn,n,.6)/dbinom(sn,n,.5)`\]

```{r 12-Verosimiglianza-16}
n  <- 20
sn <- .6*n
```

Se $n=`r n`$, $s_n=`r sn`$ il valore $\hat\pi=0.6$ è il $`r (dbinom(sn,n,.6)/dbinom(sn,n,.5)-1)*100`\%$ _più verosimile_ di $\pi=0.5$.
\[\frac{L(0.6;s_5=3)}{L(0.5;s_5=3)}=\frac{0.6^{`r sn`}(1-0.6)^{`r n-sn`}}{0.5^{`r sn`}(1-0.5)^{`r n-sn`}}=`r dbinom(sn,n,.6)/dbinom(sn,n,.5)`\]

```{r 12-Verosimiglianza-17}
n  <- 100
sn <- .6*n
```

Se $n=`r n`$, $s_n=`r sn`$ il valore $\hat\pi=0.6$ è il $`r (dbinom(sn,n,.6)/dbinom(sn,n,.5)-1)*100`\%$ _più verosimile_ di $\pi=0.5$.
\[\frac{L(0.6;s_5=3)}{L(0.5;s_5=3)}=\frac{0.6^{`r sn`}(1-0.6)^{`r n-sn`}}{0.5^{`r sn`}(1-0.5)^{`r n-sn`}}=`r dbinom(sn,n,.6)/dbinom(sn,n,.5)`\]

```{r 12-Verosimiglianza-18}
n  <- 1000
sn <- .6*n
```

Se $n=`r n`$, $s_n=`r sn`$ il valore $\hat\pi=0.6$ è il $`r (dbinom(sn,n,.6)/dbinom(sn,n,.5)-1)*100`\%$ _più verosimile_ di $\pi=0.5$.
\[\frac{L(0.6;s_5=3)}{L(0.5;s_5=3)}=\frac{0.6^{`r sn`}(1-0.6)^{`r n-sn`}}{0.5^{`r sn`}(1-0.5)^{`r n-sn`}}=`r dbinom(sn,n,.6)/dbinom(sn,n,.5)`\]

## Il modello Poisson

Siano $X_1,...,X_n$ $n$ VC IID, replicazioni della stessa $X\sim\text{Pois}(\lambda)$, e dunque con funzione di probabilità:
\[f(x_i;\lambda)=\frac{\lambda^{x_i}}{x_i!}e^{-\lambda}\]

La verosimiglianza per $\lambda$ è
\begin{eqnarray*}
  L(\lambda) &=& \prod_{i=1}^n\frac{\lambda^{x_i}}{x_i!}e^{-\lambda}\\
             &=& \frac{\lambda^{x_1}}{x_1!}e^{-\lambda}\cdot \frac{\lambda^{x_2}}{x_2!}e^{-\lambda}\cdot ...\cdot \frac{\lambda^{x_n}}{x_n!}e^{-\lambda}\\
             &=& \frac{1}{x_1!x_2!...x_n!} ~ \lambda^{x_1}\lambda^{x_2}...\lambda^{x_n} ~ e^{-\lambda}e^{-\lambda}...e^{-\lambda}\\
             &=& \frac{1}{\prod_{i=1}^n x_i!} \lambda^{x_1+...+x_n} e^{-\lambda-...-\lambda}\\
             &\propto& \lambda^{\sum_{i=1}^n x_i} e^{-n\lambda}\\
             &\propto& \lambda^{s_n} e^{-n\lambda},\qquad s_n=\sum_{i=1}^n x_i
\end{eqnarray*}

Tutta l'**informazione** sulla Poisson è contenuta nella statistica $s_n$.


### La log-verosimiglianza della Poisson

Essendo
\[L(\lambda)\propto \lambda^{s_n} e^{-n\lambda}\]

Allora
\begin{eqnarray*}
          \ell(\lambda)   &=& \log \lambda^{s_n} e^{-n\lambda} \\
                          &=& \log \lambda^{s_n} + \log e^{-n\lambda} \\
                          &=& s_n\log\lambda - n\lambda,\qquad \text{in quanto } \log e^a = a
\end{eqnarray*}

### La stima di massima verosimiglianza della Poisson

Essendo
\[\ell(\lambda)=s_n\log\lambda - n\lambda\]

Allora
\[
          \ell'(\lambda) = \frac {s_n}\lambda-n
\]

E dunque
\begin{eqnarray*}
  \ell'(\lambda)            &=& 0\\
  \frac {s_n}\lambda-n      &=& 0\\
  \frac {s_n}\lambda        &=& n\\
  n\lambda                  &=& s_n\\
  \hat\lambda               &=& \frac{s_n}n\\
  \hat\lambda               &=& \frac{1}n\sum_{i=1}^n x_i
\end{eqnarray*}

### Proprietà dello stimatore di massima verosimiglianza della Poisson $\hat\lambda$

Dunque

:::: {.info data-latex=""}
Siano $X_1,...X_n$ $n$ VC IID, tali che $X_i\sim\text{Pois}(\lambda)$ lo stimatore
di massima verosimiglianza per $\pi$ è
\[\hat \lambda=\frac 1n \sum_{i=1}^nX_i\]
::::

:::: {.info data-latex=""}
Correttezza:
\[
  E(\hat\lambda) =  E\left(\frac{1}n\sum_{i=1}^n X_i\right) = \frac 1 n \sum_{i=1}^n E(X_i) = \frac 1 n \sum_{i=1}^n \lambda = \lambda
\]
::::

:::: {.info data-latex=""}
Mean Squared Error:
\[
  MSE(\hat\lambda) = V(\hat\lambda)
                   = V\left(\frac 1 n \sum_{i=1}^n X_i\right)
                 = \frac 1 {n^2} \sum_{i=1}^n V(X_i)
                 = \frac n {n^2} \lambda
                 = \frac {\lambda}n
\]
::::

:::: {.info data-latex=""}
Consistenza:
\[
  \lim_{n\to+\infty} MSE(\hat\lambda) = \lim_{n\to+\infty} \frac {\lambda}n = 0
\]
::::

:::: {.info data-latex=""}
Standard Error
\[SE(\hat\lambda)=\sqrt{\frac {\lambda}n}\]

Standard Error stimato
\[\widehat{SE(\hat\lambda)}=\sqrt{\frac {\hat\lambda}n}\]
::::

### Esempio $n=5$

```{r 12-Verosimiglianza-19}
set.seed(1)
n <- 5
xpois <- rpois(5,4.6)
```
Il numero di clienti del negozio $A$ è distribuito come una Poisson di parametro $\lambda$ incognito. Dopo $n=`r n`$ giorni di osservazione si sono osservati i seguenti ingressi $(`r xpois`)$.
La stima $\hat\lambda$ di  $\lambda$ è
\[\hat\lambda=\frac 1 5`r sum(xpois)`=`r mean(xpois) `\]

Lo Standard Error stimato
\[\widehat{SE(\hat\lambda)}=\sqrt{\frac {`r mean(xpois) `}`r n`}=`r sqrt(mean(xpois)/n)`\]


```{r 12-Verosimiglianza-20}
par(cex=cex)
par(mfrow=c(1,2),cex=cex)
sn <- sum(xpois)
n <- 5
se <- sqrt(sn/n/n)
L <- function(x) x^sn*exp(-n*x)/prod(factorial(xpois))*10000
curve(L,0,12,axes=F,xlab=expression(lambda),ylab = expression(L(lambda)))
axis(1,c(seq(0,10,by=2),sn/n))
axis(2,las=2)
segments(sn/n,0,sn/n,L(sn/n),lty=2)

l <- function(x) log(x^sn*exp(-n*x)/prod(factorial(xpois))*10000)
curve(l,0,10,axes=F,xlab=expression(lambda),ylab = expression(log~L(lambda)))
axis(1,c(seq(0,12,by=2),sn/n))
axis(2,las=2)
segments(sn/n,-100,sn/n,l(sn/n),lty=2)
par(cex=cex)
```

### Esempio $n=50$

```{r 12-Verosimiglianza-21}
set.seed(1)
n <- 50
hl <- mean(xpois)
```
Il numero di clienti del negozio $A$ è distribuito come una Poisson di parametro $\lambda$ incognito. Dopo $n=`r n`$ giorni di osservazione si è osservata una media di ingressi pari a ingressi $(`r hl`)$.
La stima $\hat\lambda$ di  $\lambda$ è
\[\hat\lambda=`r hl`\]

Lo Standard Error stimato
\[\widehat{SE(\hat\lambda)}=\sqrt{\frac {`r mean(xpois) `} {`r n`}}=`r sqrt(mean(xpois)/n)`\]


```{r 12-Verosimiglianza-22}
par(cex=cex)
par(mfrow=c(1,2),cex=cex)
n <- 50
sn <- mean(xpois)*n


se <- sqrt(hl/n)
L <- function(x) x^sn*exp(-n*x)/3.5e52
curve(L,max(0,sn/n-4*se),sn/n+5*se,axes=F,xlab=expression(lambda),ylab = expression(L(lambda)))
axis(1,c(seq(0,sn/n+5*se,by=.2),sn/n))
axis(2,las=2)
segments(sn/n,0,sn/n,L(sn/n),lty=2)
l <- function(x) log(x^sn*exp(-n*x)/3.5e52)
curve(l,max(0,sn/n-4*se),sn/n+5*se,axes=F,xlab=expression(lambda),ylab = expression(log~L(lambda)))
axis(1,c(seq(0,sn/n+5*se,by=.2),sn/n))
axis(2,las=2)
segments(sn/n,-100,sn/n,l(sn/n),lty=2)
```



## Il modello Normale

Siano $X_1,...,X_n$ $n$ VC IID, replicazioni della stessa $X\sim N(\mu,\sigma^2)$, e dunque con funzione di probabilità:
\[f(x_i;\mu,\sigma^2)\]

:::: {.info data-latex=""}
La verosimiglianza per $(\mu,\sigma^2)$ è
\begin{eqnarray*}
  L(\lambda) &=& \prod_{i=1}^n f(x_i;\mu,\sigma^2)
\end{eqnarray*}
::::

La log-verosimiglianza della Normale

Allora
\begin{eqnarray*}
          \ell(\mu,\sigma^2)   &=& \log \prod_{i=1}^n f(x_i;\mu,\sigma^2)\\
                               &=& \sum_{i=1}^n \log  f(x_i;\mu,\sigma^2)
\end{eqnarray*}


### Verosimiglianza e log-verosimiglianza della Normale


```{r 12-Verosimiglianza-23}
fig.def(5)
```


```{r 12-Verosimiglianza-24}
par(cex=cex)
set.seed(2)
n <- 10
xnorm <- rnorm(n,1)
mmu <- seq(1-4/sqrt(n),1+5/sqrt(n),length.out = 101)
ss2 <- seq(0,sqrt(n),length.out = 101)
Lms <- Vectorize(function(x,y) prod(dnorm(xnorm,x,y)))
LL <- outer(mmu,ss2,Lms)
LL <- LL/max(LL)

muh <- mean(xnorm)
sh2 <- mean(xnorm^2)-muh^2
sh  <- sqrt(sh2)
s2  <- (n)/(n-1)*sh2
se  <- sh/sqrt(n-1)


ms <- expand.grid(mmu,ss2)
ll <- Lms(ms[,1],ms[,2])
mshat <- ms[which(ll==max(ll)),]
#mshat <- c(muh,sqrt(sh2))
Lmax   <- Lms(mshat[1],mshat[2])

xxx<-mmu
yyy<-ss2

par(mfrow=c(1,2),cex=cex)
M <- persp(mmu,ss2,LL,box=F,xlab=expression(mu),ylab=expression(sigma^2),theta = 50,expand = .6,border = NA,shade = .5,ltheta = +30,r=.01)

arrows(
  trans3d(0,min(yyy)-.1,0,M)$x,
  trans3d(0,min(yyy)-.1,0,M)$y,
  trans3d(0,max(yyy)+.1,0,M)$x,
  trans3d(0,max(yyy)+.1,0,M)$y,length = .1)

arrows(
  trans3d(min(xxx)-.1,0,0,M)$x,
  trans3d(min(xxx)-.1,0,0,M)$y,
  trans3d(max(xxx)+.1,0,0,M)$x,
  trans3d(max(xxx)+.1,0,0,M)$y,length = .1)

arrows(
  trans3d(0,0,-.05,M)$x,
  trans3d(0,0,-.05,M)$y,
  trans3d(0,0,.5,M)$x,
  trans3d(0,0,1.1,M)$y,length = .1)


mx <- mshat[,1]
my <- mshat[,2]
points(rbind(
  trans3d(mx,max(yyy)+.1,0,M),
  trans3d(mx,min(yyy)-.1,0,M)
  ), type = "l",lty=2)

points(rbind(
  trans3d(max(xxx)+.1,my,0,M),
  trans3d(min(xxx)-.1,my,0,M)
  ), type = "l",lty=2)


points(rbind(
  trans3d(mx,my,0,M),
  trans3d(mx,my,max(LL),M)
  ), type = "l",lty=2)


text(trans3d(max(xxx),0,+.05,M),expression(mu))
text(trans3d(mx,0,-.05,M),expression(hat(mu)))


text(trans3d(0,max(yyy)+.5,-.05,M),expression(sigma^2))
text(trans3d(0,my,+.05,M),expression(hat(sigma^2)))

text(trans3d(+1.5,.1,1.1,M),expression(L(mu,sigma^2)))


Lms <- Vectorize(function(x,y) sum(dnorm(xnorm,x,y,log = T)))
LL <- outer(mmu,ss2,Lms)
#LL <- LL/max(LL)
LL[is.infinite(LL)]<- NA
LL <- LL-max(LL,na.rm = T)



ms <- expand.grid(mmu,ss2)
ll <- Lms(ms[,1],ms[,2])
mshat <- ms[which(ll==max(ll)),]
#mshat <- c(muh,sqrt(sh2))
Lmax   <- Lms(mshat[1],mshat[2])

xxx<-mmu
yyy<-ss2

M <- persp(mmu,ss2,LL+2,box=F,xlab=expression(mu),ylab=expression(sigma^2),theta = 50,expand = .6,border = NA,shade = .5,ltheta = +30,zlim = c(-.5,3),r=.01)

arrows(
  trans3d(0,min(yyy)-.1,0,M)$x,
  trans3d(0,min(yyy)-.1,0,M)$y,
  trans3d(0,max(yyy)+.1,0,M)$x,
  trans3d(0,max(yyy)+.1,0,M)$y,length = .1)

arrows(
  trans3d(min(xxx)-.1,0,0,M)$x,
  trans3d(min(xxx)-.1,0,0,M)$y,
  trans3d(max(xxx)+.1,0,0,M)$x,
  trans3d(max(xxx)+.1,0,0,M)$y,length = .1)

arrows(
  trans3d(0,0,-1,M)$x,
  trans3d(0,0,-1,M)$y,
  trans3d(0,0,2.9,M)$x,
  trans3d(0,0,2.9,M)$y,length = .1)


mx <- mshat[,1]
my <- mshat[,2]
points(rbind(
  trans3d(mx,max(yyy)+.1,0,M),
  trans3d(mx,min(yyy)-.1,0,M)
  ), type = "l",lty=2)

points(rbind(
  trans3d(max(xxx)+.1,my,0,M),
  trans3d(min(xxx)-.1,my,0,M)
  ), type = "l",lty=2)


points(rbind(
  trans3d(mx,my,0,M),
  trans3d(mx,my,max(LL,na.rm = T),M)
  ), type = "l",lty=2)


text(trans3d(max(xxx),0,.5,M),expression(mu))
text(trans3d(mx,0,-.5,M),expression(hat(mu)))


text(trans3d(0,max(yyy),1,M),expression(sigma^2))
text(trans3d(0,my,1,M),expression(hat(sigma^2)))

text(trans3d(0,.5,3,M),expression(log~L(mu,sigma^2)))


```

### Le stime di massima verosimiglianza della Normale

Per ottenere $\hat\mu$ e $\hat\sigma^2$ bisogna eguagliare a zero il sistema di 
equazioni di derivate di $\ell(\mu,\sigma^2)$ rispetto a $\mu$ e $\sigma^2$
\[
\begin{cases}
  \frac{d\ell(\mu,\sigma^2)}{d\mu}=0\\
  \frac{d\ell(\mu,\sigma^2)}{d\sigma^2}=0
\end{cases}
\]

:::: {.info data-latex=""}
::: {.proposition}
\begin{eqnarray*}
  \hat\mu            &=& \frac 1 n \sum_{i=1}^n x_i\\
  \hat\sigma^2       &=& \frac 1 n \sum_{i=1}^n(x_i-\hat\mu)^2\\
                     &=& \frac 1 n \sum_{i=1}^n x_i^2 -\hat\mu^2
\end{eqnarray*}
:::
::::

Tutta l'**informazione** del campione è contenuta nelle statistiche $\sum_{i=1}^n x_i$ 
e $\sum_{i=1}^n x_i^2$.

::: {.proof}
\begin{eqnarray*}
  L(\mu,\sigma^2;\,\mathbf{x}) &=&  \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}e^
  {-\frac 12\left(\frac{(x_i-\mu)^2}{\sigma^2}\right)}\\
  &=&\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n 
  e^{-\frac 1{2\sigma^2}((x_1-\mu)^2+(x_2-\mu)^2+...+(x_n-\mu)^2)}\\
  &\propto& \sigma^{-2n}\exp\left\{-\frac 1{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right\}\\
\end{eqnarray*}
\begin{eqnarray*}  
  \ell(\mu,\sigma^2) &=& \log L(\mu,\sigma^2;\,\mathbf{x})\\
  &=& \log \sigma^{-2n}\exp\left\{-\frac 1{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right\}\\
  &=& -n \log \sigma^2-\frac 1{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\\
\end{eqnarray*}
\begin{eqnarray*}  
  \frac{d\ell(\mu,\sigma^2)}{d\mu} &=& +\frac 2{2\sigma^2}\sum_{i=1}^n(x_i-\mu)\\
  \frac{d\ell(\mu,\sigma^2)}{d\sigma^2} &=& -\frac n{\sigma^2}+\frac 1{(\sigma^2)^2}\sum_{i=1}^n(x_i-\mu)^2\\
  &=& \frac{-n\sigma^2+\sum_{i=1}^n(x_i-\mu)}{(\sigma^2)^2}\\
  \frac{d\ell(\mu,\sigma^2)}{d\mu} &=& 0 \Rightarrow \hat\mu=\frac 1n \sum_{i=1}^nx_i\\
  \frac{d\ell(\mu,\sigma^2)}{d\sigma^2} &=& 0 \Rightarrow \hat\sigma^2=\frac 1 n \sum_{i=1}^n(x_i-\hat\mu)^2
\end{eqnarray*}
:::

### Proprietà di $\hat\mu$

:::: {.info data-latex=""}
Correttezza per $\mu$:
\[
  E(\hat\mu) =  E\left(\frac{1}n\sum_{i=1}^n X_i\right) = \frac 1 n \sum_{i=1}^n E(X_i) = \frac 1 n \sum_{i=1}^n \mu = \mu
\]
::::

:::: {.info data-latex=""}
Mean Squared Error per $\mu$:
\[
  MSE(\hat\mu) = V(\hat\mu)
                   = V\left(\frac 1 n \sum_{i=1}^n X_i\right)
                 = \frac 1 {n^2} \sum_{i=1}^n V(X_i)
                 = \frac n {n^2} \sigma^2
                 = \frac {\sigma^2}n
\]
::::

:::: {.info data-latex=""}
Consistenza per $\mu$:
\[
  \lim_{n\to+\infty} MSE(\hat\mu) = \lim_{n\to+\infty} \frac {\sigma^2}n = 0
\]

E lo Standard Error:
\[SE(\hat\mu)=\sqrt{\frac {\sigma^2}n}\]
::::

Standard Error stimato tra poco verrà ricavato \@ref(ssem).

### Proprietà di $\hat\sigma^2$ {#vnorm}

:::: {.info data-latex=""}
Correttezza per $\hat\sigma^2$:
\[
  E(\hat\sigma^2) =  \frac {n-1}{n}\sigma^2
\]
$\hat\sigma^2$ non è stimatore corretto per $\sigma^2$.
::::

:::: {.info data-latex=""}
Correzione di $\hat\sigma^2$
\[
  S^2=\frac{n}{n-1}\hat\sigma^2=\frac{n}{n-1}\frac{1}n\sum_{i=1}^n(X_i-\hat\mu)^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\hat\mu)^2
\]
::::

Osserviamo che
\[
  E(S^2)=E\left(\frac{n}{n-1}\hat\sigma^2\right)=\frac{n}{n-1}E\left(\hat\sigma^2\right)= \frac{n}{n-1}\frac {n-1}{n}\sigma^2=\sigma^2
\]

$S^2$ è stimatore corretto per $\sigma^2$.

### Lo $SE$ di $\hat\mu$ {#ssem}

:::: {.info data-latex=""}
Standard Error
\[SE(\hat\mu)=\sqrt{\frac {\sigma^2}n}\]

Standard Error stimato.
\[\widehat{SE(\hat\mu)}=\sqrt{\frac {S^2}n}=\sqrt{\frac {\hat\sigma^2}{n-1}}\]
::::

In quanto
\[\frac {S^2}{n}=\frac 1 nS^2=\frac 1 n \frac{n}{n-1}\hat\sigma^2=\frac{\hat\sigma^2}{n-1}\]


### Esempio $n=10$
```{r 12-Verosimiglianza-25}
xnorm<-xnorm+2
muh <- muh + 2


```
Il fatturato mensile del negozio $A$ è distribuito come una Normale di parametri $\mu$ e $\sigma^2$ incogniti. Dopo $n=`r n`$ mesi di osservazione si sono osservati i seguenti fatturati $\small (`r paste("x_{",1:n,"}=",round(xnorm,3),sep="")`)$.
La stima $\hat\mu$ di  $\mu$ è
\[\hat\mu=\frac 1 {`r n`}`r sum(xnorm)`=`r muh `\]

La varianza campionaria $\hat\sigma^2$
\[\hat\sigma^2=\frac 1 n \sum_{i=1}^n(x_i-\hat\mu)^2=\frac 1 n \sum_{i=1}^n x_i^2-\hat\mu^2=\frac{`r sum(xnorm^2)`}{`r n`}-`r muh`^2=`r sh2`\]

$S^2$ la stima corretta di $\sigma^2$
\[S^2=\frac{n}{n-1}\hat\sigma^2=\frac{`r n`}{`r n-1`}`r sh2`=`r (n)/(n-1)*(sh2)`\]

Lo $SE$ stimato di $\hat\mu$ 
\[\widehat{SE(\hat\mu)} = 
  \sqrt{\frac{`r (n)/(n-1)*(sh2)`}{`r n`}} =
  `r sqrt( (n)/(n-1)*(sh2)/n)`\]


### Esempio $n=100$

```{r 12-Verosimiglianza-26}
n <- 100
```

L'ammontare delle transazioni finanziarie compiute al minuto dal server $A$ è distribuito come una Normale di parametri $\mu$ e $\sigma^2$ incogniti. Dopo $n=`r 100`$ ore di osservazione si sono osservati $\bar x=`r muh`$, $\hat\sigma=`r sh`$ .
La stima $\hat\mu$ di  $\mu$ è
\[\hat\mu=`r muh `\]

La varianza campionaria $\hat\sigma^2$
\[\hat\sigma^2=`r sh`^2=`r sh2`\]

$S^2$ la stima corretta di $\sigma^2$
\[S^2=\frac{n}{n-1}\hat\sigma^2=\frac{`r n`}{`r n-1`}`r sh2`=`r (n)/(n-1)*(sh2)`\]

Lo $SE$ stimato di $\hat\mu$ 
\[\widehat{SE(\hat\mu)} = 
  \sqrt{\frac{`r (n)/(n-1)*(sh2)`}{`r n`}} =
  `r sqrt( (n)/(n-1)*(sh2)/n)`\]


### Perché $n-1$

Per calcolare la varianza campionaria dobbiamo prima calcolare la media dei dati.
Per calcolare la media bisogna sommare i dati, per esempio se $n=3$: 
$x_1=7$, $x_2=8$ e $x_3=11$
\[x_1+x_2+x_3 = 26\]

Ma $x_1=7$, $x_2=8$ e $x_3=11$ non sono l'unica tripla di $x$ che somma a 26, ma
$n-1=2$ valori possono essere scelti liberamente (es $x_1=5$ e $x_2=15$):
Il terzo **è vincolato**:
\[x_3=26-x_1-x_2\]

Fissata la somma il sistema _ha perso un grado di libertà_.


## Proprietà degli stimatori di massima verosimiglianza

:::: {.info data-latex=""}
::: {.proposition name="Stimatori di massima verosimgilianza"}
Siano $X_1,...,X_n$ $n$ VC IID, replicazioni di $X\sim \mathscr{L}(\theta)$ e sia $\hat\theta$ lo stimatore di massima verosimiglianza per per $\theta$, allora

1. $\hat\theta$ non è sempre stimatore corretto ma è sempre corretto asintoticamente:
\[E(\hat\theta)\xrightarrow{n\to\infty}\theta\]
  

2. $\hat\theta$ non è sempre stimatore a _massima efficienza_ ma lo è sempre asintoticamente:
\[V(\hat\theta)\xrightarrow{n\to\infty}I^{-1}(\theta)\]
dove $I(\theta)$ è l'infromazione di Fisher.   
  
3. $\hat\theta$ è asintoticamente distribuito normalmente
\[\hat\theta\operatorname*{\sim}_a N(\theta,I^{-1}(\theta))\]
  

4. Lo stimatore di massima verosimiglianza è invariante alle trasformazioni monotone invertibili $g$:
  
  \[ \text{se } \psi=g(\theta), \text{ allora } \hat\psi = g(\hat\psi)\]
:::
::::

  1. La proprietà uno riguarda la correttezza. Non sempre gli SMV sono corretti
  ma lo sono sempre asintoticamente. Esempio: lo stimatore $\hat\sigma^2$ di $\sigma^2$ non è corretto solo asintoticamente
  \[E(\hat\sigma^2)=\frac{n-1}{n}\sigma^2\xrightarrow{n\to\infty}\sigma^2\]
  2. La proprietà due riguarda l'efficienza dello stimatore: non sempre lo
  SMV è il più efficiente per piccoli campioni, ma se il campione diventa grande, lo
  SMV è lo stimatore che raggiunge la varianza minima. La varianza minima è chiamata Informazione di Fisher ed è indicata con $I^{-1}(\theta)$:
\[
  I(\theta)=-E\left(\ell''(\theta)\right)
\]
dove $\ell''(\theta)$ è la derivata seconda della log verosimiglianza calcolata in $\theta$.

  - $I(\theta)$ è la curvatura media della log verosimiglianza intorno al punto $\theta$.
  - $I^{-1}(\theta)$ è un risultato teorico ed un limite sotto al quale nessuno stimatore può scendere.
  - Se esiste lo stimatore più efficiente allora è quello di _massima verosimiglianza_.
  - Esempio: $\hat\pi$, $\hat\lambda$ e $\hat\mu$ sono stimatori a efficienza massima.

\begin{eqnarray*}
   I^{-1}(\pi) &=&  \frac{\pi(1-\pi)}{n}\\
   I^{-1}(\lambda) &=&  \frac{\lambda}{n}\\
   I^{-1}(\mu) &=&  \frac{\sigma^2}{n}
\end{eqnarray*}

  3. La proprietà tre ci garantisce che, per $n$ sufficientemente alto, sappiamo 
  la distribuzione degli SMV
  
  - Esempio: lo stimatore $\hat\pi$ di $\pi$, dal TLC
  \[\hat\pi\operatorname*{\sim}_a N\left(\pi,\frac{\pi(1-\pi)}{n}\right)\]
  - Esempio: lo stimatore $\hat\lambda$ di $\lambda$, dal TLC
  \[\hat\lambda\operatorname*{\sim}_a N\left(\lambda,\frac{\lambda}{n}\right)\]
  
  4. La proprietà 4 garantisce che trasformazioni invertibili dei parametri non 
  richiedono di ricalcolare la SMV.
  
  - Esempio: $\sigma=\sqrt{\sigma^2}$ e dunque $\hat\sigma=\sqrt{\hat\sigma^2}$


<!--chapter:end:12-Verosimiglianza.Rmd-->

---
editor_options: 
  chunk_output_type: console
---
```{r setup13, include=FALSE}
rm(list=ls())


source("intro.R")

```

# Stima Intervallare


## Obiettivo

Oltre alla stima di un punto specifico $\hat\theta$ dello spazio dei parametri potremmo essere interessati a trovare _regioni più verosimili_.

L'obiettivo è di stimare un intervallo nel quale pensiamo _verosimilmente_ si collochi il vero $\theta$ alla luce dei dati.

Nel gergo dei sondaggisti viene chiamata forbice.

La teoria della verosimiglianza offre tutti gli strumenti per la derivazione coerente di intervalli per una classe molto ampia di modelli di probabilità. La trattazione sistematica attraverso la verosimiglianza esula dagli scopi di questo corso.



## Il Contesto Probabilistico

```{r 13-stima-intervallare-2} 
n <- 5
mu <- 2.5
s2 <- 1.5^2
s  <- sqrt(s2)
se <- sqrt(s2/n)
mumax <- 5
za2 <- round(qnorm(.975),2)

```

Siano $X_1,...X_`r n`$, $n=`r n`$ VC IID, $X_i\sim N(\mu=`r mu`,\sigma^2=`r s2`)$

Dalle proprietà della normale
\[\frac 1 n \sum_{i=1}^n X_i=\bar X=\hat\mu\sim N\left(\mu,\frac{\sigma^2}{n}\right), \qquad\text{ovvero }\hat \mu\sim N\left(\mu,SE^2(\hat \mu)\right) \]


In questo caso
\begin{eqnarray*}
\hat \mu &\sim& N\left(`r mu`,\frac{`r s2`}{`r n`}\right)\\
       &\sim& N\left(`r mu`,\left(\frac{`r s`}{\sqrt{`r n`}}\right)^2\right)\\
       &\sim& N\left(`r mu`,`r se`^2\right)
\end{eqnarray*}

La densità di probabilità di $\hat \mu$.

```{r 13-stima-intervallare-3}
par(mfrow=c(1,1),cex=cex)
curve(dnorm(x,mu,se),mu-4*se,mu+4*se,axes=F,xlab=expression(hat(mu)),ylab=expression(f(hat(mu),mu,sigma^2/n)) )
axis(1,round(mu+se*(-4:4),3) )
axis(2)
```

### Un intervallo per $\hat \mu$

Domanda: qual è quel valore $x^*>0$ tale che
\[P(\mu-x^*<\hat \mu<\mu+x^*)=0.95~~?\]

```{r 13-stima-intervallare-4}
q1 <- qnorm(.025,mu,se)
q2 <- qnorm(.975,mu,se)
curve(dnorm(x,mu,se),mu-4*se,mu+4*se,axes=F,xlab=expression(hat(mu)),ylab=expression(f(hat(mu),mu,sigma^2/n)) )
axis(1,at = c(mu-4*se,q1,mu,q2,mu+4*se), labels = c(mu-4*se,expression(mu-x^{'*'}),expression(mu),expression(mu+x^{'*'}),mu+4*se))
axis(2)

segments(c(q1,q2),c(0,0),c(q1,q2),dnorm(c(q1,q2),mu,se),lty=2)
text(mu,.1,'0.95')
text(mu-3*se ,.05,'0.025')
text(mu+3*se ,.05,'0.025')
```

qual è quel numero $x^*$ tale che
\[P(\hat \mu>\mu+x^*)=0.025,\qquad P(\hat \mu < \mu - x^*)=0.025 ~~?\]

Trasferiamo il problema sulla normale standard

Se $Z\sim N(0,1)$

Qual è quel numero $z^*$ tale che
\[P(Z>z^*)=0.025 ~~?\]

Dall'ultima riga delle tavole osserviamo che $z^*=`r za2`$, infatti
\[P(Z>`r za2`)=1- \Phi(`r za2`)=1-0.975=0.025 \]

E dunque
\[P(-19.6<Z<+`r za2`)=0.95\]

Ci serviamo delle tavole della Z per $\hat \mu$, ricordiamo che  $\hat \mu\sim N(\mu,SE^2(\hat \mu))$, allora
\[Z=\frac{\hat \mu-\mu}{SE(\hat \mu)}\sim N(0,1)\]

e quindi
\begin{eqnarray*}
  P(-`r za2`<Z<+`r za2`) &=& 0.95 \\
  P\left(-`r za2`<\frac{\hat \mu -\mu}{SE(\hat \mu)}< +`r za2`\right) &=& 0.95 \\
  P\left(-`r za2`~SE(\hat \mu)<\hat \mu -\mu< +`r za2`~SE(\hat \mu)\right) &=& 0.95 \\  
  P\left(\mu-`r za2`~SE(\hat \mu)<\mu+\hat \mu -\mu< \mu+`r za2`~SE(\hat \mu)\right) &=& 0.95 \\    
  P(\mu-`r za2`~SE(\hat \mu)<\hat \mu<\mu+`r za2`~SE(\hat \mu)) &=& 0.95
\end{eqnarray*}

Numericamente
\begin{eqnarray*}
  P\left(`r mu`-`r za2`~\frac{`r s`}{\sqrt{`r n`}}<\hat \mu < `r mu`+`r za2`~\frac{`r s`}{\sqrt{`r n`}}\right) &=& 0.95 \\
  P\left(`r mu`-`r za2`\times`r se`<\hat \mu < `r mu`+`r za2`\times`r se`\right) &=& 0.95 \\
  P(`r mu-za2*se` < \hat \mu < `r mu+za2*se`) &=& 0.95 
\end{eqnarray*}

### $n$ e $\sigma^2$ rimangono fissi, cambiamo $\mu$
```{r 13-stima-intervallare-5}
mu <- 1.2
```
Se $n=`r n`$ e $\sigma^2=`r s2`$, ma per esempio $\mu= `r mu`$ allora
\begin{eqnarray*}
  P(\mu-`r za2`~SE(\hat \mu)<\hat \mu<\mu+`r za2`~SE(\hat \mu)) &=& 0.95\\
  P\left(\mu-`r za2`~\frac{\sigma}{\sqrt n}<\hat \mu < \mu+`r za2`~\frac{\sigma}{\sqrt n}\right) &=& 0.95 \\
  P\left(`r mu`-`r za2`~\frac{`r s`}{\sqrt{`r n`}}<\hat \mu < `r mu`+`r za2`~\frac{`r s`}{\sqrt{`r n`}}\right) &=& 0.95 \\
  P\left(`r mu`-`r za2`\times`r se`<\hat \mu < `r mu`+`r za2`\times`r se`\right) &=& 0.95 \\
  P(`r mu-za2*se` < \hat \mu < `r mu+za2*se`) &=& 0.95 
\end{eqnarray*}


```{r 13-stima-intervallare-6}
mu <- 3.4
```
Se $n=`r n`$ e $\sigma^2=`r s2`$, ma per esempio $\mu= `r mu`$ allora
\begin{eqnarray*}
  P\left(`r mu`-`r za2`~\frac{`r s`}{\sqrt{`r n`}}<\hat \mu < `r mu`+`r za2`~\frac{`r s`}{\sqrt{`r n`}}\right) &=& 0.95 \\
  P\left(`r mu`-`r za2`\times`r se`<\hat \mu < `r mu`+`r za2`\times`r se`\right) &=& 0.95 \\
  P(`r mu-za2*se` < \hat \mu < `r mu+za2*se`) &=& 0.95 
\end{eqnarray*}


```{r 13-stima-intervallare-7}
mu <- 0.6
fig.def(4)
```
Se $n=`r n`$ e $\sigma^2=`r s2`$, ma per esempio $\mu= `r mu`$ allora
\begin{eqnarray*}
  P\left(`r mu`-`r za2`\times`r se`<\hat \mu < `r mu`+`r za2`\times`r se`\right) &=& 0.95 \\
  P(`r mu-za2*se` < \hat \mu < `r mu+za2*se`) &=& 095 
\end{eqnarray*}


Rimangono fissi $n$ e $\sigma^2$ , cambiamo $\mu$

```{r 13-stima-intervallare-8}
fig.def(4)
```


```{r 13-stima-intervallare-9}
op <- par(mfrow=c(2,2),cex=cex,mar=c(.15,.15,.15,.15))
mug <- seq(-1,mumax,length.out = 4)
xbar <- mug
plot(mug,xbar,axes = F,asp = 1,xlab = "", ylab = "",type='l')

arrows(-1,0,mumax,0,length = .1)
arrows(0,-1,0,mumax,length = .1)
text(mumax-.5,-.2,expression(mu))
text(-.2,mumax-.5,expression(hat(mu)))

lines(mug,xbar+za2*se,lty=3)
lines(mug,xbar-za2*se,lty=3)

mui <- 2.5
segments(mui,-mumax,mui,mumax)
segments(mui,mui-za2*se,mui,mui+za2*se,lwd=2,col=ared)
text(mui+.2,-.2,mui)
segments(mui,mui-za2*se,0,mui-za2*se,lty=2)
segments(mui,mui+za2*se,0,mui+za2*se,lty=2)
segments(mui,mui,0,mui,lty=2)
text(-.3,mui-za2*se,round(mui-za2*se,3))
text(-.3,mui+za2*se,round(mui+za2*se,3))
text(-.3,mui,mui)

mug <- seq(-1,mumax,length.out = 4)
xbar <- mug
plot(mug,xbar,axes = F,asp = 1,xlab = "", ylab = "",type='l')

arrows(-1,0,mumax,0,length = .1)
arrows(0,-1,0,mumax,length = .1)
text(mumax-.5,-.2,expression(mu))
text(-.2,mumax-.5,expression(hat(mu)))

lines(mug,xbar+za2*se,lty=3)
lines(mug,xbar-za2*se,lty=3)

mui <- 1.2
segments(mui,-mumax,mui,mumax)
segments(mui,mui-za2*se,mui,mui+za2*se,lwd=2,col=ared)
text(mui+.2,-.2,mui)
segments(mui,mui-za2*se,0,mui-za2*se,lty=2)
segments(mui,mui+za2*se,0,mui+za2*se,lty=2)
segments(mui,mui,0,mui,lty=2)
text(-.3,mui-za2*se,round(mui-za2*se,3))
text(-.3,mui+za2*se,round(mui+za2*se,3))
text(-.3,mui,mui)

mug <- seq(-1,mumax,length.out = 4)
xbar <- mug
plot(mug,xbar,axes = F,asp = 1,xlab = "", ylab = "",type='l')

arrows(-1,0,mumax,0,length = .1)
arrows(0,-1,0,mumax,length = .1)
text(mumax-.5,-.2,expression(mu))
text(-.2,mumax-.5,expression(hat(mu)))

lines(mug,xbar+za2*se,lty=3)
lines(mug,xbar-za2*se,lty=3)

mui <- 3.4
segments(mui,-mumax,mui,mumax)
segments(mui,mui-za2*se,mui,mui+za2*se,lwd=2,col=ared)
text(mui+.2,-.2,mui)
segments(mui,mui-za2*se,0,mui-za2*se,lty=2)
segments(mui,mui+za2*se,0,mui+za2*se,lty=2)
segments(mui,mui,0,mui,lty=2)
text(-.3,mui-za2*se,round(mui-za2*se,3))
text(-.3,mui+za2*se,round(mui+za2*se,3))
text(-.3,mui,mui)

mug <- seq(-1,mumax,length.out = 4)
xbar <- mug
plot(mug,xbar,axes = F,asp = 1,xlab = "", ylab = "",type='l')

arrows(-1,0,mumax,0,length = .1)
arrows(0,-1,0,mumax,length = .1)
text(mumax-.5,-.2,expression(mu))
text(-.2,mumax-.5,expression(hat(mu)))

lines(mug,xbar+za2*se,lty=3)
lines(mug,xbar-za2*se,lty=3)

mui <- 0.6
segments(mui,-mumax,mui,mumax)
segments(mui,mui-za2*se,mui,mui+za2*se,lwd=2,col=ared)
text(mui+.2,-.2,mui)
segments(mui,mui-za2*se,0,mui-za2*se,lty=2)
segments(mui,mui+za2*se,0,mui+za2*se,lty=2)
segments(mui,mui,0,mui,lty=2)
text(-.3,mui-za2*se,round(mui-za2*se,3))
text(-.3,mui+za2*se,round(mui+za2*se,3))
text(-.3,mui,mui)

par(op)
fig.def(4.5)
```


### $n$ e $\sigma^2$ rimangono fissi e noti, $\mu$ incognita $\hat \mu=2.6$

```{r 13-stima-intervallare-10}
if (!html) par(cex=.5)
mug <- seq(-1,mumax,length.out = 4)
xg <- mug
plot(mug,xg,axes = F,asp = 1,xlab = "", ylab = "",type='l')

arrows(-1,0,mumax,0,length = .1)
arrows(0,-1,0,mumax,length = .1)
text(mumax+.1,+.2,expression(mu))
text(-.2,mumax-.5,expression(hat(mu)))

lines(mug,xg+za2*se,lty=3)
lines(mug,xg-za2*se,lty=3)

xbar <- 2.6
segments(-mumax,xbar,mumax,xbar)
segments(xbar-za2*se,xbar,xbar+za2*se,xbar,col=ared,lwd=2)

segments(xbar-za2*se,0,xbar-za2*se,xbar,lty=2)
segments(xbar+za2*se,0,xbar+za2*se,xbar,lty=2)
segments(xbar        ,0,xbar        ,xbar,lty=2)

text(xbar,-.2,xbar)
text(-.25,xbar+.15,xbar)

text(xbar-za2*se,-.2,round(xbar-za2*se,3))
text(xbar+za2*se,-.2,round(xbar+za2*se,3))
arrows(xbar-za2*se,.5,xbar,.5,length = .05,code=3)
arrows(xbar+za2*se,.5,xbar,.5,length = .05,code=3)
text(xbar-za2*se/2,.65,paste(za2,"x",round(se,3)))
text(xbar+za2*se/2,.65,paste(za2,"x",round(se,3)))
```


Algebricamente, osserviamo
\begin{eqnarray*}
  P(-`r za2`<Z<+`r za2`) &=& 0.95 \\
  P\left(-`r za2`<\frac{\hat \mu -\mu}{SE(\hat \mu)}< +`r za2`\right) &=& 0.95 \\
  P\left(- `r za2`~SE(\hat \mu)<\hat \mu-\mu<+`r za2`~SE(\hat \mu)\right) &=& 0.95\\
  P\left( - `r za2`~\frac\sigma{\sqrt n}<\hat \mu-\mu<\mu+`r za2`~\frac\sigma{\sqrt n}\right) &=& 0.95\\
  P\left(-\hat \mu - `r za2`~\frac\sigma{\sqrt n}< -\hat \mu+\hat \mu-\mu<-\hat \mu+`r za2`~\frac\sigma{\sqrt n}\right) &=& 0.95 \qquad\text{sottraggo $\hat \mu$}\\
  P\left(-\hat \mu- `r za2`~\frac\sigma{\sqrt n}<-\mu<-\hat \mu+`r za2`~\frac\sigma{\sqrt n}\right) &=& 0.95\\
  \left(+\hat \mu+ `r za2`~\frac\sigma{\sqrt n}>+\mu>+\hat \mu-`r za2`~\frac\sigma{\sqrt n}\right) &=& 0.95\qquad\text{cambio segno e verso}\\
  P\left(\hat \mu- `r za2`~\frac\sigma{\sqrt n}<\mu<\hat \mu+`r za2`~\frac\sigma{\sqrt n}\right) &=& 0.95\\
\end{eqnarray*}


## Intervalli casuali

::: {.att data-latex=""}
\[  P\left(\hat \mu- `r za2`~\frac\sigma{\sqrt n}<\mu<\hat \mu+`r za2`~\frac\sigma{\sqrt n}\right) = 0.95\]
**non** è la probabilità che $\mu$ si trovi tra $\hat \mu- `r za2`~\frac\sigma{\sqrt n}$ e $\hat \mu+ `r za2`~\frac\sigma{\sqrt n}$,

\[  P\left(\hat \mu- `r za2`~\frac\sigma{\sqrt n}<\mu<\hat \mu+`r za2`~\frac\sigma{\sqrt n}\right) = 0.95\]
è la probabilità che **l'intervallo casuale** $\left[\hat \mu- `r za2`~\frac\sigma{\sqrt n},\hat \mu+ `r za2`~\frac\sigma{\sqrt n}\right]$  cada su $\mu$.
:::

Se quindi $\hat \mu = `r xbar`$ l'intervallo
\begin{eqnarray*}
 \left[\hat \mu- `r za2`~\frac\sigma{\sqrt n},\hat \mu+ `r za2`~\frac\sigma{\sqrt n}\right] &=& 
\left[`r xbar`- `r za2`~\frac{`r s`}{\sqrt `r n`},\hat \mu+ `r za2`~\frac{`r s`}{\sqrt `r n`}\right]\\
 &=& \left[`r xbar- za2*se`,`r xbar+ za2*se`\right]
\end{eqnarray*}

 L'intervallo $\left[`r xbar- za2*se`,`r xbar+ za2*se`\right]$ è una realizzazione dell'intervallo casuale $\left[\hat \mu- `r za2`~\frac\sigma{\sqrt n},\hat \mu+ `r za2`~\frac\sigma{\sqrt n}\right]$.



## Intervallo di confidenza per $\mu$ al 95%, $n=5$ e $\sigma^2=2.25$.

Se $\hat \mu = `r xbar`$ l'intervallo
\begin{eqnarray*}
 \left[\hat \mu- `r za2`~\frac\sigma{\sqrt n},\hat \mu+ `r za2`~\frac\sigma{\sqrt n}\right] &=& 
\left[`r xbar`- `r za2`~\frac{`r s`}{\sqrt `r n`},\hat \mu+ `r za2`~\frac{`r s`}{\sqrt `r n`}\right]\\
 &=& \left[`r xbar- za2*se`,`r xbar+ za2*se`\right]\\
\end{eqnarray*}
è chiamato *intervallo di confidenza* per $\mu$ al 95%.

::: {.nota data-latex=""}
Infelice traduzione di _Confidence Interval_,

  - _to be confident that the real parameter is in $\left[`r xbar- za2*se`,`r xbar+ za2*se`\right]$, with a confidence level of 95%_.
  - _Siamo fiduciosi che il vero parametro si trovi in $\left[`r xbar- za2*se`,`r xbar+ za2*se`\right]$, con un livello di fiducia del 95%_
:::

L'intervallo è costruito con una metodologia che 95 volte su 100 produce intervalli che coprono il vero $\mu$.
Il long run:

```{r 13-stima-intervallare-11}
fig.def(4)
```

```{r 13-stima-intervallare-12}
par(cex=.45)
mu <- 2
  plot(mug,xg,axes = F,asp = 1,xlab = "", ylab = "",type='l')
i <- 1
intc <- function(xbar){
  ex <- substitute(expression(mu[obs]^(i)),list(i=i))
  mug <- seq(-1,mumax,length.out = 4)
  xg <- mug

  arrows(-1,0,mumax,0,length = .1)
  arrows(0,-1,0,mumax,length = .1)
  text(mumax+.1,+.2,expression(mu))
  text(-.2,mumax-.5,expression(hat(mu)))
  
  lines(mug,xg+za2*se,lty=3)
  lines(mug,xg-za2*se,lty=3)
  
  points(0,xbar,pch=4,cex=.8,col=4)
  text(-.5,xbar,eval(ex))
  
  segments(0,mu-za2*se,mu,col='grey',lty=3)
  segments(0,mu+za2*se,mu,col='grey',lty=3)
  
  segments(mu,-mumax,mu,mumax,col='grey30')
  segments(mu,mu-za2*se,mu,mu+za2*se,lwd=1,col=ared)
  segments(0,mu-za2*se,0,mu+za2*se,lwd=1,col=ared)
  text(mu+.4,-.2,expression(mu[vera]),col=ared)
  points(mu,0,pch=16,col=ared,cex=cex)
  segments(xbar-za2*se,xbar,xbar+za2*se,xbar)
  segments(xbar-za2*se,0,xbar-za2*se,xbar,lty=2)
  segments(xbar+za2*se,0,xbar+za2*se,xbar,lty=2)
  segments(xbar        ,0,xbar        ,xbar,lty=2)
  
  text(xbar+.4,.2,eval(ex))
}

intc(0.5); i <- i+1
intc(3.7); i <- i+1
intc(2.8); i <- i+1
intc(1.1); i <- i+1
intc(4.4); i <- i+1
intc(-0.9); i <- i+1



```


## Stimatori e intervalli di confidenza

Se $\theta$ è il parametro da stimare uno stimatore puntuale $h$ è una VC
\[h(X_1,...,X_n)=h=\hat\theta\]

Siano $L_1(X_1,...,X_n)=L_1$ e $L_2(X_1,...,X_n)=L_2$ due statistiche tali che 
  $L_1\leq L_2$ per ogni campione $X_1,...,X_n$.
L'intervallo
$$[L_1,L_2]$$
è un *intervallo casuale*.

:::: {.info data-latex=""}
::: {.definition}
Un  __intervallo di confidenza__ per $\theta$ al livello $(1-\alpha)\times 100\%$ è costruito su quella coppia di statistiche $L_1$ e $L_2$ tali che
\[P(L_1<\theta<L_2)=1-\alpha\]
:::
::::


Un  _intervallo di confidenza_ per $\theta$ al livello $(1-\alpha)\times 100\%$ è l'intervallo $[L_1,L_2]$ calcolato sui dati del campione.

## Massima Verosimiglianza e intervalli di confidenza

Se $\hat\theta$ è lo stimatore di massima verosimiglianza per $\theta$, se $n$ è sufficientemente alto
\[\hat\theta\operatorname*{\sim}_a N(\theta,\widehat{SE^2(\hat\theta)}\equiv I^{-1}(\theta))\]


L'_intervallo di confidenza_ per $\theta$ al livello $(1-\alpha)\times 100\%$ è ricavato da: 
\[P(\hat\theta-z_{\alpha/2}\widehat{SE(\hat\theta)}<\theta<\hat\theta+z_{\alpha/2}\widehat{SE(\hat\theta)})=1-\alpha\]

Un  _intervallo di confidenza_ per $\theta$ al livello $(1-\alpha)\times 100\%$ è l'intervallo $[\hat\theta-z_{\alpha/2}\widehat{SE(\hat\theta)},\hat\theta+z_{\alpha/2}\widehat{SE(\hat\theta)}]$ calcolato sui dati del campione.


## Intervalli di Confidenza per $\mu$ al livello $(1-\alpha)\times 100$, $\sigma^2$ nota
```{r 13-stima-intervallare-13}
za2 <- round(qnorm(.975),4)
```
Sia $0<\alpha<1$, osserviamo che
\begin{eqnarray*}
  P(-z_{\alpha/2}<Z<+z_{\alpha/2}) &=& 1-\alpha \\
  P\left(-z_{\alpha/2}<\frac{\hat \mu -\mu}{SE(\hat \mu)}< +z_{\alpha/2}\right) &=& 1-\alpha \\
  P\left(\hat \mu- z_{\alpha/2}~\frac\sigma{\sqrt n}<\mu<\hat \mu+z_{\alpha/2}~\frac\sigma{\sqrt n}\right) &=& 1-\alpha
\end{eqnarray*}

:::: {.info data-latex=""}
::: {.definition name="Intervallo di Confidenza per $\mu$ ($\sigma^2$ nota)"}
Si definisce L'IdC al livello $(1-\alpha)\times100\%$ per $\mu$ con $\sigma^2$ nota, l'intervallo
\[IdC:~~\left[\hat \mu- z_{\alpha/2}~\frac\sigma{\sqrt n},\hat \mu+ z_{\alpha/2}~\frac\sigma{\sqrt n}\right]\]
:::
::::

Dove $z_{\alpha/2}$ è quel valore tale che $P(Z>z_{\alpha/2})=\alpha/2$

\[  P\left(\hat \mu- z_{\alpha/2}~\frac\sigma{\sqrt n}<\mu<\hat \mu+z_{\alpha/2}~\frac\sigma{\sqrt n}\right) = 1-\alpha\]
è la probabilità che **l'intervallo casuale**
$\left[\hat \mu- z_{\alpha/2}~\frac\sigma{\sqrt n},\hat \mu+ z_{\alpha/2}~\frac\sigma{\sqrt n}\right]$
cada su $\mu$.

Il valore $0<\alpha<1$, può essere qualunque ma solitamente si usa

  - $\alpha=0.05$: intervalli al $(1-0.05)\times 100\%=0.95\times 100\%=95\%$
  - $\alpha=0.01$: intervalli al $(1-0.01)\times 100\%=0.99\times 100\%=99\%$
  - Gli intervalli per $\alpha=0.1$ e $\alpha=0.001$, intervalli al $90\%$ e al $99.9\%$ meno usati.


dalle tavole abbiamo<div/>
| $\alpha$        | $\alpha/2$            | $z_{\alpha/2}$                    |                       |           |
|:----------------|:----------------------|:----------------------------------|:----------------------|-----------|
|  $\alpha=0.1$   | con $\alpha/2=0.05$   | e quindi $z_{\alpha/2}=z_{0.05}$  | $=`r qnorm(1-.05)`$   | Raro      |
|  $\alpha=0.05$  | con $\alpha/2=0.025$  | e quindi $z_{\alpha/2}=z_{0.025}$ | $=`r qnorm(1-.025)`$  | Freq.     |
|  $\alpha=0.01$  | con $\alpha/2=0.005$  | e quindi $z_{\alpha/2}=z_{0.005}$ | $=`r qnorm(1-.005)`$  | Freq.     |
|  $\alpha=0.001$ | con $\alpha/2=0.0005$ | e quindi $z_{\alpha/2}=z_{0.005}$ | $=`r qnorm(1-.0005)`$ | Raro      |

```{r 13-stima-intervallare-14}
za2 <- qnorm(.995)
```


:::: {.example}
Intervallo al 99\%, $\hat \mu=2.6$, $n=5$, $\sigma^2=`r s2`$.
L'intervallo al 99% implica un $\alpha=0.01$, infatti
\[1-\alpha=0.99\]

E dunque
\[\alpha/2=0.005\]

Dalle tavole
\[z_{\alpha/2}=z_{0.005}=`r qnorm(.995)`\]

Se quindi $\hat \mu = `r xbar`$ l'intervallo
\begin{eqnarray*}
 \left[\hat \mu- `r za2`~\frac\sigma{\sqrt n},\hat \mu+ `r za2`~\frac\sigma{\sqrt n}\right] &=&
\left[`r xbar`- `r za2`~\frac{`r s`}{\sqrt `r n`},`r xbar`+ `r za2`~\frac{`r s`}{\sqrt `r n`}\right]\\
 &=& \left[`r xbar- za2*se`,`r xbar+ za2*se`\right]
\end{eqnarray*}

È l'intervallo di confidenza per $\mu$ al $99\%$
::::

```{r 13-stima-intervallare-15}
za2 <- qnorm(.95)
fig.def()
```

:::: {.example }
Intervallo al $99\%$, $\hat \mu=2.6$, $n=5$, $\sigma^2=`r s2`$. L'intervallo al 90% implica un $\alpha=0.1$, infatti
\[1-\alpha=0.90\]

E dunque
\[\alpha/2=0.05\]

Dalle tavole
\[z_{\alpha/2}=z_{0.05}=`r qnorm(.95)`\]

Se quindi $\hat \mu = `r xbar`$ l'intervallo
\begin{eqnarray*}
 \left[\hat \mu- `r za2`~\frac\sigma{\sqrt n},\hat \mu+ `r za2`~\frac\sigma{\sqrt n}\right] &=&
\left[`r xbar`- `r za2`~\frac{`r s`}{\sqrt `r n`},`r xbar`+ `r za2`~\frac{`r s`}{\sqrt `r n`}\right]\\
 &=& \left[`r xbar- za2*se`,`r xbar+ za2*se`\right]
\end{eqnarray*}

È l'intervallo di confidenza per $\mu$ al $90\%$.
::::

Osserviamo graficamente gli intervalli di confidenza per $\mu$ con $\hat \mu=`r xbar`$, $n=`r n`$ e $\sigma^2=`r s2`$, al 90%, 95%, 99% e 99.9%.



```{r 13-stima-intervallare-1,results='asis'}
plot(c(xbar-4.5*se,xbar+4.5*se),c(-.4,1),axes=F,xlab='',ylab="",type='n')
arrows(xbar-4.5*se,0,xbar+4.5*se,0,length = .1)
points(xbar,0,pch='|')
text(xbar+.15,-.05,expression(hat(mu)))
alp <- c(.1,.05,.01,.001)
IdC <- function(a) c(xbar+c(-1,1)*qnorm(1-a/2)*se,2*qnorm(1-a/2)*se)
for (i in 1:length(alp)){
segments(IdC(alp[i])[1],i/7,IdC(alp[i])[2],i/7,col=i+1)
segments(IdC(alp[i])[1:2],0,IdC(alp[i])[1:2],i/7,lty = 2,col= i+1)
text(xbar,i/7,pos = 1,paste("intervallo al",(1-alp[i])*100,"%"),cex=.5)
}

tb <- data.frame(sapply(alp, IdC))
row.names(tb) <- c("$\\hat \\mu - z_{\\alpha/2}SE(\\hat \\mu)$","$\\hat \\mu + z_{\\alpha/2}SE(\\hat \\mu)$","Ampiezza")
colnames(tb) <- paste("$\\alpha=",alp,"$",sep='')

kable(tb,row.names = T,booktabs = T, escape = F,linesep = "", digits = 4)
```


:::: {.example}

```{r 13-stima-intervallare-16}
xbar <- 19
sbar <- 1.979
s    <- 1.5
n    <- 6
```
Un venditore di bustine di tè assicura che ogni bustina ha un
peso medio pari a 20g con una SD pari a 1.5g.
L'acquirente esegue 6 misure di controllo e ottiene i seguenti
risultati: 19; 20; 20.5; 21; 18.5; 15.
La media di questi numeri è $\hat\mu = 19$ e $\widehat{\sigma}=1.979$.
Determinare un IdC al livello di $(1-\alpha)=0.99$ per $\mu$.

\begin{eqnarray*}
& & \left[\hat\mu -z_{\alpha/2} \frac{\sigma} {\sqrt{n}};
          \hat\mu +z_{\alpha/2} \frac{\sigma} {\sqrt{n}} \right] \\
&=& \left[ 19 -2.576 \frac{1.5} {\sqrt{6}}; 19 +2.576 \frac{1.5} {\sqrt{6}} \right] \\
&=& \left[ 19 - 1.5775; 19 + 1.5775 \right]        \\
&=& \left[ 17.4225;\quad 20.5775 \right]
\end{eqnarray*}

Notare la modalità di indicare la SD della $\cal{P}$ e dei dati.
Vi sono due informazioni su $\sigma$ e bisogna scegliere quella giusta.
::::


## Intervalli di Confidenza per $\mu$ al livello $(1-\alpha)\times 100$, $\sigma^2$ incognita

Se $\sigma^2$ è incognito va stimato dai dati

Consideriamo lo stimatore $S^2$ di $\sigma^2$
\[S^2=\frac {1}{n-1}\sum_{i=1}^n (X_i-\hat \mu)^2=\frac {n}{n-1}\frac 1 n\sum_{i=1}^n (X_i-\hat \mu)^2=\frac n {n-1}\hat\sigma^2\]

Ricordiamo che
\[\widehat{SE(\hat\mu)}=\sqrt{\frac {S^2}n}=\frac S {\sqrt n}\]

Ricordiamo infine che
\[T=\frac{\hat \mu-\mu}{\widehat{SE(\hat\mu)}}\sim t_{n-1}\]

Ovvero
\[T=\frac{\hat \mu-\mu}{S/\sqrt n}\sim t_{n-1}\]



### $\sigma$ nota e $\sigma$ incognita

Se $\sigma$ è nota
\[Z=\frac{\hat \mu-\mu}{\sigma/\sqrt n}\sim N(0,1)\]

Se $\sigma$ è incognita
\[T=\frac{\hat \mu-\mu}{S/\sqrt n}\sim t_{n-1}\]


Sia $0<\alpha<1$, osserviamo che
\begin{eqnarray*}
  P(-t_{n-1;\alpha/2}<T<+t_{n-1;\alpha/2}) &=& 1-\alpha \\
  P\left(-t_{n-1;\alpha/2}<\frac{\hat \mu -\mu}{\widehat{SE(\hat \mu)}}< +t_{n-1;\alpha/2}\right) &=& 1-\alpha \\
  P\left(\hat \mu- t_{n-1;\alpha/2}~\frac S{\sqrt n}<\mu<\hat \mu+t_{n-1;\alpha/2}~\frac S{\sqrt n}\right) &=& 1-\alpha
\end{eqnarray*}

Dove $t_{n-1;\alpha/2}$ è quel valore tale che $P(T>t_{n-1;\alpha/2})=\alpha/2,\qquad T\sim t_{n-1}$

:::: {.info data-latex=""}
::: {.definition name="Intervallo di Confidenza per $\mu$ ($\sigma^2$ incognita)"}
Si definisce L'IdC al livello $(1-\alpha)\times100\%$ per $\mu$ con $\sigma^2$ incognita, l'intervallo
\[IdC:~~\left[\hat \mu- t_{n-1;\alpha/2}~\frac S{\sqrt n},\hat \mu+ t_{n-1;\alpha/2}~\frac S{\sqrt n}\right]\]
:::
::::

\[  P\left(\hat \mu- t_{n-1;\alpha/2}~\frac S{\sqrt n}<\mu<\hat \mu+t_{n-1;\alpha/2}~\frac S{\sqrt n}\right) = 1-\alpha\]
è la probabilità che **l'intervallo casuale** $\left[\hat \mu- t_{n-1;\alpha/2}~\frac S{\sqrt n},\hat \mu+ t_{n-1;\alpha/2}~\frac S{\sqrt n}\right]$  cada su $\mu$.


:::: {.example}
Un venditore di bustine di tè assicura che ogni bustina ha un
peso medio pari a 20g.
L'acquirente esegue 6 misure di controllo e ottiene i seguenti
risultati: 19; 20; 20.5; 21; 18.5; 15.
La media di questi numeri è $\hat\mu = 19$ e $\widehat{\sigma}=1.979$.
Determinare un IdC al livello di $(1-\alpha)=0.99$ per $\mu$.

\[\alpha=0.01\qquad \alpha/2=0.005,\qquad t_{6-1;0.005}=`r round(qt(1-.005,5),4)`\]


\begin{eqnarray*}
& & S = \sqrt{\frac{n} {n-1}} \widehat{\sigma}
      =  \sqrt{\frac{6} {6-1}} 1.979 = 2.1679                        \\
& & \left[\hat\mu -t_{(n-1); \alpha/2} \frac{S} {\sqrt{n}};
          \hat\mu +t_{(n-1); \alpha/2} \frac{S} {\sqrt{n}} \right] \\
&=& \left[ 19 -4.0321 \frac{2.1679} {\sqrt{6}};  19 +4.0321 \frac{2.1679} {\sqrt{6}} \right]               \\
&=& \left[ 19 - 3.5687;~ 19 + 3.5687 \right]                          \\
&=& \left[ 15.4313; 22.5687 \right]
\end{eqnarray*}

Si noti che la mancanza di informazioni sulla varianza rende più
incerto il risultato; infatti, la lunghezza dell'IdC aumenta.
::::

:::: {.example}
Si sono rilevati i tempi dedicati a ciascun cliente da un impiegato
di banca in 49 casi e si è ottenuta una media $\bar{x}=3.5$ minuti
con una SD pari a 0.5 minuti.
Determinare un IdC al livello di $(1-\alpha)=0.95$ per $\mu$.

\[\alpha=0.05\qquad \alpha/2=0.025.\qquad t_{49-1;0.025}=`r round(qt(1-.025,48),4)`\]

\begin{eqnarray*}
& & s = \sqrt{\frac{n} {n-1}} \widehat{\sigma}
      =  \sqrt{\frac{49} {49-1}} 0.5 = 0.505                         \\
& & \left[\bar{X} -t_{(n-1);  \alpha/2} \frac{S} {\sqrt{n}};
          \bar{X} +t_{(n-1);  \alpha/2} \frac{S} {\sqrt{n}} \right] \\
&=& \left[ 3.5 -2.0106 \frac{0.505} {\sqrt{49}};
           3.5 +2.0106 \frac{0.505} {\sqrt{49}} \right]              \\
&=& \left[ 3.5 - 0.145; 3.5 + 0.145 \right]                          \\
&=& \left[ 3.355; 3.645 \right]
\end{eqnarray*}

Si noti che per $n>120$ si può approssimare con una normale.
::::




## IDC per la proporzione

$X_{1}, \ldots, X_{n}$ VC IID, tutte $\text{Ber}(\pi)$. Per il TLC
\[\bar{X} = \hat\pi \operatorname*{\sim}_a
    N\left( \pi;\, \frac{\pi (1-\pi)} {n} \right) \Rightarrow \hat\pi \operatorname*{\sim}_a N\left( \pi; SE^2(\hat\pi)\right)\]

E quindi
\[Z = \frac{\hat\pi - \pi} {SE(\hat\pi)} \operatorname*{\sim}_a N(0,1) \Rightarrow P\left(-z_{\alpha/2} < \frac{\hat\pi - \pi} {SE(\hat\pi)}   < z_{\alpha/2} \right) = 1-\alpha
    \]


Infine
\begin{eqnarray*}
P\left(\hat\pi -z_{\alpha/2} {SE(\hat\pi)} < \pi
          <\hat\pi +z_{\alpha/2} {SE(\hat\pi)} \right) &=& 1-\alpha \\
P\left(\hat\pi -z_{\alpha/2} \sqrt{\frac{\pi(1-\pi)}{n}} < \pi
          <\hat\pi +z_{\alpha/2} \sqrt{\frac{\pi(1-\pi)}{n}} \right) &=& 1-\alpha
\end{eqnarray*}

l'IdC DIPENDE da $\pi$ NON NOTA.
Per il TLC e $n$ sufficientemente grande si può sostituire
a $\pi$ dell'IdC la sua stima $\hat\pi$.
\[\widehat{SE(\hat\pi)}=\sqrt\frac{\hat\pi(1-\hat\pi)}{n}\]

Condizioni per l'approssimazione
\[n\, \pi \ge 5 \quad\text{e}\quad  n\, (1-\pi) \ge 5\]

e quindi l'IdC al livello $(1-\alpha)\times 100$ per $\hat\pi$ è:

:::: {.info data-latex=""}
::: {.definition name="Intervallo di Confidenza per $\pi$"}
Si definisce L'IdC al livello $(1-\alpha)\times100\%$ per $\pi$ l'intervallo
\[\left[\,\hat\pi-z_{\alpha/2}\sqrt\frac{\hat\pi(1-\hat\pi)}{n};\hat\pi+z_{\alpha/2}\sqrt\frac{\hat\pi(1-\hat\pi)}{n}\,\right]\]
:::
::::



:::: {.example}
Una indagine sulle intenzioni di voto degli italiani per lo schieramento $A$ ha mostrato
che 240 su 500 lo voterebbero. Determinare un IdC al livello di $(1-\alpha)=0.99$ per $\pi$.
\[\alpha=0.01,\qquad \alpha/2=0.005,\qquad z_{0.005}=`r round(qnorm(1-.005),4)`\]

\begin{eqnarray*}
& & \hat\pi = \frac{\mbox{favorevoli}} {n}
                = \frac{240} {500} = 0.48                            \\
& & \left[\hat\pi -z_{\alpha/2}
          \sqrt{\frac{\hat\pi (1-\hat\pi)} {n}};
          \hat\pi +z_{\alpha/2}
          \sqrt{\frac{\hat\pi (1-\hat\pi)} {n}} \right]     \\
&=& \left[  0.48 -2.576 \sqrt{\frac{0.48 (1-0.48)} {500}}; 0.48 +2.576 \sqrt{\frac{0.48 (1-0.48)} {500}} \right]  \\
&=& \left[ 0.48 - `r qnorm(.995)*sqrt(.48*(1-.48)/500)`; 0.48 + `r qnorm(.995)*sqrt(.48*(1-.48)/500)`\right]                     \\
&=& \left[ `r .48-qnorm(.995)*sqrt(.48*(1-.48)/500)`; `r .48+qnorm(.995)*sqrt(.48*(1-.48)/500)` \right]
\end{eqnarray*}
::::

:::: {.example}
Una indagine sulle intenzioni di voto degli italiani per lo schieramento $A$ ha mostrato
che 2400 su 5000 lo voterebbero. Determinare un IdC al livello di $(1-\alpha)=0.99$ per $\pi$.

\begin{eqnarray*}
& & \hat\pi = \frac{\mbox{favorevoli}} {n}
                = \frac{2400} {5000} = 0.48                            \\
& & \left[\hat\pi -z_{\alpha/2}
          \sqrt{\frac{\hat\pi (1-\hat\pi)} {n}};
          \hat\pi +z_{\alpha/2}
          \sqrt{\frac{\hat\pi (1-\hat\pi)} {n}} \right]     \\
&=& \left[  0.48 -2.576 \sqrt{\frac{0.48 (1-0.48)} {5000}}; 0.48 +2.576 \sqrt{\frac{0.48 (1-0.48)} {5000}} \right]  \\
&=& \left[ 0.48 - `r qnorm(.995)*sqrt(.48*(1-.48)/5000)`; 0.48 + `r qnorm(.995)*sqrt(.48*(1-.48)/5000)`\right]                     \\
&=& \left[ `r .48-qnorm(.995)*sqrt(.48*(1-.48)/5000)`; `r .48+qnorm(.995)*sqrt(.48*(1-.48)/5000)` \right]
\end{eqnarray*}
::::

### IdC per $\pi$ per $\alpha$ ed $n$ fissati

`r n <- 50`Se fissiamo $\alpha$ ed $n$, per esempio $\alpha=0.05$ ed $n=`r n`$, possiamo variare $S_n\in\{0,...,n\}$ e quindi $\hat\pi\in{0/n,1/n,...,n/n}$. Per ogni valore di $\hat\pi$ calcoliamo l'IdC al livello $(1-\alpha)$ e rappresentiamo graficamente

```{r 13-stima-intervallare-17}
fig.def(3,3)
```

```{r 13-stima-intervallare-18}

op <- par(cex=.5,mar=c(5.1,5.1,.1,.1))

ap <- 0
plot(c(0,1),c(0,1),axes=F,xlab="",ylab="",type="n",asp=1,col="grey")
curve(x-1.96*sqrt(x*(1-x)/n),ap,1-ap,add=T,col="grey")
curve(x+1.96*sqrt(x*(1-x)/n),ap,1-ap,add=T,col="grey")
curve(x-1.96*sqrt(x*(1-x)/n),5/n,1-5/n,add=T,col=iblue,lwd=1.5)
curve(x+1.96*sqrt(x*(1-x)/n),5/n,1-5/n,add=T,col=iblue,lwd=1.5)


title(xlab=expression(hat(pi)),mgp=c(4,4,0))
title(ylab=expression(pi),mgp=c(4,4,0))

segments(x0 = .5,y0 = .5-1.96*sqrt(.25/n),x1 = 0,y1 = .5-1.96*sqrt(.25/n),lty=2,col=iblue)
segments(x0 = .5,y0 = .5+1.96*sqrt(.25/n),x1 = 0,y1 = .5+1.96*sqrt(.25/n),lty=2,col=iblue)

segments(x0 = .1,y0 = .1-1.96*sqrt(.09/n),x1 = 0,y1 = .1-1.96*sqrt(.09/n),lty=2,col=iblue)
segments(x0 = .1,y0 = .1+1.96*sqrt(.09/n),x1 = 0,y1 = .1+1.96*sqrt(.09/n),lty=2,col=iblue)


axis(1,(0:20)/20,las=2)
axis(2,round(c(.5-1.96*sqrt(.25/n),.5+1.96*sqrt(.25/n),.1+1.96*sqrt(.09/n),.1-1.96*sqrt(.09/n),.5,.1),4),las=2,mgp = c(100,1,0))
abline(0,1,lty=2)
abline(h=.5,lty=2,col=ared)
abline(v=.5,lty=2,col=ared)
text(.51,.01,"per pi = 0.5 si ottiene l'intervallo più ampio",pos=4)
par(op)
```

## Specchietto Finale per gli IdC

::: {.info2 data-latex=""}

- Si definisce L'IdC al livello $(1-\alpha)\times100\%$ per $\mu$ con $\sigma^2$ nota, l'intervallo
\[IdC:~~\left[\hat \mu- z_{\alpha/2}~\frac\sigma{\sqrt n},\hat \mu+ z_{\alpha/2}~\frac\sigma{\sqrt n}\right]\]
- Si definisce L'IdC al livello $(1-\alpha)\times100\%$ per $\mu$ con $\sigma^2$ incognita, l'intervallo
\[IdC:~~\left[\hat \mu- t_{n-1;\alpha/2}~\frac S{\sqrt n},\hat \mu+ t_{n-1;\alpha/2}~\frac S{\sqrt n}\right]\]
- Si definisce L'IdC al livello $(1-\alpha)\times100\%$ per $\pi$ l'intervallo
\[\left[\,\hat\pi-z_{\alpha/2}\sqrt\frac{\hat\pi(1-\hat\pi)}{n};\hat\pi+z_{\alpha/2}\sqrt\frac{\hat\pi(1-\hat\pi)}{n}\,\right]\]
:::

<!--chapter:end:13-stima-intervallare.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup14, include=FALSE}
rm(list=ls())


source("intro.R")
```


# Teoria dei test

## Le Ipotesi

Siano $X_1,...,X_n$ $n$ VC, replicazioni di $X\sim\mathscr{L}(\theta)$

Un _Test Statistico_ è la scelta tra due ipotesi diverse su $\theta$ alla luce dei dati che osserveremo:
\[\begin{cases}
H_0:\theta\in\Theta_0, \qquad \Theta_0\subset\Theta\\
H_1:\theta\in\Theta_1, \qquad \Theta_1\subset\Theta\\
\end{cases}
\]

  - Se $\Theta_0=\{\theta_0\}$ è un solo punto si dice che $H_0$ è un'ipotesi **semplice**, altrimenti è **composta**
  - Se $\Theta_1=\{\theta_1\}$ è un solo punto si dice che $H_1$ è un'ipotesi **semplice**, altrimenti è **composta**  



### Esempi di ipotesi

Esempio: siamo indecisi se l'urna da cui stiamo per estrarre le palline abbia l'80% di palline vincenti oppure il 40%:
\[\begin{cases}
H_0:\pi=0.8, \\
H_1:\pi=0.4
\end{cases}
\]

In questo caso, $H_0$ è un'ipotesi semplice (specifica solo un punto) e un'ipotesi $H_1$ è semplice 
serve solo per comprendere la teoria. i primi esempi saranno svolti con due ipotesi
semplici.

Esempio: siamo indecisi se il reddito medio degli italiani, di cui stiamo per estrarre un campione, sia uguale a 18 mila € annui oppure minore di 18 mila € annui:
\[\begin{cases}
H_0:\mu=18, \\
H_1:\mu< 18
\end{cases}
\]

In questo caso, $H_0$ è un'ipotesi semplice (specifica solo un punto) e $H_1$ è composta 
(specifica un'intera regione di $\Theta$) è il caso più interessante nella pratica
ed  è il caso che svilupperemo maggiormente.

Esempio: siamo indecisi se la SD del reddito degli italiani sia maggiore o minore di 2 mila € annui:
\[\begin{cases}
H_0:\sigma\geq 2, \\
H_1:\sigma<2
\end{cases}
\]

In questo caso, $H_0$ è un'ipotesi composta e $H_1$ è composta è un caso meno interessante 
e non svilupperemo

## La Decisione

Preparare un test statistico significa dividere lo spazio dei campioni in due
\[\mathcal{S}=\mathcal{S}_0\cup\mathcal{S}_1,\qquad \mathcal{S}_0\cap\mathcal{S}_1=\emptyset
\]

Il test è una **decisione**: se il campione proverrà da $\mathcal{S}_0$ il test deciderà per $H_0$, se il campione proverrà da $\mathcal{S}_1$ il test deciderà per $H_1$.

Ogni decisione ha delle conseguenze

  - Se $H_0$ è vera ma il campione cade in $\mathcal{S}_1$ sceglierò $H_1$ erroneamente
  - Se $H_1$ è vera ma il campione cade in $\mathcal{S}_0$ sceglierò $H_0$ erroneamente

## La tavola della verità


La _tavola della verità_ è una tabella simbolica in cui sulle righe viene scritto il _vero stato di natura_ cioè, se $H_0$ è vera o falsa. Per colonna viene scritta la decisione, cioè
se scelgo di tenere $H_0$ o di rifiutarla in favore di $H_1$.

:::: {.info data-latex=""}
::: {.center data-latex=""}
```{r 14-test-intro-1, results='asis'}

t.ver <- data.frame(s.n.="stato di natura",pi=c("$H_0$","$H_1$"),matrix(c("Corretta","Errore II tipo","Errore I tipo","Corretta"),2))
kable(t.ver,row.names = F,col.names = c("","","decido $H_0$","decido $H_1$"),align = 'c',
      booktabs = T, escape = F,linesep = "", digits = 4) %>%
  column_spec(1, bold = T) %>%
  column_spec(3:4, width = "10em") %>%
  collapse_rows(columns = 1, valign = "middle") %>%
   add_header_above(c(" ", "","Decisione" = 2))

mumax <- 5
za2 <- round(qnorm(.975),2)
n <- 10

```
:::
::::

Dunque:

:::: {.info data-latex=""}
::: {.definition name="Errori di primo e secondo tipo"}

Si definiscono

- L'**errore di primo tipo** è l'errore che si commette scegliendo $H_1$ quando è vera $H_0$.
- L'**errore di secondo tipo** è l'errore che si commette scegliendo $H_0$ quando è vera $H_1$.

:::
::::

Dunque ad ogni decisione corrisponde un possibile errore. 
Per valutare un test si devono calcolare le probabilità di errore

::: {.center data-latex=""}
```{r 14-test-intro-2, results='asis'}

t.ver <- data.frame(s.n.="stato di natura",pi=c("$H_0$","$H_1$"),matrix(c("$1-\\alpha$","$\\beta$","$\\alpha$","$1-\\beta$"),2))
kable(t.ver,row.names = F,col.names = c("","","decido $H_0$","decido $H_1$"),align = 'c',
      booktabs = T, escape = F,linesep = "", digits = 4) %>%
  column_spec(1, bold = T) %>%
  column_spec(3:4, width = "10em") %>%
  collapse_rows(columns = 1, valign = "middle") %>%
   add_header_above(c(" ", "","Decisione" = 2))

```
:::

Dove

:::: {.info data-latex=""}
::: {.definition name="Probabilità degli Errori di primo e secondo tipo"}

\[\alpha=P(\text{Errore I tipo})=P(\text{Decidere $H_1$};H_0)=P(X_1,...,X_n\in\mathcal{S}_1;H_0)\]

\[\beta=P(\text{Errore II tipo})=P(\text{Decidere $H_0$};H_1)=P(X_1,...,X_n\in\mathcal{S}_0;H_1)\]

$\alpha$ è il livello di **significatività** del test, $\alpha$ è la probabilità di scegliere $H_1$ quando invece è vera $H_0$. 
$\beta$ è la probabilità di scegliere $H_0$ quando invece è vera $H_1$. 

:::
::::

Infine

:::: {.info data-latex=""}
::: {.definition name="Potenza di un Test"}
\[1-\beta =P(\text{Decidere $H_1$}; H_1)=P(X_1,...,X_n\in\mathcal{S}_1;H_1)\]
$1-\beta$ è la **potenza del test**, $1-\beta$ è la probabilità di scegliere $H_1$ quando $H_1$ è vera.
:::
::::


<!-- ## Esempio: COVID-19 e Tamponi -->

<!-- - Il test della coltura attraverso il _tampone_ **non** è un _test certo_ -->
<!-- \begin{align} -->
<!-- P(\text{Falso Positivo}) =& P(\text{Tampone Positivo} ;& \text{Individuo non è infetto})&=0.02 = \alpha \\ -->
<!-- P(\text{Falso Negativo}) =& P(\text{Tampone Negativo} ;& \text{Individuo è infetto})&=0.30 = \beta \\ -->
<!-- \end{align} -->

<!-- - La tavola della verità -->
<!-- ```{r 14-test-intro-3, results='asis'} -->
<!-- alpha <- .02 -->
<!-- bbeta <- .3 -->
<!-- t.ver <- data.frame(s.n.="stato di natura",pi=c("Non Infetto","Infetto"),matrix(c(1-alpha,bbeta,alpha,1-bbeta),2)) -->
<!-- kable(t.ver,row.names = F,col.names = c("","","Negativo","Positivo"),align = 'c') %>% -->
<!-- kable_styling(booktabs = T, escape = F,linesep = "", digits = 4) %>% -->
<!--   column_spec(1, bold = T) %>% -->
<!--   column_spec(3:4, width = "10em") %>% -->
<!--   collapse_rows(columns = 1, valign = "middle") %>% -->
<!--    add_header_above(c(" ", "","Tampone" = 2)) -->
<!-- ``` -->

<!-- - Gli **errori del primo tipo** sono i **Falsi Positivi**,  -->

<!-- - Gli **errori del secondo tipo** sono i **Falsi Negativi**,  -->

<!-- - La **significatività** del test è -->
<!-- \[\alpha=0.02\Rightarrow 2\%\] -->

<!-- - La **potenza** del test è -->
<!-- \[1-\beta=0.70\Rightarrow 70\%\] -->

<!-- - Il test del _tampone_ non è un test statistico, non possiamo agire sulla scelta di $\alpha$ né di $\beta$. -->


:::: {.example name="Spam e Filtri email"}
Il Un filtro delle email che separa lo _spam_ dal _non spam_ **non** è una _procedura certa_
\begin{align*}
P(\text{Falso Positivo}) =& P(\text{filtrata spam} ; \text{non è spam})= \alpha \\
P(\text{Falso Negativo}) =& P(\text{filtrata non spam} ; \text{è spam})= \beta \\
\end{align*}

La tavola della verità

::: {.center data-latex=""}
```{r 14-test-intro-4, results='asis'}
alpha <- .02
bbeta <- .3
t.ver <- data.frame(s.n.="stato di natura",pi=c("Non Spam","Spam"),matrix(c("$1-\\alpha$","$\\beta$","$\\alpha$","$1-\\beta$"),2))
kable(t.ver,row.names = F,col.names = c("","","Non Spam","Spam"),align = 'c',
      booktabs = T, escape = F,linesep = "", digits = 4) %>%
  column_spec(1, bold = T) %>%
  column_spec(3:4, width = "10em") %>%
  collapse_rows(columns = 1, valign = "middle") %>%
   add_header_above(c(" ", "","Filtro" = 2))
```
:::

Obiettivo: costruire il filtro in modo tale che
$\alpha$ sia fissato ad un valore arbitrariamente piccolo
$1-\beta$ sia la più alta possibile, per $\alpha$ fissato
::::


```{r 14-test-intro-5,echo=FALSE,include=FALSE}
############# Esempio dell'Orsi


crop <- function(im, left = 0, top = 0, right = 0, bottom = 0) {
  d <- dim(im[[1]]); w <- d[2]; h <- d[3]
  image_crop(im, glue::glue("{w-left-right}x{h-top-bottom}+{left}+{top}"))
}


n <- 10
x  <- 0:n
p0 <- .8
p1 <- .4
f0 <- dbinom(x,n,p0)
f1 <- dbinom(x,n,p1)
ff <- round(cbind(f0,f1),4)
colnames(ff) <- c("$H_0:\\pi=0.8$","$H_1:\\pi=0.4$")
row.names(ff) <- paste("$P(S_{10}=",0:10,")$",sep = '')

RA <- x < .8*n
RB <- x < .6*n
RC <- x < .2*n | x > .8*n

alphaA <- sum(f0[RA])
alphaB <- sum(f0[RB])
alphaC <- sum(f0[RC])

betaA <- sum(f1[!RA])
betaB <- sum(f1[!RB])
betaC <- sum(f1[!RC])

tvA <-data.frame(s.n.="stato di natura",pi=c("$\\pi=0.8$","$\\pi=0.4$"),round(matrix(c(1-alphaA,betaA,alphaA,1-betaA),2),4))
tvB <-data.frame(s.n.="stato di natura",pi=c("$\\pi=0.8$","$\\pi=0.4$"),round(matrix(c(1-alphaB,betaB,alphaB,1-betaB),2),4))
tvC <-data.frame(s.n.="stato di natura",pi=c("$\\pi=0.8$","$\\pi=0.4$"),round(matrix(c(1-alphaC,betaC,alphaC,1-betaC),2),4))

# 
# png("img/deca.png",width = 1024, height = 480)
# plot(c(0,10),c(-1,1),axes=F,xlab="",ylab="",type="n")
# abline(0,0)
# points(0:10,(0:10)*0,pch="|")
# points(8,0,pch="|",col=ared)
# text(0:10,(0:10)*0-.15,0:10)
# text(8,-.15,8,col=ared)
# text(8,-.3,"Valore  critico",col=ared)
# brackets(0,.1,7,.1,h = .1)
# text(3.5,.5,expression(S[1]),cex=2)
# brackets(8,.1,10,.1,h=.1)
# text(9,.5,expression(S[0]),cex=2)
# dev.off()
# 
# 
# png("img/decb.png",width = 1024, height = 480)
# plot(c(0,10),c(-1,1),axes=F,xlab="",ylab="",type="n")
# abline(0,0)
# points(0:10,(0:10)*0,pch="|")
# points(6,0,pch="|",col=ared)
# text(0:10,(0:10)*0-.15,0:10)
# text(6,-.15,8,col=ared)
# text(6,-.3,"Valore  critico",col=ared)
# brackets(0,.1,5,.1,h = .1)
# text(2.5,.5,expression(S[1]),cex=2)
# brackets(6,.1,10,.1,h=.1)
# text(8,.5,expression(S[0]),cex=2)
# dev.off()
# 
# png("img/decc.png",width = 1024, height = 480)
# plot(c(0,10),c(-1,1),axes=F,xlab="",ylab="",type="n")
# abline(0,0)
# points(0:10,(0:10)*0,pch="|")
# points(2,0,pch="|",col=ared)
# points(8,0,pch="|",col=ared)
# text(0:10,(0:10)*0-.15,0:10)
# text(2,-.15,2,col=ared)
# text(8,-.15,8,col=ared)
# text(2,-.3,"Valore  critico inferiore",col=ared)
# text(8,-.3,"Valore  critico superiore",col=ared)
# brackets(2,.1,8,.1,h = .1)
# brackets(0,.1,1,.1,h = .1)
# brackets(9,.1,10,.1,h = .1)
# text(.5,.5,expression(S[1]),cex=2)
# text(9.5,.5,expression(S[1]),cex=2)
# text(5,.5,expression(S[0]),cex=2)
# dev.off()



```


## Esempio: Scegliere tra due ipotesi semplici


Esempio: siamo indecisi se l'urna da cui stiamo per estrarre le palline abbia l'80% di palline vincenti oppure il 40%:
\[\begin{cases}
H_0:\pi=0.8, \\
H_1:\pi=0.4
\end{cases}
\]

Decidiamo di estrarre 10  palline CR (IID), $X_1,...,X_{10}$ per decidere tra $H_0$ e $H_1$.
Prima di estrarre le palline possiamo calcolare la probabilità di tutti i possibili campioni _sotto ipotesi $H_0$_ e _sotto ipotesi $H_1$_. La somma
\[S_{10}=X_1+...+X_{10}\]
descrive tutti i possibili campioni Bernoulli IID di ampiezza $n=10$, $S_{10}\sim\text{Binom}(\pi)$ e quindi
\[\mathcal{S}=\{0,~1,~2,...,10\}\]
e
\[P(S_{10}=s;\pi)=\binom{10}{s}\pi^s(1-\pi)^{n-s},\qquad \pi\in\{0.8,0.4\}\]

Possiamo quindi calcolare

\scriptsize
```{r 14-test-intro-6, results='asis'}
gg <- cbind(0:10,format(ff,digits = 4))
colnames(gg)<-c("$S_n$","$P(S_n;\\pi=0.8)$","$P(S_n;\\pi=0.4)$")
rownames(gg)<-NULL
kable(t(gg),booktabs = T, escape = F,linesep = "", digits = 4,col.names = NA) %>%
  kable_styling(font_size = 10)
```
\normalsize

### Tre diversi Test a confronto

Sia dato il sistema di ipotesi:
\[\begin{cases}
H_0:\pi=0.8, \\
H_1:\pi=0.4
\end{cases}
\]

Un test è una divisione dello spazio dei campioni, e nel modello Bernoulli IID i campioni di ampiezza $n=10$ sono riassunti dalla loro somma $S_{10}$

__Decisione A__ Se $S_{10}/10\geq 0.8\Rightarrow S_{10}\geq 8$ allora scelgo $H_0$; se $S_{10}/10 <   0.8\Rightarrow S_{10}<    8$ allora scelgo $H_1$
  
```{r 14-test-intro-16, fig.height=1.5}
par(mar=c(0,3,0,3))
plot(c(0,10),c(-.5,.7),axes=F,xlab="",ylab="",type="n")
abline(0,0)
points(0:10,(0:10)*0,pch="|")
points(8,0,pch="|",col=ared)
text(0:10,(0:10)*0-.15,0:10)
text(8,-.15,8,col=ared)
text(8,-.3,"Valore critico",col=ared)
brackets(0,.1,7,.1,h = .1)
text(3.5,.5,expression(S[1]),cex=2)
brackets(8,.1,10,.1,h=.1)
text(9,.5,expression(S[0]),cex=2)
```

__Decisione B__ Se $S_{10}/10\geq 0.6\Rightarrow S_{10}\geq 6$ allora scelgo $H_0$; se $S_{10}/10 <   0.6\Rightarrow S_{10}<    6$ allora scelgo $H_1$
  
```{r 14-test-intro-17, fig.height=1.5}
plot(c(0,10),c(-1,1),axes=F,xlab="",ylab="",type="n")
abline(0,0)
points(0:10,(0:10)*0,pch="|")
points(6,0,pch="|",col=ared)
text(0:10,(0:10)*0-.15,0:10)
text(6,-.15,8,col=ared)
text(6,-.3,"Valore  critico",col=ared)
brackets(0,.1,5,.1,h = .1)
text(2.5,.5,expression(S[1]),cex=2)
brackets(6,.1,10,.1,h=.1)
text(8,.5,expression(S[0]),cex=2)
```
  
__Decisione C__ Se $0.2\leq S_{10}/10\leq 0.8\Rightarrow 2\leq S_{10}\leq 8$ allora scelgo $H_0$; se $S_{10}/10 <   0.2$ oppure $S_{10}>0.8$, $\Rightarrow S_{10}<  2$ oppure $S_{10}>8$ allora scelgo $H_1$
  
```{r 14-test-intro-18, fig.height=2}
plot(c(0,10),c(-1,1),axes=F,xlab="",ylab="",type="n")
abline(0,0)
points(0:10,(0:10)*0,pch="|")
points(2,0,pch="|",col=ared)
points(8,0,pch="|",col=ared)
text(0:10,(0:10)*0-.15,0:10)
text(2,-.15,2,col=ared)
text(8,-.15,8,col=ared)
text(2,-.3,"Valore  critico inferiore",col=ared)
text(8,-.3,"Valore  critico superiore",col=ared)
brackets(2,.1,8,.1,h = .1)
brackets(0,.1,1,.1,h = .1)
brackets(9,.1,10,.1,h = .1)
text(.5,.5,expression(S[1]),cex=2)
text(9.5,.5,expression(S[1]),cex=2)
text(5,.5,expression(S[0]),cex=2)
```

### Gli errori della decisione A

Siccome sappiamo calcolare la distribuzione di $S_{10}$ sotto $H_0$ e sotto $H_1$:

\scriptsize
```{r 14-test-intro-7, results='asis'}
kable(t(gg),booktabs = T, escape = F,linesep = "", digits = 4,col.names = NA)%>%
  column_spec(10:12, color =  iblue) %>%
  column_spec(2:9, color =  ared) %>%
  kable_styling(font_size = 10)

```
\normalsize


Decisione A: $\mathcal{S}_0=\{S_{10}\geq 8\}$ e $\mathcal{S}_1=\{S_{10}< 8\}$

\begin{eqnarray*}
  \alpha &=& P(\mathcal{S}_{1};H_0)=P(S_{10}=0~\cup S_{10}=1~\cup...\cup~ S_{10}=7)\\
  &=&`r paste(round(f0[1:8],3),collapse ='+')`=`r sum(f0[1:8])`\\
  \beta &=& P(\mathcal{S}_{0};H_1)=P(S_{10}=8~\cup S_{10}=9~\cup~ S_{10}=10)\\
  &=&`r paste(round(f1[9:11],3),collapse ='+')`=`r sum(f1[9:11])`
\end{eqnarray*}
  
La significatività del test A è $\alpha=`r tvA[1,4]`$.
La potenza del test A è $1-\beta=`r tvA[2,4]`$

::: {.center data-latex=""}
```{r 14-test-intro-8, results='asis'}
kable(tvA,row.names = F,col.names = c("","","$H_0$","$H_1$"),
      booktabs = T, escape = F,linesep = "", digits = 4) %>%
  column_spec(1, bold = T) %>%
  column_spec(3:4, width = "10em") %>%
  collapse_rows(columns = 1, valign = "middle") %>%
   add_header_above(c(" ", "","Decisione A" = 2))
```
:::

### Gli errori della decisione B

Siccome sappiamo calcolare la distribuzione di $S_{10}$ sotto $H_0$ e sotto $H_1$:
\scriptsize
```{r 14-test-intro-9, results='asis'}
kable(t(gg),booktabs = T, escape = F,linesep = "", digits = 4,col.names = NA)%>%
  column_spec(8:12, color =  iblue) %>%
  column_spec(2:7, color =  ared) %>%
  kable_styling(font_size = 10)
```
\normalsize


Decisione B: $\mathcal{S}_0=\{S_{10}\geq 6\}$ e $\mathcal{S}_1=\{S_{10}< 6\}$
\begin{eqnarray*}
\alpha  &=&P(\mathcal{S}_{1};H_0)=P(S_{10}=0~\cup S_{10}=1~\cup...\cup~ S_{10}=5)\\
        &=&`r paste(round(f0[1:6],3),collapse ='+')`=`r sum(f0[1:6])`\\
\beta   &=& P(\mathcal{S}_{0};H_1)=P(S_{10}=7~\cup ...\cup~ S_{10}=10)\\
        &=& `r paste(round(f1[7:11],3),collapse ='+')`=`r sum(f1[7:11])`
\end{eqnarray*}

La significatività del test B è $\alpha=`r tvB[1,4]`$.
La potenza del test B è $1-\beta=`r tvB[2,4]`$

::: {.center data-latex=""}
```{r 14-test-intro-10, results='asis'}
kable(tvB,row.names = F,col.names = c("","","$H_0$","$H_1$"),
      booktabs = T, escape = F,linesep = "", digits = 4) %>%
  column_spec(1, bold = T) %>%
  column_spec(3:4, width = "10em") %>%
  collapse_rows(columns = 1, valign = "middle") %>%
   add_header_above(c(" ", "","Decisione B" = 2))
```
:::


### Gli errori della decisione C

Siccome sappiamo calcolare la distribuzione di $S_{10}$ sotto $H_0$ e sotto $H_1$:

\scriptsize
```{r 14-test-intro-11, results='asis'}
kable(t(gg),booktabs = T, escape = F,linesep = "", digits = 4,col.names = NA)%>%
  column_spec(4:10, color =  iblue) %>%
  column_spec(c(2:3,11:12), color =  ared) %>%
  kable_styling(font_size = 10)
```
\normalsize

Decisione C: $\mathcal{S}_0=\{2\leq S_{10}\leq 8\}$ e $\mathcal{S}_1=\{S_{10}< 2\}\cup\{S_{10}>8\}$
\begin{eqnarray*}
\small\alpha &=&P(\mathcal{S}_{1};H_0)=P(S_{10}= 0~\cup S_{10}=1~\cup~ S_{10}=9~\cup S_{10}=10)\\
                    &=&`r paste(round(f0[c(1,2,10,11)],3),collapse ='+')`=`r sum(f0[c(1,2,10,11)])`\\
 \beta       &=& P(\mathcal{S}_{0};H_1)=P(S_{10}=2~\cup ...\cup~ S_{10}=8)\\
             &=&`r paste(round(f1[3:9],3),collapse ='+')`=`r sum(f1[3:9])`
\end{eqnarray*}

La significatività del test C è $\alpha=`r tvC[1,4]`$,
la potenza del test C è $1-\beta=`r tvC[2,4]`$

::: {.center data-latex=""}
```{r 14-test-intro-12, results='asis'}
kable(tvC,row.names = F,col.names = c("","","$H_0$","$H_1$"),
      booktabs = T, escape = F,linesep = "", digits = 4) %>%
  column_spec(1, bold = T) %>%
  column_spec(3:4, width = "10em") %>%
  collapse_rows(columns = 1, valign = "middle") %>%
   add_header_above(c(" ", "","Decisione C" = 2))
```
:::

### Confronto

::: {.center data-latex=""}
```{r 14-test-intro-13, results='asis'}
kable(tvA,row.names = F,col.names = c("","","$H_0$","$H_1$"),
      booktabs = T, escape = F,linesep = "", digits = 4) %>%
  column_spec(1, bold = T) %>%
  column_spec(3:4, width = "10em") %>%
  collapse_rows(columns = 1, valign = "middle") %>%
   add_header_above(c(" ", "","Decisione A" = 2))
```
:::

::: {.center data-latex=""}
```{r 14-test-intro-14, results='asis'}
kable(tvB,row.names = F,col.names = c("","","$H_0$","$H_1$"),
      booktabs = T, escape = F,linesep = "", digits = 4) %>%
  column_spec(1, bold = T) %>%
  column_spec(3:4, width = "10em") %>%
  collapse_rows(columns = 1, valign = "middle") %>%
   add_header_above(c(" ", "","Decisione B" = 2))
```
:::

::: {.center data-latex=""}
```{r 14-test-intro-15, results='asis'}
kable(tvC,row.names = F,col.names = c("","","$H_0$","$H_1$"),
      booktabs = T, escape = F,linesep = "", digits = 4) %>%
  column_spec(1, bold = T) %>%
  column_spec(3:4, width = "10em") %>%
  collapse_rows(columns = 1, valign = "middle") %>%
   add_header_above(c(" ", "","Decisione C" = 2))
```
:::

## Ipotesi Nulla e Ipotesi Alternativa

Nell'approccio _decisionista_ la statistica è di aiuto al decision manager: la decisione tra $H_0$ e $H_1$ deve tenero conto dei _costi_ che una decisione sbagliata comporta e pesare i _costi_ con la probabilità di sbagliare per ottenere una valutazione di _rischio_. Interessante approccio ma non verrà seguito in questo corso.

Nell'approccio _falsificazionista_ (<https://it.wikipedia.org/wiki/Principio_di_falsificabilit%C3%A0>), la statistica è di aiuto alla conoscenza del mondo esterno e delle sue _leggi_. Seguiremo questo approccio.
In molte discipline $H_0$ viene chiamata **Ipotesi Nulla** e ha una particolare interpretazione:
$H_0$ è lo stato di conoscenza pregresso. 
Rappresenta l'ipotesi che i dati non abbiano aggiunto niente di diverso da ciò che già conoscevamo.
Sotto $H_0$ asserisce che la **differenza** tra l'evidenza del campione e l'ipotesi nulla è solo **dovuta al caso**.
$H_1$ è chiamata **ipotesi alternativa**, sotto $H_1$ la **differenza** tra l'evidenza dei dati e $H_0$ **non è dovuta al caso** ma ad un _fattore sistematico_.
La maggior parte delle volte l'obiettivo della ricerca si conclude positivamente se $H_0$ viene rifiutata, e dunque i dati presentati nella ricerca hanno aggiunto _conoscenza_.



## Rifiutare o non rifiutare $H_0$

Quando si prepara test, spesso, si vuole mostrare che i dati campionari smentiscono alcune ipotesi pregresse. 
$H_0$ viene abbandonata (**rifiutata**,**confutata**) solo se c'è una forte evidenza campionaria contro

  - Se $H_0$ **non** viene rifiutata allora la differenza tra i dati e l'ipotesi nulla è considerata non significativa
  - Se $H_0$ viene rifiutata allora la differenza tra i dati e l'ipotesi è considerata significativa

Rifiutare $H_0$ **non** significa accettare $H_1$, ma sostituire una ipotesi pregressa $H_0$ con una nuova supportata dai dati, $H_1$. $H_1$ diventa la nuova $H_0$ e sarà considerata lo stato di conoscenza fino a quando non si troveranno nuovi dati che la **confutano**.
Per preservare $H_0$ scegliamo una probabilità di significatività bassa
  \[\alpha=\{0.05,~0.01,~0.005,~0.001\}\]

  - $\alpha=0.05$ produce un test al 5%
  - $\alpha=0.01$ produce un test all'1%
  - ecc.

L'obiettivo è trovare, tra tutte i test a livello $\alpha$ fissato, quello con potenza più alta. 
Non sempre il test più potente esiste. 
Non tratteremo il problema del test più potente in maniera sistematica.

## Test per $\mu$: due ipotesi semplici, $\sigma^2$ nota

```{r 14-test-intro-19}
n <- 10
mu1 <- 21
mu2 <- 24
sig <- 3.5
si2 <- sig^2
se <- sig/sqrt(n)
```

Stiamo per estrarre $n=10$ VC IID, $X_1,...,X_{10}$ da una normale $X\sim N(\mu,\sigma^2)$, di cui conosciamo $\sigma^2=`r sig`^2$.
Per esempio siamo indecisi tra:
\[\begin{cases}
H_0:\mu=\mu_0=`r mu1` \\
H_1:\mu=\mu_1=`r mu2`
\end{cases}
\]

Siccome $S_{10}=X_1+...+X_{10}$ racchiude tutta l'informazione sulla media $\mu$ che possiamo estrarre da un campione di normali IID con varianza nota, possiamo ragionare su $\hat\mu=S_{10}/10$ invece che su tutto lo spazio dei campioni $\mathcal{S}$.
\[\hat\mu=\frac 1 n \sum_{i=1}^n X_i\sim N\left(\mu,\frac{\sigma^2}{n}=`r se`^2\right)\]

  - Sotto $H_0$
  \[\hat\mu\sim N(`r mu1`,`r se`^2)\]
  - Sotto $H_1$
  \[\hat\mu\sim N(`r mu2`,`r se`^2)\]


### Test per $\mu$: scegliere il punto critico

Una decisione consiste nello scegliere il punto critico sullo spazio delle medie:

```{r 14-test-intro-20}
par(mar=c(5, 4, 4, 2) + 0.1)
pc <- qnorm(.975,mu1,se)
curve(dnorm(x,mu1,se),21-4.1,21+7.1,axes=F,xlab=expression(bar(X)),ylab=expression(f(bar(X))),col=iblue)
curve(dnorm(x,mu2,se),21-4.1,21+7.1,add=T,col=ared)
axis(1,c(mu1,mu2,21-4,21+7))
axis(2)
axis(1,qnorm(.975,mu1,se),"Punto \n critico",col=ared,las=2,t)

# curve(dnorm(x,mu1,se),qnorm(.975,mu1,se),21+7,type="h",col=3,add=T,n=1001)
# xp <- c(seq(17,qnorm(.975,mu1,se),length.out = 101),qnorm(.975,mu1,se))
# yp <- c(dnorm(xp,mu2,se)[-101],0)
# polygon(xp,yp,density = 10,col=iblue)
# text(21,0.15,expression(beta),col=iblue,cex=2)
# text(24,0.15,expression(alpha),col=3,cex=2)

brackets(17,.01,pc-.05,.01,h = .01,col = iblue)
brackets(pc+.05,.01,28,.01,h = .01,col=ared)
text((17+pc)/2,.08,expression(S[0]),cex=2,col=iblue)
text((28+pc)/2,.08,expression(S[1]),cex=2,col=ared)
text(18,.1,expression(f(bar(X),mu[0])),col=iblue)
text(27,.1,expression(f(bar(X),mu[1])),col=ared)
lines(c(pc,pc),c(0,dnorm(pc,mu2,se)),lty=2)
```

Potremmo prendere, per esempio, come punto critico $\bar x=23$: se la media dei dati
è minore di 23 sceglierò $H_0$ altrimenti sceglierò $H_1$.

### Probabilità di errore di primo e di secondo tipo

Fissato il punto critico, per esempio 23, osserviamo

\begin{eqnarray*}
  \alpha &=&  P(\text{Errore I Tipo})\\
  &=& P(\text{Scegliere $H_1$ quando è vera $H_0$})\\
  &=& P(\bar X>23;H_0)\qquad\text{sotto ipotesi $H_0$: $\mu=`r mu1`$}\\
  &=& 1-P(\bar X \le 23;H_0)\\
  &=& 1-P\left(\frac{\bar X-\mu_0}{\sigma/\sqrt{n}}\le \frac{23-`r mu1`}{`r sig`/\sqrt{`r n`}}\right)\\
  &=& 1-\Phi(`r (23-mu1)/se`)\\
  &=& `r 1-pnorm((23-mu1)/se)`\\
  \beta &=&  P(\text{Errore II Tipo})\\
  &=& P(\text{Scegliere $H_0$ quando è vera $H_1$})\\
  &=& P(\bar X<23;H_1)\qquad\text{sotto ipotesi $H_1$: $\mu=`r mu2`$}\\
  &=& P(\bar X \le 23;H_1)\\
  &=& P\left(\frac{\bar X-\mu_1}{\sigma/\sqrt{n}}\le \frac{23-`r mu2`}{`r sig`/\sqrt{`r n`}}\right)\\
  &=& \Phi(`r (23-mu2)/se`)\\
  &=& `r pnorm((23-mu2)/se)`
\end{eqnarray*}

Graficamente osserviamo.  

```{r 14-test-intro-21}
pc <- qnorm(.975,mu1,se)
curve(dnorm(x,mu1,se),21-4.1,21+7.1,axes=F,xlab=expression(bar(X)),ylab="",col=iblue)
curve(dnorm(x,mu2,se),21-4,21+7,add=T,col=ared)
axis(1,c(mu1,mu2,21-4,21+7))
#axis(2)
axis(1,qnorm(.975,mu1,se),"Punto \n critico",col=ared,las=2,t)

curve(dnorm(x,mu1,se),qnorm(.975,mu1,se),21+7,type="h",col=iblue,add=T,n=1001)
xp <- c(seq(17,qnorm(.975,mu1,se),length.out = 101),qnorm(.975,mu1,se))
yp <- c(dnorm(xp,mu2,se)[-101],0)
polygon(xp,yp,density = 25,col=ared)
text(21,0.05,expression(beta),col=ared,cex=2)
text(24,0.05,expression(alpha),cex=2,col=iblue)

# brackets(17,.01,pc-.05,.01,h = .01)
# brackets(pc+.05,.01,28,.01,h = .01)
# text((17+pc)/2,.05,expression(S[0]),cex=2)
# text((28+pc)/2,.05,expression(S[1]),cex=2)
text(18,.1,expression(f(bar(X),mu[0])),col=iblue)
text(27,.1,expression(f(bar(X),mu[1])),col=ared)
lines(c(pc,pc),c(0,dnorm(pc,mu2,se)),lty=2)
```

### Test per $\alpha$ fissato, $\alpha=0.05$

Scegliere il punto critico in questo modo è molto arbitrario e si preferisce scegliere $\alpha$.
Il valore della probabilità di primo tipo viene scelto basso preservare $H_0$ e rifiutarla solo 
se l'evidenza campionaria è molto a favore di $H_1$. 
Fissiamo un valore di $\alpha$ abbastanza piccolo, per esempio, $\alpha=0.05$.
Trovare il __punto critico__ per $\alpha$ fissato significa risolvere il seguente problema inverso
\[P(\hat\mu>\text{Punto Critico};H_0)=0.05\]

Sotto $H_0$
\[\frac{\hat\mu-\mu_0}{\sigma/\sqrt n}=\frac{\hat\mu-`r mu1`}{`r sig`/\sqrt{`r n`}}=\frac{\hat\mu-`r mu1`}{`r se`}\sim N(0,1)\]

Problema inverso lo trasferiamo su $Z$,  per esempio, e trovo il punto critico
\begin{eqnarray*}
P(Z>z_{0.05}) &=& 0.05 \\
z_{0.05} &=& `r qnorm(1-0.05)` \qquad \text{letto dalle tavole}\\
P(Z>`r qnorm(1-0.05)`) &=& 0.05 \\
P\left(\frac{\hat\mu-`r mu1`}{`r se`}>`r qnorm(1-0.05)`\right)&=&0.05\\
P\left(\hat\mu>`r mu1`+`r qnorm(1-0.05)`\cdot`r se`\right)&=&0.05\\
P\left(\hat\mu>`r mu1 + qnorm(1-0.05) * sig/sqrt(n)`\right)&=&0.05\\
\text{Punto Critico} &=& \mu_0+z_{0.05}\frac{\sigma}{\sqrt{n}} \\
                     &=& `r mu1 + qnorm(1-0.05) * se`
\end{eqnarray*}


### La regola di decisione, $\alpha=0.05$

- Se risulterà $\hat\mu\leq `r mu1 + qnorm(1-0.05) * se`$ allora decideremo di **non rifiutare** $H_0$
- Se risulterà $\hat\mu>`r mu1 + qnorm(1-0.05) * se`$ allora decideremo di **rifiutare** $H_0$

La **significatività** di questo test è al 5%:
\[\alpha=0.05=P(\hat\mu>`r mu1 + qnorm(1-0.05) * se`;H_0)\]

Per calcolare $\beta$ dobbiamo calcolare
\begin{eqnarray*}
  \beta &=& P(\hat\mu<`r mu1 + qnorm(1-0.05) * se`;H_1) \\
  &=&      P\left(\frac{\hat\mu-\mu_1}{\sigma/\sqrt n}<\frac{`r mu1 + qnorm(1-0.05) * se`-`r mu2`}{`r se`}\right)\\
      &=& P\left(Z<`r qnorm(1-0.05)`+\frac{`r mu1`-`r mu2`}{`r se`}\right)\\
  &=&P(Z<`r (mu1 + qnorm(1-0.05) * se -mu2)/se`)\\
  &=&\Phi(`r (mu1 + qnorm(1-0.05) * se -mu2)/se`)\\
  &=&`r pnorm((mu1 + qnorm(1-0.05) * se -mu2)/se)`
\end{eqnarray*}

La **potenza** del test è `r (1-pnorm((mu1 + qnorm(1-0.05) * se -mu2)/se))*100`%
\[1-\beta=`r 1-pnorm((mu1 + qnorm(1-0.05) * se -mu2)/se)`\]

### Test per $\alpha$ fissato, $\alpha=0.01$


Fissiamo un valore di $\alpha$ abbastanza piccolo, per esempio, $\alpha=0.01$. 
Trovare il punto critico per $\alpha$ fissato significa risolvere il seguente problema inverso
\[P(\hat\mu>\text{Punto Critico};H_0)=0.01\]

Sotto $H_0$
\[\frac{\hat\mu-\mu_0}{\sigma/\sqrt n}=\frac{\hat\mu-`r mu1`}{`r sig`/\sqrt{`r n`}}=\frac{\hat\mu-`r mu1`}{`r se`}\sim N(0,1)\]

Problema inverso lo trasferiamo su $Z$,  per esempio, e trovo il punto critico
\begin{eqnarray*}
P(Z>z_{0.01}) &=& 0.01 \\
z_{0.01} &=& `r qnorm(1-0.01)` \qquad \text{letto dalle tavole}\\
P(Z>`r qnorm(1-0.01)`) &=& 0.01 \\
P\left(\frac{\hat\mu-`r mu1`}{`r se`}>`r qnorm(1-0.01)`\right)&=&0.01\\
P\left(\hat\mu>`r mu1`+`r qnorm(1-0.01)`\cdot`r se`\right)&=&0.01\\
\text{Punto Critico} &=& \mu_0+z_{0.01}\frac{\sigma}{\sqrt{n}} \\
                     &=& `r mu1 + qnorm(1-0.01) * se`
\end{eqnarray*}

### La regola di decisione, $\alpha=0.01$

- Se risulterà $\hat\mu\leq `r mu1 + qnorm(1-0.01) * se`$ allora decideremo di **non rifiutare** $H_0$.
- Se risulterà $\hat\mu>`r mu1 + qnorm(1-0.01) * se`$ allora decideremo di **rifiutare** $H_0$.

La significatività di questo test è all'1%:
\[\alpha=0.01=P(\hat\mu>`r mu1 + qnorm(1-0.01) * se`;H_0)\]

Per calcolare $\beta$ dobbiamo calcolare
\begin{eqnarray*}
  \beta &=& P(\hat\mu<`r mu1 + qnorm(1-0.01) * se`;H_1) \\
  &=&      P\left(\frac{\hat\mu-\mu_1}{\sigma/\sqrt n}<\frac{`r mu1 + qnorm(1-0.01) * se`-`r mu2`}{`r se`}\right)\\
      &=& P\left(Z<`r qnorm(1-0.01)`+\frac{`r mu1`-`r mu2`}{`r se`}\right)\\
  &=&P(Z<`r (mu1 + qnorm(1-0.01) * se -mu2)/se`)\\
  &=&\Phi(`r (mu1 + qnorm(1-0.01) * se -mu2)/se`)\\
  &=&`r pnorm((mu1 + qnorm(1-0.01) * se -mu2)/se)`
\end{eqnarray*}

La **potenza** del test è `r (1-pnorm((mu1 + qnorm(1-0.01) * se -mu2)/se))*100`%
\[1-\beta=`r 1-pnorm((mu1 + qnorm(1-0.01) * se -mu2)/se)`\]

## $H_0$ semplice e $H_1$ composta

Nella pratica più comune, se $\theta$ è il parametro da testare,
analizzeremo i tre seguenti sistemi di ipotesi

Unilaterale destra $$\begin{cases}
H_0:\theta=\theta_0\\
H_1:\theta>\theta_0
\end{cases}$$

Unilaterale sinistra $$\begin{cases}
H_0:\theta=\theta_0\\
H_1:\theta<\theta_0
\end{cases}$$

Bilaterale $$\begin{cases}
H_0:\theta=\theta_0\\
H_1:\theta\neq \theta_0
\end{cases}$$

## La Statistica Test

La **Statistica Test** è una statistica (una funzione dei dati) che
agevola il processo decisionale. 
Consente di non dover trovare il punto critico nello spazio di
$\mathcal{S}$. 
Calcolata sul campione osservato consente di decidere immediatamente se
il campione porta più evidenza ad $H_0$ oppure ad $H_1$. 

::: {.nota data-latex=""}
Se $\hat\theta$ è stimatore per $\theta$, con errore di stima
$SE(\hat\theta)$, una pratica comune è la statistica test
$$T=\frac{\hat\theta-\theta_0}{SE(\hat\theta)}$$ 
come misura di allontanamento da $H_0$ 

  - se $\hat\theta=\theta_0$, allora $T=0$ 
  - se $\hat\theta>\theta_0$, allora $T>0$ 
  - se $\hat\theta<\theta_0$, allora $T<0$

Se l'errore di stima $SE(\hat\theta)$ è grande, allora piccole
differenze tra $\hat\theta$ e $\theta_0$ **non comportano** a grandi
variazioni di $T$ intorno a zero.

Se l'errore di stima $SE(\hat\theta)$ è piccolo, allora piccole
differenze tra $\hat\theta$ e $\theta_0$ **comportano** grandi
variazioni di $T$ intorno a zero.

Trovare il punto critico sullo spazio di $\mathcal{S}$ equivale a
trovalo su $T$.
:::

<!--chapter:end:14-test-intro.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup15}
rm(list = ls())

source("intro.R")
acex <- 1.5
```

# Test per una media e una proporzione

In questo capitolo svilupperemo il test per una media. Essendo la
proporzione una particolare media su elementi 0 e 1 la costruzione verrà
in automatico. In tutti i casi considereremo un solo campione
$x_1,...,x_n$ proveniente da una popolazione che ha media
$E(X_i)=\mu, \forall i$ e $V(X_i)=\sigma^2, \forall i$.

Il sistema di ipotesi porrà sempre $H_0:\mu=\mu_0$ e si leggerà: la vera
media che ha generato i dati è $\mu_0$, il fatto che la media dei dati
sia $\bar x\ne\mu_0$ è dovuto all'effetto del campionamento e la
differenza tra $\bar x$ e $\mu_0$ **non è significativa**. L'ipotesi
alternativa $H_1$, che sarà unilaterale o bilaterale, si legge al
contrario: la differenza tra la media osservata e la media prescritta da
$H_0$ **è significativa** e il campione proviene da una popolazione che
non ha media $\mu_0$.

Alla luce dei dati decideremo se scegliere di rifiutare $H_0$ o se non
rifiutarla. Costruiremo anche una misura di *vicinanza* empirica ad
$H_1$, il $p_{\text{value}}$, che ci consentirà di capire quanto
l'evidenza empirica dei dati supporti supporti una delle due ipotesi.

## Test sulla media, $\sigma^2$ noto
### Test sulla media, ipotesi unilaterale destra, $\sigma^2$ noto

Siano $X_1,...X_n$, $n$ VC, IID: $X_i\sim N(\mu,\sigma^2)$ con
$\sigma^2$ noto. Consideriamo il seguente sistema di ipotesi

$$\begin{cases}
H_0:\mu=\mu_0\\
H_1:\mu>\mu_0
\end{cases}$$

Dalle proprietà della normale

$$\hat\mu=\bar X=\frac 1 n\sum_{i=1}^nX_i\sim N\left(\mu,\frac{\sigma^2} {n}\right)$$

E quindi

$$Z=\frac{\hat\mu-\mu}{\sigma/\sqrt n}\sim N(0,1)$$

Sotto $H_0: \mu=\mu_0$ la statistica

$$Z=\frac{\hat\mu-\mu_0}{\sigma/\sqrt n}\sim N(0,1)$$

$Z$ osservata sul campione misura l'evidenza contro $H_0$, tanto più è
alto il valore di $Z$ tanto più $H_0$ è inverosimile.

::: {.info data-latex=""}
**Decisione sul campione.** Si decide un livello $\alpha$ e si ricava
$z_\alpha$, si estrae un campione. Lo stimatore $\hat\mu$ si realizza
nella media osservata del campione $\bar x$

$$z_{\text{obs}}=\frac{\bar x -\mu_0}{\sigma/\sqrt n}$$

-   Se $z_{\text{obs}}<z_\alpha$ $H_0$ **non viene rifiutata** al
    livello di significatività $\alpha\times100\%$
-   Se $z_{\text{obs}}>z_\alpha$ $H_0$ **viene rifiutata** al livello di
    significatività $\alpha\times100\%$

```{r 15-test-mu-pi-14}
#par(mar = c(5.1, 4.1, 2, 2.1))
curve(dnorm,-4,4,xlab = expression(Z==(hat(mu)-mu[0])/se),ylab='',axes=F)
segments(-4,0,1.64,0,lwd=2,col=mblue)
segments(1.64,0,4,0,lwd=2,col=ared)
segments(1.64,0,1.64,dnorm(1.64))
axis(1,c(-4,0,1.64,4),c(-4,0,"Punto \n critico",4))
text(2.5,.05,expression(alpha),cex=acex)
title("Normale Standard")
```
:::

### Test sulla media, $\sigma^2$ noto, vari livelli di $\alpha$

Se $\alpha=0.05$ allora
$$\text{Punto Critico}=z_{0.05},\qquad P(Z>z_{0.05})=0.05$$
$z_{0.05}=`r qnorm(1-.05)`$

Se $\alpha=0.01$ allora
$$\text{Punto Critico}=z_{0.01},\qquad P(Z>z_{0.01})=0.01$$
$z_{0.01}=`r qnorm(1-.01)`$

```{r 15-test-mu-pi-15}
#par(mar = c(5.1, 4.1, 2, 2.1))
par(mfrow=c(1,2),cex=.7)
#par(mar = c(5.1, 4.1, 2, 2.1))
curve(dnorm,-4,4,xlab = expression(Z==(hat(mu)-mu[0])/se),ylab='',axes=F)
draw_dist(dnorm,qnorm(.95),4,col=ablue)
segments(-4,0,qnorm(1-.05),0,lwd=2,col=mblue)
segments(qnorm(1-.05),0,4,0,lwd=2,col=ared)
segments(qnorm(1-.05),0,qnorm(1-.05),dnorm(qnorm(1-.05)))
axis(1,c(-4,0,qnorm(1-.05),4),c(-4,0,expression(z[0.05]==1.6449),4))
text(qnorm(.95),dnorm(qnorm(.95)),expression(alpha==0.05),pos=4)
title("Normale Standard")

curve(dnorm,-4,4,xlab = expression(Z==(hat(mu)-mu[0])/se),ylab='',axes=F)
draw_dist(dnorm,qnorm(.99),4,col=ablue)
segments(-4,0,qnorm(1-.01),0,lwd=2,col=mblue)
segments(qnorm(1-.01),0,4,0,lwd=2,col=ared)
segments(qnorm(1-.01),0,qnorm(1-.01),dnorm(qnorm(1-.01)))
axis(1,c(-4,0,qnorm(.99),4),c(-4,0,expression(z[0.01]==2.3263),4))
text(qnorm(.99),dnorm(qnorm(.99)),expression(alpha==0.01),pos=4)
title("Normale Standard")

par(mfrow=c(1,1),cex=.7)
```

::: {.example}
In un laboratorio, vi sono cavie con peso medio uguale a 30g e una DS
pari a 5g. Uno studente seleziona 25 cavie e ottiene un peso medio
uguale a 32g con una $\hat\sigma$ pari a 7g. La selezione è casuale o il
valore ottenuto della media è troppo alto, a un LdS del 5%?

```{r 15-test-mu-pi-1, results='asis'}
muh <- 32
mu0 <- 30
s   <- 5
n   <- 25
alpha <- 0.05
h1 <- ">"

cat(ztest_mu(muh = muh,s = s,n = n,mu0 = mu0,h1 = h1,alpha = alpha,pvalue = F))
```
:::

::: {.example}
In un laboratorio, vi sono cavie con peso medio uguale a 30g e una DS
pari a 5g. Uno studente seleziona 25 cavie e ottiene un peso medio
uguale a 32g con una $\hat\sigma$ pari a 7g. La selezione è casuale o il
valore ottenuto della media è troppo alto, a un LdS del 1%?

```{r 15-test-mu-pi-2, results='asis'}
muh <- 32
mu0 <- 30
s   <- 5
n   <- 25
alpha <- 0.01
h1 <- ">"
um <- "g"

cat(ztest_mu(muh = muh,s = s,n = n,mu0 = mu0,h1 = h1,alpha = alpha,um = um,pvalue = F))
```
:::

### La probabilità di significatività osservata il $p_\text{value}$

Il $p_\text{value}$ risponde a questa domanda: *Se nell'esempio
precedente usassimo* $z_{\text{obs}}$ *come punto critico quale sarebbe
la probabilità di significatività?*

::: {.info data-latex=""}
::: {.definition name="$p_\\text{value}$ caso unilaterale destro"}
$$p_\text{value}=P(Z>z_{\text{obs}};H_0)$$
:::
:::

```{r 15-test-mu-pi-16}
um <- ""
#par(mar = c(5.1, 4.1, 2, 2.1))
curve(dnorm,1.0,3.5,xlab = expression(Z==(hat(mu)-mu[0])/se),ylab='',axes=F)
axis(1,c(1.0,qnorm(.95),2,qnorm(.99),3.5),round(c(1.0,qnorm(.95),2,qnorm(.99),3.5),3))
points(2,0,pch=4,cex=1.5)
segments(qnorm(.95),0,qnorm(.95),dnorm(qnorm(.95)),lty=2)
segments(qnorm(.99),0,qnorm(.99),dnorm(qnorm(.99)),lty=2)
segments(2,0,2,dnorm(2),lty=2)
title("Normale Standard")
xs1 <- seq(2,3.5,length.out = 101)
ys1 <- dnorm(xs1)
xs1 <- c(2,xs1)
ys1 <- c(0,ys1)
polygon(xs1,ys1,col="grey60",density = 20)
```

```{=tex}
\begin{eqnarray*}
p_{\text{value}}&=& P(Z>2) \\
                &=& 1-P(Z<2)\\
                &=& 1-\Phi(2)\\
                &=& 1-`r pnorm(2)`\\
                &=& `r 1-pnorm(2)`
\end{eqnarray*}
```

### Lettura del $p_\text{value}$

La probabilità di significatività osservata $p_\text{value}$ ci dice la
probabilità, *se fosse vera* $H_0$ di trovare un campione ancora più in
favore di $H_1$ di quello che ho trovato. In altre parole ci dice, *se
fosse vera* $H_0$, quanto sarebbe improbabile il nostro campione.

::: {.nota data-latex=""}
Tanto più basso è il $p_\text{value}$ tanto più forte è l'evidenza dei
dati contro $H_0$:

-   Se $p_\text{value}>0.1$ $\rightarrow$ il test **non** è significativo 
-   Se $0.05< p_\text{value}\le 0.1\rightarrow$ il test è _marginalmente_ significativo $\fbox{.}$
-   Se $0.05< p_\text{value}\le 0.01\rightarrow$ il test è significativo 
$\rightarrow$ significativo al 5% $\fbox{*}$
-   Se $0.001< p_\text{value}\le 0.01\rightarrow$ il test è _molto_ significativo $\rightarrow$ significativo all'1% $\fbox{**}$
-   Se $p_\text{value}\le 0.001\rightarrow$ il test è _estremamente_ significativo $\rightarrow$ significativo sotto all'1‰ $\fbox{***}$
:::

### Test sulla media, ipotesi unilaterale sinistra, $\sigma^2$ noto

Siano $X_1,...X_n$ $n$ VC, IID: $X_i\sim N(\mu,\sigma^2)$ con $\sigma^2$
noto.

Consideriamo il seguente sistema di ipotesi $$\begin{cases}
H_0:\mu=\mu_0\\
H_1:\mu<\mu_0
\end{cases}$$

Dalle proprietà della normale
$$\hat\mu=\bar X=\frac 1 n\sum_{i=1}^nX_i\sim N\left(\mu,\frac{\sigma^2} {n}\right)$$

E quindi $$Z=\frac{\hat\mu-\mu}{\sigma/\sqrt n}\sim N(0,1)$$

Sotto $H_0$ la statistica
$$Z=\frac{\hat\mu-\mu_0}{\sigma/\sqrt n}\sim N(0,1)$$

$Z$ osservata sul campione misura l'evidenza contro $H_0$, tanto più è
basso il valore di $Z$ tanto più $H_0$ è inverosimile

::: {.info data-latex=""}
**Decisione sul campione.** Si decide un livello $\alpha$ e si ricava
$z_\alpha$, si estrae un campione. Lo stimatore $\hat\mu$ si realizza
nella media osservata del campione $\bar x$

$$z_{\text{obs}}=\frac{\bar x -\mu_0}{\sigma/\sqrt n}$$

-   Se $z_{\text{obs}}>-z_\alpha$ $H_0$ **non viene rifiutata** al
    livello di significatività $\alpha\times100\%$
-   Se $z_{\text{obs}}<-z_\alpha$ $H_0$ **viene rifiutata** al livello
    di significatività $\alpha\times100\%$

```{r 15-test-mu-pi-17}
#par(mar = c(5.1, 4.1, 2, 2.1))
curve(dnorm,-4,4,xlab = expression(Z==(hat(mu)-mu[0])/se),ylab='',axes=F)
segments(-4,0,-1.64,0,lwd=2,col=ared)
segments(-1.64,0,4,0,lwd=2,col=mblue)
segments(-1.64,0,-1.64,dnorm(-1.64))
axis(1,c(-4,-1.64,0,4),c(-4,expression(-z[alpha]),0,4))
text(-2.5,.05,expression(alpha),cex=acex)
title("Normale Standard")
```
:::

::: {.example}
La durata, in ore, della resistenza di un transistor alle alte
temperature sia $X \,\sim\, N(\mu,\, \sigma^{2})$ e, in base
all'esperienza, sia $\mu_{0}=6$ ore e $\sigma_{0}=0.5$ ore. Si apportano
modifiche alla composizione dei materiali per fare diminuire la durata
della resistenza alla temperatura. Si eseguono 16 osservazioni, dalle
quali si ottiene una durata media $\bar{x}=5.7$ e uno deviazione
standard $s=0.6$. Verificare l'ipotesi, al LdS dell'1%, che la
resistenza sia rimasta invariata contro l'alternativa che sia diminuita.

```{r 15-test-mu-pi-3,results='asis'}
muh <- 5.7
mu0 <- 6
s   <- .5
n   <- 16
alpha <- 0.01
h1 <- "<"
um <- "h"
ztest_mu(muh = muh,s = s,n = n,mu0 = mu0,h1 = h1,alpha = 0.01,um = um,pvalue = F)
```
:::

#### La probabilità di significatività osservata il $p_\text{value}$

::: {.definition name="$p_\\text{value}$ caso unilaterale sinistro"}
Nel caso di ipotesi unilaterale sinistra abbiamo
$$p_\text{value}=P(Z<z_{\text{obs}};H_0)$$
:::

```{r 15-test-mu-pi-18}
um <- ""
#par(mar = c(5.1, 4.1, 2, 2.1))
curve(dnorm,-4,4,xlab = expression(Z==(hat(mu)-mu[0])/se),ylab='',axes=F)
axis(1,c(-4,-2.4,0,+4))
title("Normale Standard")
xs1 <- seq(-4,-2.4,length.out = 101)
ys1 <- dnorm(xs1)
xs1 <- c(-2.4,xs1)
ys1 <- c(0,ys1)
polygon(xs1,ys1,col="grey60",density = 50)
```

\begin{eqnarray*}
p_{\text{value}}&=& P(Z<-2.4) \\
                &=& \Phi(-2.4)\\
                &=& 1-\Phi(2.4)\\
                &=& 1-`r pnorm(2.4)`\\
                &=& `r 1-pnorm(2.4)`
\end{eqnarray*}

### Test sulla media, ipotesi bilaterale, $\sigma^2$ noto

Siano $X_1,...X_n$ $n$ VC, IID: $X_i\sim N(\mu,\sigma^2)$ con $\sigma^2$
noto. Consideriamo il seguente sistema di ipotesi $$\begin{cases}
H_0:\mu=\mu_0\\
H_1:\mu\neq\mu_0
\end{cases}$$

Sotto $H_0$ la statistica
$$Z=\frac{\hat\mu-\mu_0}{\sigma/\sqrt n}\sim N(0,1)$$

$Z$ osservata sul campione misura l'evidenza contro $H_0$, tanto più è
il valore di $Z$ è **diverso** da zero tanto più $H_0$ è inverosimile

::: {.info data-latex=""}
**Decisione sul campione.** Si decide un livello $\alpha$ e si ricava
$z_{\alpha/2}$, si estrae un campione. Lo stimatore $\hat\mu$ si
realizza nella media osservata del campione $\bar x$
$$z_{\text{obs}}=\frac{\bar x -\mu_0}{\sigma/\sqrt n}$$

-   Se $-z_{\alpha/2}\leq z_{\text{obs}}\leq z_{\alpha/2}$ $H_0$ **non
    viene rifiutata** al livello di significatività $\alpha\times100\%$
-   Se $z_{\text{obs}}<-z_{\alpha/2}$, o $z_{\text{obs}}>+z_{\alpha/2}$
    $H_0$ **viene rifiutata** al livello di significatività
    $\alpha\times100\%$

```{r 15-test-mu-pi-19}
#par(mar = c(5.1, 4.1, 2, 2.1))
curve(dnorm,-4,4,xlab = expression(Z==(hat(mu)-mu[0])/se),ylab='',axes=F)
segments(-4,0,-1.96,0,lwd=2,col=ared)
segments(4,0,1.96,0,lwd=2,col=ared)
segments(-1.96,0,1.96,0,lwd=2,col=mblue)
segments(-1.96,0,-1.96,dnorm(-1.96))
segments(1.96,0,1.96,dnorm(1.96))
axis(1,c(-4,-1.96,0,1.96,4),c(-4,expression(-z[alpha/2]),0,expression(+z[alpha/2]),4))
text(-3,.05,expression(alpha/2),cex=acex)
text(+3,.05,expression(alpha/2),cex=acex)
title("Normale Standard")
```
:::

::: {.example}
Un produttore di semiconduttori afferma che la durata media dei suoi chip è di 300 ore con una deviazione standard di 3 ore. Un'azienda decide di testare la qualità dei chip e conduce un esperimento con 16 chip, ottenendo una durata media di 298 ore e una deviazione standard di 4 ore. I risultati di questo test sono coerenti con l'affermazione del produttore i chip tendono a durare in modo diverso dalla durata dichiarata, al 5%?

```{r 15-test-mu-pi-4, results='asis'}
muh <- 298
mu0 <- 300
s   <- 3
n   <- 16
alpha <- 0.05
h1 <- "\\neq"
um <- "~h"
ztest_mu(muh = muh,s = s,n = n,mu0 = mu0,h1 = h1,um = um,pvalue = T,pv_only=F,alpha = alpha)
```
:::

#### La probabilità di significatività osservata il $p_\text{value}$

::: {.info data-latex=""}
::: {.definition name="$p_\\text{value}$ caso bilaterale"}
Nel caso di ipotesi bilaterale abbiamo
$$p_\text{value}=P(|Z|>|z_{\text{obs}}|;H_0)$$
:::
:::

```{r 15-test-mu-pi-20}
#par(mar = c(5.1, 4.1, 2, 2.1))
curve(dnorm,-4,4,xlab = expression(Z==(hat(mu)-mu[0])/se),ylab='',axes=F)
axis(1,c(-4,-2.67,0,2.67,+4))
title("Normale Standard")
xs1 <- seq(-4,-2.67,length.out = 101)
ys1 <- dnorm(xs1)
xs1 <- c(-2.67,xs1)
ys1 <- c(0,ys1)
polygon(xs1,ys1,col="grey60",density = 50)
polygon(-xs1,ys1,col="grey60",density = 50)
```

```{=tex}
\begin{eqnarray*}
p_{\text{value}}&=& P(|Z|>2.67) \\
                &=& P(Z<-2.67)+P(Z>2.67)\\
                &=& 2\cdot P(Z>2.67)\\
                &=& 2\cdot (1-\Phi(2.67))\\
                &=& 2\cdot `r 1-pnorm(2.67)`\\
                &=&  `r 2*(1-pnorm(2.67))`
\end{eqnarray*}
```


## Significatività non fissata

Le strategie per decidere per $H_0$ invece di $H_1$ cambiano a seconda di cosa
rappresentano realmente le due ipotesi. In controllo di qualità industriale
$H_0$ significa conforme agli standard e $H_1$ non conforme, fissare $\alpha$ e 
di conseguenza $\beta$ è frutto di un bilancio rischi benefici che richiede di
esplicitare i costi che l'azienda paga in base all'errore: quando costa commettere
l'errore di primo tipo? Quanto costa commettere l'errore di secondo tipo?

In altri contesti di ricerca la lontananza dei dati da $H_0$ può essere oggetto 
di dibattito e il $p_\text{value}$ misura esattamente questo. Senza avere fissato
in anticipo $\alpha$ il ricercatore di trova a discutere se il $p_\text{value}$
è sufficientemente piccolo da rigettare l'ipotesi nulla. 
Il prossimo esempio esemplifica l'approccio

::: {.example}
Un'azienda farmaceutica afferma che la concentrazione media di principio attivo 
in una certa medicina è di 50 mg con una deviazione standard di 2 mg. Un 
laboratorio indipendente decide di verificare questa affermazione eseguendo 
25 analisi e ottiene una concentrazione media di 51 mg e una deviazione standard 
di 2.5 mg. I risultati delle analisi sono coerenti con l'affermazione 
dell'azienda o la concentrazione del principio attivo è maggiore dalla quantità 
dichiarata?

```{r 15-test-mu-pi-21}
ztest_mu(muh = 51,s = 2,n = 25,mu0 = 50,h1 = ">",um = "mg",pv_only = T,pvalue = F)
```
:::

## Test per $\mu$, $\sigma$ incognita

Se $\sigma^2$ è incognito va stimato dai dati. Consideriamo lo stimatore
$S^2$ di $\sigma^2$
$$S^2=\frac {1}{n-1}\sum_{i=1}^n (X_i-\hat \mu)^2=\frac {n}{n-1}\frac 1 n\sum_{i=1}^n (X_i-\hat \mu)^2=\frac n {n-1}\hat\sigma^2$$

Ricordiamo che
$$\widehat{SE(\hat\mu)}=\sqrt{\frac {S^2}n}=\frac S {\sqrt n}$$

Ricordiamo infine che
$$T=\frac{\hat \mu-\mu}{\widehat{SE(\hat\mu)}}\sim t_{n-1}$$

Ovvero $$T=\frac{\hat \mu-\mu}{S/\sqrt n}\sim t_{n-1}$$

E quindi sotto $H_0$ $$T=\frac{\hat \mu-\mu_0}{S/\sqrt n}\sim t_{n-1}$$
$T$ è la statistica test che misura l'evidenza dei dati contro $H_0$

### Test sulla media, ipotesi unilaterale destra, $\sigma^2$ incognito

Siano $X_1,...X_n$ $n$ VC, IID: $X_i\sim N(\mu,\sigma^2)$ con $\sigma^2$
incognito.

Consideriamo il seguente sistema di ipotesi $$\begin{cases}
H_0:\mu=\mu_0\\
H_1:\mu>\mu_0
\end{cases}$$

Sotto $H_0$ la statistica
$$T=\frac{\hat \mu-\mu_0}{S/\sqrt n}\sim t_{n-1}$$

La $T$ osservata sul campione misura l'evidenza contro $H_0$, tanto più
è alto il valore di $Z$ tanto più $H_0$ è inverosimile

::: {.info data-latex=""}
**Decisione sul campione** Si decide un livello $\alpha$ e si ricava
$t_{n-1;\alpha}$, si estrae un campione. Lo stimatore $\hat\mu$ si
realizza nella media osservata del campione $\bar x$
$$t_{\text{obs}}=\frac{\bar x -\mu_0}{S/\sqrt n}$$

-   Se $t_{\text{obs}}<t_{n-1;\alpha}$ $H_0$ **non viene rifiutata** al
    livello di significatività $\alpha\times100\%$
-   Se $t_{\text{obs}}>t_{n-1;\alpha}$ $H_0$ **viene rifiutata** al
    livello di significatività $\alpha\times100\%$

```{r 15-test-mu-pi-22}
#par(mar = c(5.1, 4.1, 2, 2.1))
curve(dt(x,4),-4,4,xlab = expression(T==(hat(mu)-mu[0])/hat(se)),ylab='',axes=F)
segments(-4,0,2,0,lwd=2,col=mblue)
segments(2,0,4,0,lwd=2,col=ared)
segments(2,0,2,dt(2,4))
axis(1,c(-4,0,2,4),c(-4,0,expression(t[alpha]),4))
text(2.5,.05,expression(alpha),cex=acex)
title(expression(t[n-1]))
```
:::

::: {.example}
Un produttore di aspirine afferma che il prodotto lenisce il mal di
testa in 30 minuti. Un campione casuale di 25 persone la usa. Risultato:
$\bar{x}=31.4$m con $\hat{\sigma}= 4.2$m. Verificare l'affermazione del
produttore a un LdS del 5%.

```{r 15-test-mu-pi-5, results='asis'}
muh <- 31.4
mu0 <- 30
sh  <- 4.2
n   <- 25
alpha <- 0.05
h1 <- ">"

cat(ttest_mu(muh = muh,sh = sh,n = n,mu0 = mu0,h1 = h1,alpha = alpha,um="m"))
```

:::

### Test sulla media, ipotesi unilaterale sinistra, $\sigma^2$ incognito

Siano $X_1,...X_n$ $n$ VC, IID: $X_i\sim N(\mu,\sigma^2)$ con $\sigma^2$
incognito. Consideriamo il seguente sistema di ipotesi $$\begin{cases}
H_0:\mu=\mu_0\\
H_1:\mu<\mu_0
\end{cases}$$

Sotto $H_0$ la statistica
$$T=\frac{\hat \mu-\mu_0}{S/\sqrt n}\sim t_{n-1}$$

Lo stimatore $\hat\mu$ si realizza nella media osservata del campione
$\bar x$ $$t_{\text{obs}}=\frac{\bar x -\mu_0}{S/\sqrt n}$$

::: {.info data-latex=""}
**Decisione sul campione** Si decide un livello $\alpha$ e si ricava
$t_{n-1;\alpha}$, si estrae un campione. Lo stimatore $\hat\mu$ si
realizza nella media osservata del campione $\bar x$
$$t_{\text{obs}}=\frac{\bar x -\mu_0}{S/\sqrt n}$$

-   Se $t_{\text{obs}}>-t_{n-1;\alpha}$ $H_0$ **non viene rifiutata** al
    livello di significatività $\alpha\times100\%$
-   Se $t_{\text{obs}}<-t_{n-1;\alpha}$ $H_0$ **viene rifiutata** al
    livello di significatività $\alpha\times100\%$
:::

```{r 15-test-mu-pi-23}
#par(mar = c(5.1, 4.1, 2, 2.1))
curve(dt(x,4),-4,4,xlab = expression(T==(hat(mu)-mu[0])/hat(se)),ylab='',axes=F)
segments(-4,0,-2,0,lwd=2,col=ared)
segments(-2,0, 4,0,lwd=2,col=mblue)
segments(-2,0,-2,dt(-2,4))
axis(1,c(-4,-2,0,4),c(-4,expression(t[alpha]),0,4))
text(-3,.05,expression(alpha),cex=acex)
title(expression(t[n-1]))
```

::: {.example}
Un Centro dietetico afferma che con i loro programmi si perdono in media
2kg nella prima settimana. Si selezionano casualmente 25 soggetti, tra
gli iscritti al programma.

Risultato: $\overline{\Delta x}=1.5$kg con $\hat\sigma=1.4$kg.
Verificare l'affermazione del Centro al LdS del 5%.

```{r 15-test-mu-pi-6, results='asis'}
muh <- 1.5
mu0 <- 2
sh  <- 1.4
n   <- 25
alpha <- 0.05
h1 <- "<"
um <- "kg"
cat(ttest_mu(muh = muh,sh = sh,n = n,mu0 = mu0,h1 = h1,alpha = alpha,um=um))

```
:::

### Test sulla media, ipotesi bilaterale, $\sigma^2$ incognito

Siano $X_1,...X_n$ $n$ VC, IID: $X_i\sim N(\mu,\sigma^2)$ con $\sigma^2$
incognito. Consideriamo il seguente sistema di ipotesi $$\begin{cases}
H_0:\mu=\mu_0\\
H_1:\mu\neq\mu_0
\end{cases}$$

Sotto $H_0$ la statistica
$$T=\frac{\hat \mu-\mu_0}{S/\sqrt n}\sim t_{n-1}$$

::: {.info data-latex=""}
**Decisione sul campione.** Si decide un livello $\alpha$ e si ricava
$t_{n-1;\alpha}$, si estrae un campione. Lo stimatore $\hat\mu$ si
realizza nella media osservata del campione $\bar x$
$$t_{\text{obs}}=\frac{\bar x -\mu_0}{S/\sqrt n}$$

-   Se $-t_{n-1;\alpha/2}<t_{\text{obs}}<t_{n-1;\alpha/2}$ $H_0$ **non
    viene rifiutata** al livello di significatività $\alpha\times100\%$
-   Se $t_{\text{obs}}>t_{n-1;\alpha/2}$ o
    $t_{\text{obs}}<-t_{n-1;\alpha/2}$ $H_0$ **viene rifiutata** al
    livello di significatività $\alpha\times100\%$

```{r 15-test-mu-pi-24}
#par(mar = c(5.1, 4.1, 2, 2.1))
curve(dt(x,4),-4,4,xlab = expression(T==(hat(mu)-mu[0])/hat(se)),ylab='',axes=F)
segments(-4,0,-2,0,lwd=2,col=ared)
segments(2,0,4,0,lwd=2,col=ared)
segments(-2,0,2,0,lwd=2,col=mblue)
segments(-2,0,-2,dt(-2,4))
segments( 2,0, 2,dt( 2,4))
axis(1,c(-4,-2,0,2,4),c(-4,expression(-t[alpha/2]),0,expression(t[alpha/2]),4))
text(3,.05,expression(alpha/2),cex=acex)
text(-3,.05,expression(alpha/2),cex=acex)
title(expression(t[n-1]))
```
:::

::: {.example}
Eseguite 17 misure di resistenza su 17 campioni di filo. Risultato: la
media è 7.5N (Newton) con $\hat\sigma=$ 1.2N.

Il filo è stato ottenuto con un nuovo procedimento, ma non si conoscono
i possibili effetti sulla resistenza. Verificare, a un LdS del 5%,
l'ipotesi che la resistenza media sia uguale al filo standard, che è
6.8N, contro l'alternativa che sia diverso.

```{r 15-test-mu-pi-7, results='asis'}
muh <- 7.5
mu0 <- 6.8
sh  <- 1.2
n   <- 17
alpha <- 0.05
h1 <- "\\neq"
um <- "N"
cat(ttest_mu(muh = muh,sh = sh,n = n,mu0 = mu0,h1 = h1,alpha = alpha,um=um))
```
:::


### Significatività non fissata

Come per lo $z$-test se la significatività non è fissata potremmo interpretare il solo
$p_\text{value}$, ma il $p_\text{value}$ per la distribuzione $t$ non è calcolabile 
senza un'opportuna funzione che manca nelle gran parte delle calcolatrici scientifiche.

Se non si dispone di un software adeguato si possono usare le tavole statistiche 
e ricavare le soglie critiche per diversi $\alpha$. Una strategia comune è fissare

$$
  \alpha=\{1/10,1/20,1/100,1/1000\}=\{0.1,0.05,0.01,0.001\},
$$

ricavare dalle tavole i rispettivi $t_{n-1,\alpha}$ ($t_{n-1,\alpha/2}$, se il 
test è bilaterale) e vedere dove cade il $t_\text{obs}$

::: {.example}
Un'azienda farmaceutica afferma che la concentrazione media di principio attivo 
in una certa medicina è di 50 mg. Un laboratorio indipendente decide di 
verificare questa affermazione eseguendo 25 analisi e ottiene una concentrazione 
media di 51 mg e una deviazione standard di 2.5 mg. 
I risultati delle analisi sono coerenti con l'affermazione dell'azienda o la 
concentrazione del principio attivo tende a differire dalla quantità dichiarata?

```{r 15-test-mu-pi-8, results='asis'}
muh <- 51
mu0 <- 50
sh  <- 2.5
n   <- 25

h1 <- "\\neq"
um <- "\\%"
ttest_mu(muh = muh,sh = sh,n = n,mu0 = mu0,h1 = h1,um=um,rbow = T)
```
:::

## Massima verosimiglianza e test

Se $\hat\theta$ è stimatore di massima verosimiglianza per $\theta$
allora, per $n$ sufficientemente grande
$$\hat\theta\sim N(\theta, SE^2(\hat\theta))$$

Sotto ipotesi $H_0$ $$\hat\theta\sim N(\theta_0, SE^2(\hat\theta))$$

La statistica test
$$Z=\frac{\hat\theta-\theta_0}{SE(\hat\theta)}\sim N(0,1)$$

Esempio: se $\sigma$ è incognita, la statistica test per $\mu$ è:
$$T=\frac{\hat\mu-\mu_0}{S/\sqrt n}\sim t_{n-1}$$

Se $n$ diverge $$t_{n-1}\to N(0,1)$$

Se $n>100$ il $t$-test diventa lo $z$-test.

## Test per $\pi$

::: {.info data-latex=""}
Siano $X_1,...,X_n$ $n$ VC IID, replicazioni di $X\sim\text{Ber}(\pi)$.
Lo stimatore di massima verosimiglianza
$$\hat\pi=\frac 1 n \sum_{i=1}^n X_i\operatorname*{\sim}_a N\left(\pi,\frac{\pi(1-\pi)}{n}\right)$$

Sotto $H_0$, $\pi=\pi_0$
$$\hat\pi\operatorname*{\sim}_a N\left(\pi_0,\frac{\pi_0(1-\pi_0)}{n}\right)$$

E quindi
$$\frac{\hat\pi-\pi_0}{\sqrt{\pi_0(1-\pi_0)/n}}\operatorname*{\sim}_a N(0,1)$$

La statistica osservata è
$$z_{\text{obs}}=\frac{\hat\pi_{\text{obs}}-\pi_0}{\sqrt{\pi_0(1-\pi_0)/n}}$$

A seconda di $H_1$ decideremo con le solite regole
:::

::: {.example}
Lanciamo $n=50$ un moneta di cui non siamo sicuri se è truccata oppure
no. Osserviamo 30 successi su 50 lanci. Verificare l'ipotesi che la moneta sia 
bilanciata ($\pi=0.5$), contro l'alternativa che sia maggiore.

Lo stimatore di massima verosimiglianza è
$$\hat\pi=\frac 1 n \sum_{i=1}^n X_i$$

```{r 15-test-mu-pi-9, results='asis'}
sn <- 30
p0 <- .5
n   <- 50
alpha <- 0.05
h1 <- ">"
ztest_pi(sn = sn,n = n,p0 = p0,h1 = h1)
```
:::

::: {.example}
Lanciamo $n=100$ un moneta di cui non siamo sicuri se è truccata oppure
no. Osserviamo 60 successi su 50 lanci. Verificare l'ipotesi che la moneta sia bilanciata ($\pi=0.5$), contro
l'alternativa che sia maggiore.

Lo stimatore di massima verosimiglianza è
$$\hat\pi=\frac 1 n \sum_{i=1}^n X_i$$

```{r 15-test-mu-pi-10, results='asis'}
sn <- 60
p0 <- .5
n   <- 100
alpha <- 0.05
h1 <- ">"
ztest_pi(sn = sn,n = n,p0 = p0,h1 = h1)
```
:::

::: {.example}
Lanciamo $n=1000$ un moneta di cui non siamo sicuri se è truccata oppure
no. Osserviamo 600 successi su 50 lanci. Verificare l'ipotesi che la moneta sia bilanciata ($\pi=0.5$), contro
l'alternativa che sia maggiore.

Lo stimatore di massima verosimiglianza è
$$\hat\pi=\frac 1 n \sum_{i=1}^n X_i$$

```{r 15-test-mu-pi-11, results='asis'}
sn <- 600
p0 <- .5
n   <- 1000
alpha <- 0.05
h1 <- ">"
ztest_pi(sn = sn,n = n,p0 = p0,h1 = h1)
```
:::

::: {.example}
In una indagine su 100 imprese, si ha che 30 imprese decentrano la
lavorazione tipo A. Il censimento precedente aveva rilevato una
proporzione di decentramento, $\pi = 0.4$. Verificare l'ipotesi 
che il valore osservato sia dovuto al caso, contro
l'alternativa che vi sia stata una diminuzione della proporzione di
decentramento.

```{r 15-test-mu-pi-12, results='asis'}
sn <- 30
p0 <- .4
n   <- 100
alpha <- 0.05
h1 <- "<"
ztest_pi(sn = sn,n = n,p0 = p0,h1 = h1)
```
:::

::: {.example}
Una città è composta da 50000 soggetti. Si estrae un campione casuale di
100 soggetti e si trova che 20 soggetti possiedono una connessione a
banda ultra-larga. La proporzione posseduta a livello nazionale è pari
al 25%. Verificare che nella città la
percentuale sia diversa (in più o in meno).

```{r 15-test-mu-pi-13, results='asis'}
sn <- 20
p0 <- .25
n   <- 100
alpha <- 0.01
h1 <- "\\neq"
ztest_pi(sn = sn,n = n,p0 = p0,h1 = h1)

```
:::

\clearpage

## Specchietto Finale per i Test ad un Campione

::: {.info2 data-latex=""}
<div style="font-size:0.75em;">
\small
\begin{align*}
\hline 
H_0:\mu&=\mu_0 &&\text{$\sigma^2$} &\text{Dist.}   & && \text{Statistica Test} &&   \text{Zona Rifiuto} &&p_\text{value}\\
\hline\\
H_1:\mu&>\mu_0 & \text{Noto} &&  Z && z_\text{obs}&=\frac{\hat \mu-\mu_0}{\sigma/\sqrt{n}} & z_\text{obs}&>z_\alpha & p_\text{value}&= P(Z>z_\text{obs})\\
H_1:\mu&<\mu_0 & \text{Noto} &&  Z && z_\text{obs}&=\frac{\hat \mu-\mu_0}{\sigma/\sqrt{n}} & z_\text{obs}&<-z_\alpha& p_\text{value}&= P(Z<z_\text{obs})\\
H_1:\mu&\ne\mu_0 & \text{Noto} &&  Z && z_\text{obs}&=\frac{\hat \mu-\mu_0}{\sigma/\sqrt{n}} & |z_\text{obs}|&>|z_{\alpha/2}| & p_\text{value}&= 2P(Z>|z_\text{obs}|)\\
H_1:\mu&>\mu_0 & \text{Incognito} &&  t_{n-1} && z_\text{obs}&=\frac{\hat \mu-\mu_0}{S/\sqrt{n}} & t_\text{obs}&>t_{n-1;~\alpha}& p_\text{value}&= P(T>t_\text{obs})\\
H_1:\mu&<\mu_0 & \text{Incognito} &&  t_{n-1} && z_\text{obs}&=\frac{\hat \mu-\mu_0}{S/\sqrt{n}} & t_\text{obs}&<-t_{n-1;~\alpha}& p_\text{value}&= P(T<t_\text{obs})\\
H_1:\mu&\ne\mu_0 & \text{Incognito} &&  t_{n-1} && z_\text{obs}&=\frac{\hat \mu-\mu_0}{S/\sqrt{n}} & |t_\text{obs}|&>|t_{n-1;~\alpha/2}|& p_\text{value}&= 2P(T>|t_\text{obs}|)\\
\hline
H_0:\pi&=\pi_0 && &\text{Dist.}   && &\text{Statistica Test} &&   \text{Zona Rifiuto} &&p_\text{value}\\
\hline\\
H_1:\pi&>\pi_0 &  &&  Z && z_\text{obs}&=\frac{\hat \pi-\pi_0}{\sqrt{\pi(/1-\pi)}/\sqrt{n}} & z_\text{obs}&>z_\alpha& p_\text{value}&= P(Z>z_\text{obs})\\
H_1:\pi&<\pi_0 &  &&  Z && z_\text{obs}&=\frac{\hat \pi-\pi_0}{\sqrt{\pi(/1-\pi)}/\sqrt{n}} & z_\text{obs}&<-z_\alpha& p_\text{value}&= P(Z<z_\text{obs})\\
H_1:\pi&\ne\pi_0 &  &&  Z && z_\text{obs}&=\frac{\hat \pi-\pi_0}{\sqrt{\pi(/1-\pi)}/\sqrt{n}} & |z_\text{obs}|&>|z_{\alpha/2}| & p_\text{value}&= 2P(Z>|z_\text{obs}|)\\
\hline
\end{align*}
</div>
:::
\normalsize

<!--chapter:end:15-test-mu-pi.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r intro16}
rm(list = ls())

source("intro.R")

acex <- 1.5
```

# Confronto tra due Popolazioni

 Abbiamo trattato un gruppo di 15 pazienti con un placebo e un secondo gruppo di 10 pazienti con il farmaco A, il numero medio di giorni dei pazienti trattati con il placebo è di 10.4 giorni, con una sd di 2.3 giorni; il numero medio di giorni dei pazienti trattati con il farmaco è di 9.6 giorni, con una sd di 1.9 giorni.
  - La differenza tra 10.4 e 9.6 è **significativa**?
  - Il farmaco davvero diminuisce il numero medio di giorni oppure la differenza è colpa del caso?

 In un'indagine sul reddito estraiamo un campione di 56 individui dalla città $A$ e un secondo campione di 67 individui dalla città $B$.
Il reddito medio del campione estratto da $A$ è $\bar x_A=15.1$mila euro lordi annui, con una sd $\hat\sigma_A=3.1$mila euro; Il reddito medio del campione estratto da $B$ è $\bar x_A=18.6$mila euro lordi annui, con una sd $\hat\sigma_A=5.7$mila euro; 
  - Le due città hanno lo stesso reddito medio e la differenza nei campioni è dovuta ala caso oppure i due campioni provengono da due città con reddito medio diverso?
  
 Facciamo un sondaggio di opinione sul gradimento di un personaggio _X_ in due provincie, A e B. Nella provincia A 35 persone su 130 intervistate gradisce il personaggio _X_, nella provincia B 54 su 150 persone intervistate gradisce il personaggio _X_.
  - la differenza tra i due campioni è significativa?
  - nella provincia A e in quella B la proporzione di gradimento è la stessa e la differenza nei campioni è dovuta al caso oppure no?


## Test per due medie 

### il contesto probabilistico

 Siano $X_{1,A},X_{2,A},...,X_{n_A,A}$ $n_A$ VC IID replicazioni di $X_A\sim N(\mu_A,\sigma_A^2)$ e siano $X_{1,B},X_{2,B},...,X_{n_B,B}$ $n_B$ VC IID replicazioni di $X_B\sim N(\mu_B,\sigma_B^2)$

 $X_{1,A},X_{2,A},...,X_{n_A,A}$ è un campione di ampiezza $n_A$ dalla popolazione $\mathscr{P}_A$ e $X_{1,B},X_{2,B},...,X_{n_B,B}$ è un campione di ampiezza $n_B$ dalla popolazione $\mathscr{P}_B$

 Ci possiamo chiedere se:
\[\begin{cases}
H_0:\mu_A=\mu_B\\
H_1:\mu_A>\mu_B
\end{cases}\]
_Test Unilaterale_

 Oppure se
\[\begin{cases}
H_0:\mu_A=\mu_B\\
H_1:\mu_A\neq\mu_B
\end{cases}\]
_Test Bilaterale_

 Perché solo due possibili $H_1$? Perché l'ipotesi
\[\begin{cases}
H_0:\mu_A=\mu_B\\
H_1:\mu_A<\mu_B
\end{cases}\]
equivale a
\[\begin{cases}
H_0:\mu_B=\mu_A\\
H_1:\mu_B>\mu_A
\end{cases}\]


### Derivazione della statistica test

 Se $X_{1,A},X_{2,A},...,X_{n_A,A}$ $n_A$ VC IID replicazioni di $X_A\sim N(\mu_A,\sigma_A^2)$ e siano $X_{1,B},X_{2,B},...,X_{n_B,B}$ $n_B$ VC IID replicazioni di $X_B\sim N(\mu_B,\sigma_B^2)$, allora
\[\hat\mu_A\sim N\left(\mu_A,\frac {\sigma_A^2}{n_A}\right)\qquad\text{e}\qquad\hat\mu_B\sim N\left(\mu_B,\frac {\sigma_B^2}{n_B}\right)\]

 E dunque, dalle proprietà delle normali
\[\hat\mu_A - \hat\mu_B\sim N\left(\mu_A-\mu_B,\frac{\sigma^2_A}{n_A}+\frac{\sigma^2_B}{n_B}\right)\]

 E quindi
\[\frac{(\hat\mu_A - \hat\mu_B)-(\mu_A-\mu_B)}{\sqrt{\frac{\sigma^2_A}{n_A}+\frac{\sigma^2_B}{n_B}}}\sim N(0,1)\]

 Sotto $H_0:\mu_A=\mu_B$
\[Z=\frac{\hat\mu_A - \hat\mu_B}{\sqrt{\frac{\sigma^2_A}{n_A}+\frac{\sigma^2_B}{n_B}}}\sim N(0,1)\]

 Ma $\sigma_A$ e $\sigma_B$ sono incogniti

### Stima di $\sigma_A$ e $\sigma_B$

 Dipende dalle ipotesi che abbiamo sulle due popolazioni:

 Ipotesi 1: omogeneità: si ipotizza che in popolazione $\sigma_A^2=\sigma_B^2=\sigma^2$, e dunque sia il campione proveniente da A che quello proveniente da B contribuiscono a stimare la varianza comune di popolazione $\sigma^2$. 

 Ipotesi 2: eterogeneità: si ipotizza che in popolazione $\sigma_A^2\neq\sigma_B^2$

 In teoria per scegliere tra l'ipotesi di omogeneità e quella di eterogeneità dovremmo fare un test
\[\begin{cases}
H_0:\sigma^2_A=\sigma^2_B\\
H_1:\sigma^2_A\neq\sigma^2_B
\end{cases}\]
il cui sviluppo esula dagli scopi del corso.

 In tutti gli esercizi che faremo l'ipotesi verrà **assunta** nel problema.



### Ipotesi 1: omogeneità

 Sotto ipotesi di omogeneità, entrambi i campioni contribuiscono alla stima della stima comune di $\sigma^2$, lo stimatore congiunto ( _pooled_ ) è
\[S^2_p=\frac{n_A\hat\sigma_A^2+n_B\hat\sigma_B^2}{n_A+n_B-2}\]

 E quindi sotto $H_0$
\[T=\frac{\hat\mu_A-\hat\mu_B}{\sqrt{\frac{S_p^2}{n_A}+\frac{S_p^2}{n_B}}}\sim t_{n_A+n_B-2}\]

 La statistica osservata
\[t_{\text{obs}}=\frac{\bar x_A-\bar x_B}{\sqrt{\frac{S_p^2}{n_A}+\frac{S_p^2}{n_B}}}\]

 Andrà letta nella direzione di $H_1$, sulle tavole della $t$ con $n_A+n_B-2$ gradi di libertà.



### Ipotesi 2: eterogeneità

 Sotto ipotesi di eterogeneità, costruiamo gli stimatori corretti per $\sigma^2_A$ e $\sigma^2_B$
\[S^2_A=\frac{n_A}{n_A-1}\hat\sigma_A^2\qquad S^2_B=\frac{n_B}{n_B-1}\hat\sigma_B^2\]

 E quindi sotto $H_0$
\[T=\frac{\hat\mu_A-\hat\mu_B}{\sqrt{\frac{S_A^2}{n_A}+\frac{S_B^2}{n_B}}}\sim t_{n_A+n_B-2}\]

 La statistica osservata
\[t_{\text{obs}}=\frac{\bar x_A-\bar x_B}{\sqrt{\frac{S_A^2}{n_A}+\frac{S_B^2}{n_B}}}\]

 Andrà letta nella direzione di $H_1$, sulle tavole della $t$ con $n_A+n_B-2$ gradi di libertà.



### Esempio 

 Si sperimentano due diete: $A$ e $B$. Per la dieta $A$ si
selezionano a caso 15 soggetti. Dopo due settima-
ne, si osserva una diminuzione di peso: $\bar x_A$ = 6kg con
$\hat\sigma_A=$ 1.2kg. Per la dieta $B$ si selezionano a caso 18
soggetti. Dopo due settimane, si osserva una diminuzione di peso: 
$\bar x_B$ = 5kg con $\hat\sigma_B=$ 1.8kg. Nell'ipotesi
di varianze **eterogenee**, verificare, se le due diete 
sono equivalenti contro l'alternativa che la dieta $A$ sia più efficace 
(maggiore) della dieta $B$.

```{r 16-test-2C-1,results='asis'}
mu1 <- 6
mu2 <- 5
s1h   <- 1.2
s2h <- 1.8
a <- "$A$"
b <- "$B$"
um <- ""
et <- TRUE
n1   <- 15
n2 <- 18
alpha <- 0.05
h1 <- ">"
ttest_2c_et(mu1 = mu1,mu2 = mu2,s1h = s1h,s2h =s2h,n1 = n1,n2 = n2,h1 = h1,a = a,b = b,um = um,rbow = T)
```

### Esempio 

La direzione vuole verificare se l'ammontare delle vendite di due
supermercati, $A$ e $B$, sia la stessa. Un campione di 18 giorni per
il supermercato $A$ fornisce una vendita media giornaliera pari a
$\bar x_A=$ 55 mila euro, con $\hat\sigma_A=$ 2.9 mila euro. Un campione di 24
giorni per il supermercato $B$ fornisce $\bar x_B=$ 57 mila euro, con 
 $\hat\sigma_A=$ 3.1 mila euro. 
 
Sotto assunto di **omogeneità**  delle varianze
verificare l'ipotesi che la vendita media del supermercato $A$ sia uguale a quella del supermercato $B$, contro l'alternativa sia diversa.

```{r 16-test-2C-2,results='asis'}
mu1 <- 55
mu2 <- 57
s1h   <- 2.9
s2h <- 3.1
et <- FALSE
n1   <- 18
n2 <- 24
alpha <- 0.01
h1 <- "\\neq"

cat(ttest_2c_om(mu1 = mu1,mu2 = mu2,s1h = s1h,s2h =s2h,n1 = n1,n2 = n2,h1 = h1,a = a,b = b,um = um ))

```


## Test per due proporzioni 

### Il contesto probabilistico

 Siano $X_{1,A},X_{2,A},...,X_{n_A,A}$ $n_A$ VC IID replicazioni di $X_A\sim \text{Ber}(\pi_A)$ e siano $X_{1,B},X_{2,B},...,X_{n_B,B}$ $n_B$ VC IID replicazioni di $X_B\sim \text{Ber}(\pi_B)$

 $X_{1,A},X_{2,A},...,X_{n_A,A}$ è un campione di ampiezza $n_A$ dalla popolazione $\mathscr{P}_A$ e $X_{1,B},X_{2,B},...,X_{n_B,B}$ è un campione di ampiezza $n_B$ dalla popolazione $\mathscr{P}_B$

 Ci possiamo chiedere se:
\[\begin{cases}
H_0:\pi_A=\pi_B\\
H_1:\pi_A>\pi_B
\end{cases}\]
_Test Unilaterale_

 Oppure se
\[\begin{cases}
H_0:\pi_A=\pi_B\\
H_1:\pi_A\neq\pi_B
\end{cases}\]
_Test Bilaterale_


### Derivazione della statistica test

 Se $X_{1,A},X_{2,A},...,X_{n_A,A}$ $n_A$ VC IID replicazioni di $X_A\sim \text{Ber}(\pi_A)$ e siano $X_{1,B},X_{2,B},...,X_{n_B,B}$ $n_B$ VC IID replicazioni di $X_B\sim \text{Ber}(\pi_B)$,
allora
\[\hat\pi_A\operatorname*{\sim}_a N\left(\pi_A,\frac {\pi_A(1-\pi_A)}{n_A}\right)\qquad\text{e}\qquad\hat\pi_B\operatorname*{\sim}_a N\left(\pi_B,\frac {\pi_B(1-\pi_B)}{n_B}\right)\]

 E dunque, dalle proprietà delle normali
\[\hat\pi_A - \hat\pi_B\operatorname*{\sim}_a N\left(\pi_A-\pi_B,\frac{\pi_A(1-\pi_A)}{n_A}+\frac{\pi_B(1-\pi_B)}{n_B}\right)\]

 E quindi
\[\frac{(\hat\pi_A - \hat\pi_B)-(\pi_A-\pi_B)}{\sqrt{\frac{\pi_A(1-\pi_A)}{n_A}+\frac{\pi_B(1-\pi_B)}{n_B}}}\sim N(0,1)\]

 Sotto $H_0:\pi_A=\pi_B=\pi_C$
\[Z=\frac{\hat\pi_A - \hat\pi_B}{\sqrt{\frac{\pi_C(1-\pi_C)}{n_A}+\frac{\pi_C(1-\pi_C)}{n_B}}}\sim N(0,1)\]

 La stima di $\pi_C$ è
\[\hat\pi_C=\frac{\#\{\text{successi nel gruppo A}\}+\#\{\text{successi nel gruppo B}\}}{n_A+n_B}=\frac{n_A\hat\pi_A+n_B\hat\pi_B}{n_A+n_B}\]

 E dunque
\[Z=\frac{\hat\pi_A - \hat\pi_B}{\sqrt{\frac{\hat\pi_C(1-\hat\pi_C)}{n_A}+\frac{\hat\pi_C(1-\hat\pi_C)}{n_B}}}\operatorname*{\sim}_a N(0,1)\]

 La statistica osservata è 
\[z_{\text{obs}}=\frac{\hat\pi_A-\hat\pi_B}{\sqrt{\frac{\hat\pi_C(1-\hat\pi_C)}{n_A}+\frac{\hat\pi_C(1-\hat\pi_C)}{n_B}}}\]
e andrà letta nella direzione di $H_1$ sulle tavole della $Z$.

### Esempio

Tra i abitanti del comune $A$, si intervistano 80 uomini e 100 donne
per capire l'impatto che avrà la legge comunale sul divieto di
fumo nei parchi pubblici. Dalle interviste risulta che 70
uomini e 70 donne si dichiarano a favore di tale legge.
Verificare l'ipotesi che la nuova legge sia 
accolta in modo equivalente da donne e uomini,
contro l'alternativa che le donne si dimostrino meno
propense a accettare tale legge.


```{r 16-test-2C-3, results='asis'}
s1 <- 70
n1 <- 80
s2 <- 70
n2 <- 100
alpha <- 0.05
h1 <- ">"
a <- "$U$"
b <- "$D$"
ztest_2c_pi(s1 = s1,s2 = s2,n1 = n1,n2 = n2,h1 = h1,a = a,b = b)

```



## Specchietto Finale per i Test ad Due Campioni

::: {.info2 data-latex=""}
<div style="font-size:0.9em;">
```{r 16-test-2C-4}
if (!html) {
  tabella <- 
"\\begin{align*}
    \\text{Test $t$, 2 Campioni} & 
    \\text{\\qquad Test $t$, 2 Campioni} & 
    \\text{Proporzione, 2 Campioni} \\\\
    \\text{Omogeneità} & 
    \\text{\\qquad Eterogeneità} & 
     \\\\
    \\midrule
    \\begin{aligned}
      t_{\\text{obs}}&=\\frac{\\hat\\mu_A-\\hat\\mu_B}{\\sqrt{\\frac{S_p^2}{n_A}+\\frac{S_p^2}{n_B}}}\\\\
      S^2_p &= \\frac{n_A\\hat\\sigma_A^2+n_B\\hat\\sigma_B^2}{n_A+n_B-2}\\\\
      \\addlinespace \\\\
    \\end{aligned} &
    \\begin{aligned}
      \\displaystyle t_{\\text{obs}}=\\frac{\\hat\\mu_A-\\hat\\mu_B}{\\sqrt{\\frac{S_A^2}{n_A}+\\frac{S_B^2}{n_B}}}\\\\
      \\addlinespace \\\\ \\addlinespace \\\\ 
    \\end{aligned} &
    \\begin{aligned}
      z_{\\text{obs}} &=\\frac{\\hat\\pi_A-\\hat\\pi_B}{\\sqrt{\\frac{\\hat\\pi_C(1-\\hat\\pi_C)}{n_A}+\\frac{\\hat\\pi_C(1-\\hat\\pi_C)}{n_B}}}\\\\
      \\hat\\pi_C &=\\frac{\\#\\{\\text{successi A}\\}+\\#\\{\\text{successi B}\\}}{n_A+n_B}\\\\
                &=\\frac{n_A\\hat\\pi_A+n_B\\hat\\pi_B}{n_A+n_B}
    \\end{aligned} \\\\
    \\addlinespace
    \\toprule
\\end{align*}"
cat(tabella)} else {
  tabella <- 
"\\begin{align*}
    \\text{Test $t$, 2 Campioni} & 
    \\qquad\\text{ Test $t$, 2 Campioni} & 
    \\text{Proporzione, 2 Campioni} \\\\
    \\text{Omogeneità} & 
    \\qquad \\text{Eterogeneità} & 
     \\\\ \\hline
    \\begin{aligned}
      t_{\\text{obs}}&=\\frac{\\hat\\mu_A-\\hat\\mu_B}{\\sqrt{\\frac{S_p^2}{n_A}+\\frac{S_p^2}{n_B}}}\\\\
      S^2_p &= \\frac{n_A\\hat\\sigma_A^2+n_B\\hat\\sigma_B^2}{n_A+n_B-2}\\\\
    \\end{aligned} &
    \\begin{aligned}
      \\displaystyle \\quad t_{\\text{obs}}=\\frac{\\hat\\mu_A-\\hat\\mu_B}{\\sqrt{\\frac{S_A^2}{n_A}+\\frac{S_B^2}{n_B}}}\\\\
    \\end{aligned} &
    \\begin{aligned}
      z_{\\text{obs}} &=\\frac{\\hat\\pi_A-\\hat\\pi_B}{\\sqrt{\\frac{\\hat\\pi_C(1-\\hat\\pi_C)}{n_A}+\\frac{\\hat\\pi_C(1-\\hat\\pi_C)}{n_B}}}\\\\
      \\hat\\pi_C &=\\frac{\\#\\{\\text{successi A}\\}+\\#\\{\\text{successi B}\\}}{n_A+n_B}\\\\
                &=\\frac{n_A\\hat\\pi_A+n_B\\hat\\pi_B}{n_A+n_B}
    \\end{aligned} \\\\
  \\hline
\\end{align*}"
cat(tabella)}

```
</div>
:::

\normalsize

<!--chapter:end:16-test-2C.Rmd-->

---
editor_options: 
  chunk_output_type: console
---

```{r setupreg, include=FALSE}
rm(list = ls())


source("intro.R")
fig.def(3,3)
```

# Regressione Lineare

## Il modello d'errore


Siano $Y_1,...,Y_n$ $n$ VC IID, replicazioni tc $Y_i\sim N(\mu,\sigma_\varepsilon^2)$, dalle proprietà della normale possiamo riscrivere:
\[Y_i=\mu+\varepsilon_i\qquad\varepsilon_i\sim N(0,\sigma_\varepsilon^2)\]

```{r 17-regressione-I-6}
set.seed(4)
n <- 10
ysamp1 <- rnorm(n,15,1)
plot(rep(0,times=n),ysamp1,axes = F,xlab="",ylab="",pch=4,ylim=c(13,17))
abline(v = 0,lty=3,col="grey")
abline(h = 15,lty=3,col="grey")
axis(2,15,expression(mu),las=2)
points(0,15,pch=16,col=ared)
ygr <- seq(11,19,length.out = 101)
lines(dnorm(ygr,15,1),ygr,col="grey")
arrows(-1.06,13,-1.06,17,.1)
text(-.9,17,"y")
arrows( 1.06,13, 1.06,17,.1)
axis(4,15,0,las=2)
text( .9,17,expression(epsilon))
ysamp1 <- round(ysamp1 -15 +28,2)
ybar1 <- mean(ysamp1)
epsh1 <- ybar1-ysamp1
sbar1 <- round(sd(ysamp1),3)



s2c <- function(x) round(mean(x^2)-mean(x)^2,4)
sc  <- function(x) round(sqrt(s2c(x)),4)

set.seed(5)
ysamp2 <- round(rnorm(n,16,1),2)-15+28
ybar2 <- mean(ysamp2)
epsh2 <- ybar2-ysamp2
sbar2 <- round(sd(ysamp2),3)

f1 <- approxfun(c(0,1.13),c(28,29))
be <- f1(1)-28

set.seed(12)
ysamp3 <- round(rnorm(n,28+be*2.84,1),2)
ybar3<- mean(ysamp3)
epsh3 <- ybar3-ysamp3
sbar3 <- round(sd(ysamp3),3)


# plot(c(0,1.13),c(28,29))
# segments(0,28,1.13,28,lty=2)
# segments(1.13,28,1.13,29,lty=2)
# segments(0,28,1.13,29,lty=2)
# segments(1,28,1,28+b,lty=2,col=ared)

```


\[Y_i\sim N(\mu,\sigma_\varepsilon^2)~~\text{ è equivalente a dire }~~Y_i=\mu+\varepsilon_i,~~\varepsilon_i\sim N(0,\sigma_\varepsilon^2)\]

### Esempi
```{r 17-regressione-I-7}
sbar <- sbar1
ybar <- ybar1
ysamp <- ysamp1
quad <-paste(paste(paste(ysamp[1:3],collapse = "^2+"),"^2",sep=""),"+...+",paste(paste(ysamp[9:10],collapse = "^2+"),"^2",sep=""))
```

::: {.example}
Stiamo studiando la produttività media di un ettaro coltivato ad una certa varietà di riso: 
per prima cosa piantiamo 10 ettari **non** trattati con fertilizzante ($X=0$) con questa varietà e calcoliamo i quintali per ettaro, otteniamo (`r ysamp`)

\[Y_i=\mu_{(X=0)}+\varepsilon_i\qquad\varepsilon_i\sim N(0,\sigma_\varepsilon^2)\]


Stimiamo
\[\hat\mu_{(X=0)}=\frac 1 n\sum_{i=1}^n y_i=`r ybar`\]
e
\begin{eqnarray*}
\hat\sigma_\varepsilon^2&=&\frac 1 n\sum_{i=1}^n y_i^2-\hat\mu_{(X=0)}^2\\
&=&\frac 1 {10}(`r quad`)-`r ybar`^2\\
&=&`r s2c(ysamp)`\\
\hat\sigma_\varepsilon &=& `r sc(ysamp)`
\end{eqnarray*}

Costruiamo dapprima un intervallo di confidenza sui dati
Correggiamo $\hat\sigma_\varepsilon^2$

\begin{eqnarray*}
S_{(X=0)}^2&=&\frac{n}{n-1}\hat\sigma_{(X=0)}^2\\
&=&\frac{10}{10-1}\cdot`r s2c(ysamp)`\\
&=&`r sbar^2`\\
S_{(X=0)}&=&`r sbar`
\end{eqnarray*}

 L'intervallo di confidenza al 95% per $\mu_{(X=0)}$ è
\[\hat\mu_{(X=0)}\pm t_{n-1;\alpha/2}\frac S{\sqrt n}=`r ybar`\pm `r qt(.975,9)`\frac{`r sbar`}{10}=
(`r ybar-qt(.975,9)*sbar/sqrt(10)`;`r ybar+qt(.975,9)*sbar/sqrt(10)`)\]

```{r 17-regressione-I-8}
set.seed(4)
n <- 10
y1samp <- rnorm(n,15,1)
set.seed(5)
#set.seed(11)
y2samp <- rnorm(n,16,1)
plot(rep(0,each=n),y1samp,axes = F,xlab="",ylab="",pch=4,xlim=c(-1,1),ylim = c(13,18))
abline(v = 0,lty=3,col="grey")
abline(h = 15,lty=3,col="grey")

axis(2,c(13:14,mean(y1samp),15,16:17),c(26:27,ybar1,expression(mu[(X==0)]),29:30),las=2)
segments(0,mean(y1samp),-10,mean(y1samp),lty=2)
axis(1,0,c(expression(X==0)))
points(0,15,pch=16,col=ared)
points(0,mean(y1samp),pch="-",col=4,cex=2)
brackets(0.05,mean(y1samp),0.05,15,h = .1)
text(.3,(mean(y1samp)+15)/2,expression(hat(mu)-mu))

ygr <- seq(11,19,length.out = 101)
lines(dnorm(ygr,15,1),ygr,col="grey")
arrows(-1.06,13,-1.06,18,.1)
arrows( 1.06,13, 1.06,18,.1)

axis(4,15,0,las=2)
text( .9,18,expression(epsilon))

int1 <- t.test(y1samp)$conf.int
int2 <- t.test(y2samp)$conf.int
text(-.9,18,"y")


arrows(0,int1[1],0,int1[2],.1,angle = 90,code = 3,col=4,lwd = 2)

```


E quindi potremmo proporre la **previsione**:
quanta produzione ci aspetteremo sul prossimo ettaro che pianteremo con quella varietà di riso?
\begin{eqnarray*}
E(\hat Y_{11})&=&E(\hat\mu_{(X=0)}+\varepsilon_{11})\\
&=&`r ybar`+E(\varepsilon_{11})\\ 
&=&`r ybar`\qquad \text{poiché } 
\varepsilon_{11}\sim N\left(0,\sigma_\varepsilon^2\right)
\end{eqnarray*}

È una _previsione_ corretta?
\[E(\hat Y_{11})=E(\hat\mu+\varepsilon_{11})=E(\hat\mu)+E(\varepsilon_{11})=\mu+0=\mu\]

L'errore di previsione stimato è $S_{(X=0)}=`r sbar`$.
:::

::: {.example}

```{r 17-regressione-I-9}
sbar <- sbar2
ybar <- ybar2
ysamp <- ysamp2
quad <-paste(paste(paste(ysamp[1:3],collapse = "^2+"),"^2",sep=""),"+...+",paste(paste(ysamp[9:10],collapse = "^2+"),"^2",sep=""))
```

Supponiamo di aver piantato altri 10 ettari di questa varietà ma su terreni trattati con 1.13 hg ($X=1.13$) 
di concime azotato per ettaro e osserviamo (`r ysamp2`)
\[Y_i=\mu_{(X=1.13)}+\varepsilon_i\qquad\varepsilon_i\sim N(0,\sigma_\varepsilon^2)\]

Stimiamo
\[\hat\mu_{(X=1.13)}=\frac 1 n\sum_{i=1}^n y_i=`r ybar`\]
e
\begin{eqnarray*}
\hat\sigma_\varepsilon^2&=&\frac 1 n\sum_{i=1}^n y_i^2-\hat\mu_{(X=0)}^2\\
&=&\frac 1 {10}(`r quad`)-`r ybar`^2\\
&=&`r s2c(ysamp)`\\
\hat\sigma_\varepsilon &=& `r sc(ysamp)`
\end{eqnarray*}

Correggiamo $\hat\sigma_\varepsilon^2$

\begin{eqnarray*}
S_{(X=1.13)}^2&=&\frac{n}{n-1}\hat\sigma_{(X=0)}^2\\
&=&\frac{10}{10-1}\cdot`r s2c(ysamp)`\\
&=&`r sbar^2`\\
S_{(X=1.3)}&=&`r sbar`
\end{eqnarray*}

L'intervallo di confidenza al 95% per $\mu_{(X=1.13)}$ è
\[\hat\mu_{(X=1.13)}\pm t_{n-1;\alpha/2}\frac S{\sqrt n}=`r ybar`\pm `r qt(.975,9)`\frac{`r sbar`}{10}=
(`r ybar-qt(.975,9)*sbar/sqrt(10)`;`r ybar+qt(.975,9)*sbar/sqrt(10)`)\]

```{r 17-regressione-I-10}
plot(rep(c(0,1),each=n),c(y1samp,y2samp),axes = F,xlab="",ylab="",pch=4,xlim=c(-1,2),ylim = c(13,18))
abline(v = 0,lty=3,col="grey")
abline(v = 1,lty=3,col="grey")
abline(h = 15,lty=3,col="grey")
abline(h = 16,lty=3,col="grey")

axis(2,15:16,c(expression(mu[(X==0)]),expression(mu[(X==1.13)])),las=2)
axis(1,0:1,c(expression(X==0),expression(X==1.13)))
points(0,15,pch=16,col=ared)
points(0,mean(y1samp),pch="-",col=4,cex=2)
points(1,mean(y2samp),pch="-",col=4,cex=2)
points(1,16,pch=16,col=ared)

ygr <- seq(11,19,length.out = 101)
lines(dnorm(ygr,15,1),ygr,col="grey")
lines(1+dnorm(ygr,16,1),ygr,col="grey")
arrows(-1.06,13,-1.06,18,.1)
int1 <- t.test(y1samp)$conf.int
int2 <- t.test(y2samp)$conf.int
text(-.9,18,"y")

text(-.5,mean(y1samp),substitute(hat(mu)[(X==0)]==ybar1,env = list(ybar1=ybar1)),cex=0.8)
text(+.5,mean(y2samp),substitute(hat(mu)[(X==1.13)]==ybar2,env = list(ybar2=ybar2)),cex=0.8)

arrows(0,int1[1],0,int1[2],.1,angle = 90,code = 3,col=4,lwd=2)
arrows(1,int2[1],1,int2[2],.1,angle = 90,code = 3,col=4,lwd=2)
fig.def()
```

Se mettiamo a test, otteniamo

```{r 17-regressione-I-1,results='asis'}
mu1 <- ybar2
mu2 <- ybar1
b <- "$(X=0)$" 
a <- "$(X=1.13)$"
s1h   <- sc(ysamp2)
s2h <- sc(ysamp1)
et <- F
n1   <- 10
n2 <- 10
alpha <- 0.05
h1 <- ">"

cat(ttest_2c_om(mu1 = mu1,mu2 = mu2,s1h = s1h,s2h = s2h,n1 = n1,n2 = n2,h1 = ">",alpha = alpha,a = a,b = b))
fig.def(3,3)
```
:::

::: {.example}

```{r 17-regressione-I-11}
sbar <- sbar3
ybar <- ybar3
ysamp <- ysamp3
quad <-paste(paste(paste(ysamp[1:3],collapse = "^2+"),"^2",sep=""),"+...+",paste(paste(ysamp[9:10],collapse = "^2+"),"^2",sep=""))
```

Supponiamo di aver piantato altri 10 ettari di questa varietà ma su terreni trattati con 2.84 hg ($X=2.84$) 
di concime azotato per ettaro e osserviamo (`r ysamp3`)
\[Y_i=\mu_{(X=2.84)}+\varepsilon_i\qquad\varepsilon_i\sim N(0,\sigma_\varepsilon^2)\]

Stimiamo
\[\hat\mu_{(X=2.84)}=\frac 1 n\sum_{i=1}^n y_i=`r ybar3`\]
e
\begin{eqnarray*}
\hat\sigma_{(X=2.84)}^2&=&\frac 1 n\sum_{i=1}^n y_i^2-\hat\mu_{(X=2.84)}^2\\
&=&\frac 1 {10}(`r quad`)-`r ybar`^2\\
&=&`r s2c(ysamp3)`\\
\hat\sigma_{(X=2.84)} &=& `r sc(ysamp3)`
\end{eqnarray*}
infine

\begin{eqnarray*}
S_{(X=2.84)}^2&=&\frac{n}{n-1}\hat\sigma_{(X=2.84)}^2\\
&=&\frac{10}{10-1}\cdot`r s2c(ysamp3)`\\
&=&`r sbar3^2`\\
S_{(X=2.84)}&=&`r sbar3`
\end{eqnarray*}

```{r 17-regressione-I-12}
set.seed(4)
n <- 10
plot(rep(c(0,1.13,2.84),each=n),c(ysamp1,ysamp2,ysamp3),axes = F,xlab="",ylab="",pch=4,xlim=c(-.5,4),ylim = c(min(ysamp1),max(ysamp3)))
abline(v = 0,lty=3,col="grey")
abline(v = 1.13,lty=3,col="grey")
abline(v = 2.84,lty=3,col="grey")

axis(2,15:16,c(expression(mu[1]),expression(mu[2])),las=2)
axis(1,c(0,1.13,2.84))
points(0,mean(ysamp1),pch="-",col=4,cex=2)
points(1.13,mean(ysamp2),pch="-",col=4,cex=2)
points(2.84,mean(ysamp3),pch="-",col=4,cex=2)

ygr <- seq(11,19,length.out = 101)
lines(dnorm(ygr,15,1),ygr,col="grey")
lines(1+dnorm(ygr,16,1),ygr,col="grey")

axis(2)

int1 <- t.test(ysamp1)$conf.int
int2 <- t.test(ysamp2)$conf.int
int3 <- t.test(ysamp3)$conf.int

text(-.5,ybar1,ybar1,cex=0.9)
text(+.5+.13,ybar2,ybar2,cex=.9)
text(2.84-.5,ybar3,ybar3,cex=.9)

arrows(0,int1[1],0,int1[2],.1,angle = 90,code = 3,col=4,lwd=2)
arrows(1.13,int2[1],1.13,int2[2],.1,angle = 90,code = 3,col=4,lwd=2)
arrows(2.84,int3[1],2.84,int3[2],.1,angle = 90,code = 3,col=4,lwd=2)
fig.def(3,4)
```

**problema 1:** abbiamo tre stime di $\mu$ e tre stime di $\sigma$ ottenute come se i campioni fossero separati.
Non abbiamo tenuto conto della natura metrica della $X$.

```{r 17-regressione-I-13}
set.seed(4)
n <- 10
plot(rep(c(0,1.13,2.84),each=n),c(ysamp1,ysamp2,ysamp3),axes = F,xlab="",ylab="",pch=4,xlim=c(-.5,4),ylim = c(min(ysamp1),max(ysamp3)))
abline(v = 0,lty=3,col="grey")
abline(v = 1.13,lty=3,col="grey")
abline(v = 2.84,lty=3,col="grey")

axis(2,15:16,c(expression(mu[1]),expression(mu[2])),las=2)
axis(1,c(0,1.13,2.84))
points(0,mean(ysamp1),pch="-",col=4,cex=2)
points(1.13,mean(ysamp2),pch="-",col=4,cex=2)
points(2.84,mean(ysamp3),pch="-",col=4,cex=2)

ygr <- seq(11,19,length.out = 101)
lines(dnorm(ygr,15,1),ygr,col="grey")
lines(1+dnorm(ygr,16,1),ygr,col="grey")

text(.25,ybar1,expression(hat(mu)[x==0]))
text(1.13+.25,ybar2,expression(hat(mu)[x==1.13]))
text(2.84+.25,ybar3,expression(hat(mu)[x==2.84]))

arrows(0,25.5,1.13,25.5,length = .05,code = 3)
arrows(1.13,25.5,2.84,25.5,length = .05,code = 3)

arrows(-.2,ybar1-sbar1/2,-.2,ybar1+sbar1/2,length = .05,code = 3,angle = 90)
text(-.3,ybar1,expression(hat(sigma)[epsilon]))
arrows(1.13-.2,ybar2-sbar2/2,1.13-.2,ybar2+sbar2/2,length = .05,code = 3,angle = 90)
text(1.13-.3,ybar2,expression(hat(sigma)[epsilon]))
arrows(2.84-.2,ybar3-sbar3/2,2.84-.2,ybar3+sbar3/2,length = .05,code = 3,angle = 90)
text(2.84-.3,ybar3,expression(hat(sigma)[epsilon]))


#arrows(-1.06,25,-1.06,35,.1)
#text(-.9,35,"y")
axis(2)
# arrows( 1.06,13, 1.06,17,.1)
# axis(4,15,0,las=2)
# text( .9,17,expression(epsilon))

```



**problema 2:** alla luce di questi dati cosa possiamo dire sulla produzione media se usassimo 1.9 hg di concime per ettaro? 
Potremmo proporre di congiungere le medie con una spezzata

```{r 17-regressione-I-14}
n <- 10
plot(rep(c(0,1.13,2.84),each=n),c(ysamp1,ysamp2,ysamp3),axes = F,xlab="",ylab="",pch=4,xlim=c(-.5,4),ylim = c(min(ysamp1),max(ysamp3)))
abline(v = 0,lty=3,col="grey")
abline(v = 1.13,lty=3,col="grey")
abline(v = 2.84,lty=3,col="grey")
abline(v = 1.9,lty=3,col=ared)

cc <- lsfit(c(1.13,2.84),c(ybar2,ybar3))$coef

yhat <-sum(cc*c(1,1.9))

segments(-10,yhat,1.9,yhat,lty=3)
segments(0,ybar1,1.13,ybar2,lty=1,col="grey60")
segments(1.13,ybar2,2.84,ybar3,lty=1,col="grey60")

points(0,mean(ysamp1),pch="-",col=4,cex=2)
points(1.13,mean(ysamp2),pch="-",col=4,cex=2)
points(2.84,mean(ysamp3),pch="-",col=4,cex=2)

axis(side = 1,at = 1.9,col.ticks = 2,col.axis=2)
axis(1,c(0,1.13,2.84))
axis(2,c(27,28,yhat,31,32),c(27,28,round(yhat,2),31,32),las=2)

title(main = "una suggestione")
```
:::

::: {.example}

**problema 3:** Dobbiamo studiare un nuova varietà di riso, ma il budget è inferiore.
osserviamo 10 ettari

```{r 17-regressione-I-15}
set.seed(1)
x <- (0:9)*1.13/5
y <- 27.5+.82*x+ rnorm(10,0,1)

xy <- data.frame(`X fertilizzante`=x,`Y raccolto`=y)
names(xy)<-c("$X=$ Fertilizzante","$Y=$ Raccolto")

kable(t(xy),booktabs = T, escape = F, linesep = "")

plot(x,y,axes = F,pch=16)
axis(1,x,las=2)
axis(2,y,round(y,2),las=2)
segments(x,0,x,y,lty=2,col="grey90")
segments(-100,y,x,y,lty=2,col="grey90")

fig.def(3)
```

La previsione sembra più difficile, 
la suggestione di prima non sembra funzionare

```{r 17-regressione-I-2,results='asis'}
opar <- par(no.readonly = T)
par(mfrow=c(1,2),cex=cex)
set.seed(1)
x <- (0:9)*1.13/5
y <- 27.5+.82*x+ rnorm(10,0,1)

plot(x,y,axes = F,type="b",lty=2)
axis(1,x,las=2)
axis(2,y,round(y,2),las=2)

set.seed(1)
x <- (0:9)*1.13/5
y <- 27.5+.82*x+ rnorm(10,0,1)

plot(x,y,axes = F,type="p",lty=2)
# xys <- smooth.spline(x = x,y = y,spar = .5)
# lines(predict(xys,data.frame(x=seq(0,2,by=.01))$x),col=ared)
 axis(1,x,las=2)
 axis(2,y,round(y,2),las=2)

xys <- smooth.spline(x = x,y = y,spar = .6)
lines(predict(xys,data.frame(x=seq(0,2,by=.01))$x),col=ared)

abline(lsfit(x,y),col=4)

```
:::

## Il modello di regressione

Vogliamo modellare la linea delle medie. Ci aspetteremmo una linea che passi tra i punti che 
che sia, per ogni $x$ dato, il valore atteso di $Y$ condizionato a quella $x$.
Cioè abbiamo bisogno di un modello per la media di $Y$ per un dato $X$
\[E(Y_i|X=x_i)=f(x_i)\]

E si legge che $Y$, in media, condizionato ad $X=x$ vale $f(x)$, dove $f$ è una funzione da scegliere.
L'idea di base è che
\[y_i=f(x_i)+\varepsilon_i\]
la $y$ osservata in corrispondenza della $x$ è pari ad $f(x)$ a meno di un errore casuale.
Dove si assume che l'errore abbia media zero
\[E(\varepsilon_i)=0\]

e varianza costante
\[V(\varepsilon_i)=\sigma^2_{\varepsilon}, ~\forall i\]

Qui di seguito un paio di esempi grafici

```{r 17-regressione-I-3,results='asis'}
opar <- par(no.readonly = T)
par(mfrow=c(1,2),cex=cex)
set.seed(1)
x <- seq(0,3/2*pi,length=100)
y <- 15*sin(x)+rnorm(100,0,2.5)+28

plot(x*2.5,y,pch=16,cex=.6,xlab="x",axes=F)
lines(x*2.5,15*sin(x)+28,col=ared)
axis(1)
axis(2)


text(9,40,expression(f(x)),col=ared)

plot(x*2.5,y,pch=16,cex=.8,xlab="x",xlim = c(2,4),ylim = c(35,48),axes=F)
lines(x*2.5,15*sin(x)+28,col=ared)
points(x*2.5,15*sin(x)+28,col=1,pch=4,cex=.5)
segments(x*2.5,15*sin(x)+28,x*2.5,y,lty=2,col="grey")
axis(1)
axis(2)

par(opar)
```


<!-- ```{r 17-regressione-I-4,results='asis'} -->

<!-- plot(x*2.5,y,pch=16,cex=.8,xlab="x",xlim = c(2,4),ylim = c(35,48)) -->
<!-- lines(x*2.5,15*sin(x)+28,col=ared) -->
<!-- points(x*2.5,15*sin(x)+28,col=1,pch=4,cex=.5) -->
<!-- segments(x*2.5,15*sin(x)+28,x*2.5,y,lty=2,col="grey") -->
<!-- ``` -->

## La Regressione Lineare


Tra le tante $f$ limitiamo l'attenzione alle funzioni lineari
\[y_i=\beta_0+\beta_1x_i+\varepsilon_i\]


```{r 17-regressione-I-5,results='asis'}
opar <- par(no.readonly = T)
par(mfrow=c(1,2),cex=cex)
set.seed(2)
x <- sort(rnorm(50,10,1))
y <- 3.2 + .5*x + rnorm(50,0,.5)
plot(x,y,pch=16,cex=.6,xlab="x = Reddito",ylab="y = Consumo",axes=F)
axis(1)
axis(2)
abline(lsfit(x,y),col=ared)

set.seed(1)
x <- sort(rnorm(50,10,1))
y <- 32 - 1.5*x + rnorm(50,0,.5)
plot(x,y,pch=16,cex=.6,xlab="x = Inflazione",ylab="y = Disoccupazione",axes=F)
axis(1)
axis(2)
abline(lsfit(x,y),col=ared)

par(opar)
```

### Il modello di regressione lineare semplice

Il modello di regressione (lineare semplice) è
\[y_i=\beta_0+\beta_1x_i+\varepsilon_i,\qquad E(\varepsilon_i)=0,~V(\varepsilon_i)=\sigma^2_{\varepsilon_i}\]


::: {.nota data-latex=""}

Le variabili sono chiamate:

- $y$ dipendente, endogena, risposta, ecc.
- $x$ indipendente, esogena, stimolo, ecc.
- $\varepsilon$ è chiamato, residuo, o errore

I parametri di popolazione sono **3**: $\beta_0$, $\beta_1$ e $\sigma^2_\varepsilon$


- $\beta_0$ è l'intercetta della retta. $\beta_0$ rappresenta la media di $y$ quando $x=0$, non sempre ha significato fenomenico
- $\beta_1$ è il coefficiente angolare. $\beta_1$ rappresenta quanto la media di $y$ aumenta all'aumentare unitario della $x$.
- $\sigma^2_\varepsilon$ è la varianza dell'errore $\varepsilon$. $\sigma^2_\varepsilon$ rappresenta la variabilità dei punti intorno alla retta.

```{r 17-regressione-I-16}
fig.def(2)
```

```{r 17-regressione-I-17}
plot(c(-.1,2),c(-.1,2),type="n",xlab="",ylab="",axes=F,asp=1)
#abline(.2,.5,col=ared)
curve(.2+.5*x,-.2,2.1,add=T,col=ared)
arrows(0,-.5,0,1.9,.1)
arrows(-.5,0,1.9,0,.1)
text(1.9,.1,"x")
text(.1,1.9,"y")
points(0,.2,pch=16,col=ared)
text(-.1,.2,expression(beta[0]))
text(1,-.1,1)
points(1,0,pch="|",cex=.8)
segments(1,.2,1,.2+.5,lty=2)
segments(0,.2,1,.2,lty = 2)
brackets(1.01,.7,1.01,.2,xpd = F)
text(1.3,.2+.25,expression(beta[1]))
text(.5,.25,1)
arrows(0,.15,1,.15,length = .05,code = 3)

fig.def(3,8)
```

:::


### La Storia del Metodo


https://en.wikipedia.org/wiki/Regression_toward_the_mean 

```{r 17-regressione-I-18}
par(cex=cex)
galton <- read_dta("data/galton.dta")
hf <- galton[galton$male==1,]$father*2.54
hs <- galton[galton$male==1,]$height*2.54

plot(hf,hs,xlab="statura dei padri",ylab="statura dei figli",pch=4,cex=.6,axes=F,asp=1,xlim=c(155,201),ylim=c(155,201))
mf <- mean(hf)
ms <- mean(hs)

abline(v=mf,lty=2)
abline(h=ms,lty=2)
axis(1,c(160,165,mf,185,190,195,200),c(160,165,paste("media\n",round(mf,2)),185,190,195,200))
axis(2,c(160,165,ms,185,190,195,200),c(160,165,paste("media\n",round(ms,2)),185,190,195,200),las=2)
abline(lsfit(x = hf,y=hs))
abline(0,1,lty=2,col=ared)
text(190,200,"Ipotesi di Galton",col=ared)
text(200,180,"Evidenza \n campionaria",col=1)
arrows(195,194.5,195,97.177+0.4477*195,.09)
title("I dati di Galton")
ab <- round(lsfit(hf,hs)$coef,2)
```

L'ipotesi di Galton:
\[
y_i = x_i +\varepsilon_i
\]
fu smentita dai dati 
\[
y_i = `r ab[1]`+`r ab[2]`\cdot x_i +\varepsilon_i
\]


### Gli assunti del modello di regressione

::: {.info data-latex=""}
0.  Dati $(x_1,y_1),...,(x_n,y_n)$, $n$ coppie di punti, si assume che
\[y_i=\beta_0+\beta_1x_i+\varepsilon_i\]
1. Il valore atteso dell'errore è nullo
\[E(\varepsilon_i)=0\]
2. Omoschedasticità
\[V(\varepsilon_{i}) = \sigma_\varepsilon^2,\qquad \text{costante }\forall i\]
3. Indipendenza dei residui
\[\varepsilon_i\text{ è indipendente da }\varepsilon_j~~\forall i\neq j\]
4. Indipendenza tra i residui e la $X$
\[X_i\text{ è indipendente da }\varepsilon_i~~\forall i\]
5. _Esogeneità_ della $X$: la distribuzione su $X$ non è oggetto di inferenza
6. Normalità dei residui 
\[\varepsilon_i\sim N(0,\sigma^2_\varepsilon)\]
:::

__Assunto 0__ 
Dati $(x_1,y_1),...,(x_n,y_n)$, $n$ coppie di punti, si assume che
\[y_i=\beta_0+\beta_1x_i+\varepsilon_i\]


Situazioni in cui l'assunto 0 è violato 

```{r 17-regressione-I-19}
fig.def(3)
```

```{r 17-regressione-I-20}

set.seed(2)
n <- 100
x <- sort(rnorm(n))
opar <- par(mfrow=c(1,2),cex=cex,mar=c(5.1,4.1,.1,.1))
y <- x^2 + rnorm(n,0,.8)
plot(x,y,pch=16,cex=.5)
curve(x^2,add=T,col=ared)

#set.seed(2)
y <- cos(3*x) + rnorm(n,0,1)
plot(x,y,pch=16,cex=.5)
curve(cos(3*x),add=T,col=ared)

#set.seed(2)
y <- exp(x) + rnorm(n,0,1)
plot(x,y,pch=16,cex=.5)
curve(exp(x),add=T,col=ared)



#set.seed(2)
y <- log(x^2) + rnorm(n,0,1)
plot(x,y,pch=16,cex=.5)
curve(log(x^2),add=T,col=ared)

par(opar)
```


__Assunto 1, il valore atteso dell'errore è nullo__

\[E(\varepsilon_i)=0\]

Inverificabile, se per esempio
\[E(\varepsilon_i)=+1,~~\forall i,~ E(Y_i|x_i)=\beta_0+\beta_1x_i+E(\varepsilon_i)=\beta_0+\beta_1x_i+1\]

```{r 17-regressione-I-21}
par(cex=cex)
set.seed(3)
y <- x + rnorm(n)
plot(c(x,x),c(y,y-1),pch=16,cex=.5,type="n",axes=F,xlab="integratore",ylab="peso del capo")
points(x,y-1,pch=16,col=ared,cex=.5)
points(x,y,pch=16,col=4,cex=.5)
segments(x,y-1,x,y,lty=2,col="grey")
abline(0,1,col=4)
abline(-1,1,col=ared)
arrows(-1.5,-1.5,-1.5,-2.5,length = .05,code = 1)
text(-1.75,-1.75-.5,expression(+1))
text(+1.75,+1.75-.5,expression(+1))
arrows(1.5,1.5,1.5,.5,length = .05,code = 1)
axis(1,-2:2,10:14)
axis(2,-4:3,25:32,las=2)

```


__Assunti 3. e 6. Omoschedasticità e Indipendenza tra i residui e la $X$__

\[V(\varepsilon_{i}) = \sigma_\varepsilon^2,\qquad \text{costante }\forall i\]
\[X_i\text{ è indipendente da }\varepsilon_i~~\forall i\]

La varianza non cambia con le osservazioni

```{r 17-regressione-I-22}
fig.def(3)
```

```{r 17-regressione-I-23}
par(mfrow=c(1,2),cex=cex)
set.seed(3)
y <- x + rnorm(n)
plot(x,y,pch=16,cex=.5)
abline(-1.5,1,lty=2)
abline(+1.5,1,lty=2)
title("Assunti rispettati")

y <- x + rnorm(n,0,exp(abs(x)^1.1))
plot(x,y,pch=16,cex=.5)
abline(-1.5,1,lty=2)
abline(+1.5,1,lty=2)
lines(x,-1.5+x-exp(abs(x)^1.1),lty=2,col=3)
lines(x,+1.5+x+exp(abs(x)^1.1),lty=2,col=3)
title("Assunti non rispettati")

eps <- c(rnorm(40,0,.3),rnorm(60,0,1))
y <- x+ eps

```
```{r 17-regressione-I-24}
plot(x,y,pch=16,cex=.5)
abline(-1.5,1,lty=2)
abline(+1.5,1,lty=2)
abline(-.5,1,lty=2,col=3)
abline(+.5,1,lty=2,col=3)
title("Assunti non rispettati")

y <- x + rnorm(n,0,dnorm(x,0,.5)*3)
plot(x,y,pch=16,cex=.5)
abline(-1.5,1,lty=2)
abline(+1.5,1,lty=2)
lines(x,x+1.5+dnorm(x,0,.5)*3,lty=2,col=3)
lines(x,x-1.5-dnorm(x,0,.5)*3,lty=2,col=3)
title("Assunti non rispettati")

par(mfrow=c(1,1),cex=cex)

```


__4. Indipendenza dei residui__

\[\varepsilon_i\text{ è indipendente da }\varepsilon_j~~\forall i\neq j\]

L'assunto vine tipicamente violato nelle serie temporali

```{r 17-regressione-I-25}
fig.def(3)
```

```{r 17-regressione-I-26}
x <- x - min(x)
y <- x + rnorm(100,0,1)
opar <- par(mfrow=c(1,2),cex=cex)
plot(x,y,pch=16,axes = F)
axis(1);axis(2)
abline(lsfit(x,y),col=ared)
title("Assunto Rispettato")
# plot(x,y,type="l")
# abline(lsfit(x,y),col=ared)
# title("Assunto Rispettato")
set.seed(10)
eps <- arima.sim(model = list(order=c(3,1,1),ar=c(.4,.3,.1),ma=.1),n = 100,sd=.5)
y <- x+ eps[-101]
plot(x,y+10,type="p",pch=16,xlab="Tempo",ylab="Y al tempo t",axes=F)
axis(1)
axis(2)
abline(lsfit(x,y+10),col=ared)
title("Assunto Non Rispettato")
# plot(x,y+10,type="l",pch=16,xlab="Tempo",ylab="Y al tempo t",axes=F)
# axis(1)
# axis(2)
# abline(lsfit(x,y+10),col=ared)
# title("Assunto Non Rispettato")

par(opar)
```

__5. Esogeneità della $X$: la distribuzione su $X$ non è oggetto di inferenza.__
In contesto di sperimentazione la $X$ viene fissata dal ricercatore. 
In un contesto di dati sul campo la $X$ non può essere fissata, ma viene usata 
_come se fosse_ fissata.
L'obiettivo di ricerca rimane la $Y$ e la relazione tra $X$ ed $Y$, **non**
la distribuzione di $X$.

__6. Normalità dei residui__
Gli errori sono normalmente distribuiti
\[\varepsilon_i\sim N\left(0,\sigma_\varepsilon^2\right)\]

Essendo
\[Y_i = \beta_0+\beta_1x_i+\varepsilon_i, ~~\varepsilon_i\sim N\left(0,\sigma_\varepsilon^2\right)\]

allora, dalle proprietà delle normali
\[Y_i\sim  N\left(\beta_0+\beta_1x_i,\sigma_\varepsilon^2\right)\]

$Y_i$ è normale con media $\beta_0+\beta_1x_i$ e varianza $\sigma_\varepsilon^2$

```{r 17-regressione-I-27}
fig.def(4,25)
```

```{r 17-regressione-I-28}
M <- persp(c(0,1.1),c(0,1),matrix(c(0,0,4,4),2),phi = .5,theta = 41,shade = NULL,border = 0,box = F,scale = T)

x1 <- seq(0,1.1,length.out = 101)
y1 <- seq(0,1.1,length.out = 101)

arrows(trans3d(0,0,0,M)$x,trans3d(0,0,0,M)$y,trans3d(0,2,0,M)$x,trans3d(0,2,0,M)$y,.1)
arrows(trans3d(0,0,0,M)$x,trans3d(0,0,0,M)$y,trans3d(2,0,0,M)$x,trans3d(2,0,0,M)$y,.1)
arrows(trans3d(0,0,0,M)$x,trans3d(0,0,0,M)$y,trans3d(0,0,4,M)$x,trans3d(0,0,4,M)$y,.1)
segments(trans3d(c(1,.35,.6),c(0,0,0),c(0,0,0),M)$x,trans3d(c(1,.35,.6),c(0,0,0),c(0,0,0),M)$y,
         trans3d(c(1,.35,.6),c(2,2,2),c(0,0,0),M)$x,trans3d(c(1,.35,.6),c(2,2,2),c(0,0,0),M)$y,
         lty=2,col="grey20")
lines(trans3d(x1,x1/2+.25,0,M)$x,trans3d(x1,x1/2+.25,0,M)$y,col=ared)
lines(trans3d(.35,y1,dnorm(.35/2+.25,y1,.1),M)$x,trans3d(.35,y1,dnorm(y1,.35/2+.25,.1),M)$y,col="grey80")
lines(trans3d(.6,y1,dnorm(.6/2+.25,y1,.1),M)$x,trans3d(.6,y1,dnorm(y1,.6/2+.25,.1),M)$y,col="grey80")
lines(trans3d(1,y1,dnorm(1/2+.25,y1,.1),M)$x,trans3d(1,y1,dnorm(y1,1/2+.25,.1),M)$y,col="grey80")
segments(trans3d(c(.35,.6,1),c(.35,.6,1)/2+.25,c(0,0,0),M)$x,trans3d(c(.35,.6,1),c(.35,.6,1)/2+.25,c(0,0,0),M)$y,
         trans3d(c(.35,.6,1),c(.35,.6,1)/2+.25,c(dnorm(c(.35,.6,1)/2+.25,c(.35,.6,1)/2+.25,.1)),M)$x,
         trans3d(c(.35,.6,1),c(.35,.6,1)/2+.25,c(dnorm(c(.35,.6,1)/2+.25,c(.35,.6,1)/2+.25,.1)),M)$y,
          lty = 2)
text(trans3d(.35,-.05,0,M),expression(x[1]))
text(trans3d(.6,-.05,0,M),expression(x[2]))
text(trans3d(1,-.05,0,M),expression(x[3]))
text(trans3d(-.05,2,0.5,M),expression(y))
text(trans3d(-.1,-.05,3,M),"f(y|x)")
text(trans3d(1.2,1.1/2,0,M),expression(y==beta[0]+beta[1]* x),col=ared)
```

### Il metodo dei minimi quadrati


È un metodo di stima per $\beta_0$ e $\beta_1$, si può dimostrare che, in virtù 
dell'assunto 6. (la normalità dei residui) il metodo dei minimi quadrati produce le
stime di massima verosimiglianza. Svilupperemo la teoria attraverso un esempio su 4 punti.

```{r 17-regressione-I-29}
fig.def(3,6.5)
```


```{r 17-regressione-I-30}
par(cex=cex)
x <- c(0:3)
y <- c(2,3.5,2.5,4)

prt <- data.frame(cbind(dato=1:4,x,y))
names(prt)<- c("$i$","$x_i$","$y_i$")
kable(prt,booktabs = T, escape = F, linesep = "")
x <- c(0:3)
y <- c(2,3.5,2.5,4)
plot(x,y,axes=F,xlim=c(-.5,3.5),pch=16)
axis(1,x)
axis(2,y)
segments(x,0,x,y,lty=2)
segments(-10,y,x,y,lty=2)

#axis(2,2.5+.25*x,las=2)
# abline(2.45,.25,col=ared)
# segments(x,y,x,2.45+.25*x,lty=2)
# text(x+.25,(y+2.45+.25*x)/2,expression(epsilon[1],epsilon[2],epsilon[3],epsilon[4]))
# segments(-10,2.45+.25*x,x,2.45+.25*x,lty=2)
# segments(x,y,-10,y,lty=2)
# text(3,3.0,expression(y==beta[0]+beta[1] * x),col=ared)

```

### La distanza di una retta dai punti (il metodo dei minimi quadrati)

Si tratta di cercare la retta che rende minima la somma dei quadrati delle distanza 
della retta misurate nella scala di misura della $y$.
Per ogni retta candidata $(\beta_0^*,\beta_1^*)$, costruiamo la previsione
\[\hat y_i^*=\beta_0^*+\beta_1^*x_i\]
e quindi i residui 
\[\hat\varepsilon_i^*=y_i-\hat y_i\]


Il criterio dei **minimi quadrati** è
\[G(\beta_0^*,\beta_1^*)=\sum_{i=1}^n ({\hat\varepsilon_i^*})^{2} =\sum_{i=1}^n(y_i-\hat y_i^*)^2=\sum_{i=1}^n(y_i-(\beta_0^*+\beta_1^*x_i))^2\]


La retta dei minimi quadrati è quella coppia di $\hat\beta_0$ e $\hat\beta_1$ tali che
\[G(\hat\beta_0,\hat\beta_1)<G(\beta_0^*,\beta_1^*),~~\forall(\beta_0^*,\beta_1^*)\neq(\hat\beta_0,\hat\beta_1)\]

Se, per esempio, scegliessimo $\beta_0^*=2.2,~\beta_1^*=0.36$, otterremmo 

```{r 17-regressione-I-31}
fig.def(3)
```


```{r 17-regressione-I-32}
opar <- par(mfrow=c(1,2),cex=cex)
x <- c(0:3)
y <- c(2,3.5,2.5,4)
bs0 <- 2.2
bs1 <- .36
yh <- bs0+bs1*x
epsh <- y-yh

plot(x,y,axes=F,xlim=c(-.5,3.5),pch=16)
axis(1,x)
axis(2,y,las=2)
axis(2,bs0+bs1*x,las=2,col.axis=6)
abline(bs0,bs1,col=6,lwd=2)
segments(x,y,x,bs0+bs1*x,lty=2,col=1:4)
text(x+.25,(y+bs0+bs1*x)/2,epsh)
segments(-10,bs0+bs1*x,x,bs0+bs1*x,lty=2,col=1:4)
segments(x,y,-10,y,lty=2,col=1:4)
text(3,3.0,expression(y==2.2+0.36 * x),col=6)

prn <- data.frame(1:4,`$x_i$`=x,`$y_i$`=y,`$\\hat y_i^*=\\beta_0^*+\\beta_1^*x_i$`=yh,`$\\hat\\varepsilon_i=y_i-\\hat y_i^*$`=y-yh,(y-yh)^2)
names(prn)<-c("$i$", "$x_i$","$y_i$","$\\hat y_i^*=\\beta_0^*+\\beta_1^*x_i$","$\\hat\\varepsilon_i^*=y_i-\\hat y^*$","$({\\hat\\varepsilon_i^*})^2$")
kable(prn,booktabs = T, escape = F, linesep = "")


g1 <- sum((y-yh)^2)

x <- c(0:3)
y <- c(2,3.5,2.5,4)
bs0 <- 3.2
bs1 <- -.15
yh <- bs0+bs1*x
epsh <- y-yh

plot(x,y,axes=F,xlim=c(-.5,3.5),pch=16)
axis(1,x)
axis(2,y,las=2)
axis(2,bs0+bs1*x,las=2,col.axis=6)
abline(bs0,bs1,col=6,lwd=2)
segments(x,y,x,bs0+bs1*x,lty=2,col=1:4)
text(x+.25,(y+bs0+bs1*x)/2,epsh)
segments(-10,bs0+bs1*x,x,bs0+bs1*x,lty=2,col=1:4)
segments(x,y,-10,y,lty=2,col=1:4)
text(3,3.0,expression(y==3.2-0.15 * x),col=6)

prn <- data.frame(1:4,`$x_i$`=x,`$y_i$`=y,`$\\hat y_i^*=\\beta_0^*+\\beta_1^*x_i$`=yh,`$\\hat\\varepsilon_i=y_i-\\hat y_i^*$`=y-yh,(y-yh)^2)
names(prn)<-c("$i$", "$x_i$","$y_i$","$\\hat y_i^*=\\beta_0^*+\\beta_1^*x_i$","$\\hat\\varepsilon_i^*=y_i-\\hat y^*$","$({\\hat\\varepsilon_i^*})^2$")
kable(prn,booktabs = T, escape = F, linesep = "")

g2 <- sum((y-yh)^2)


dxy <- function(x,y) paste(paste("(",paste(x,mean(x),sep = "-"),")"),paste("(",paste(y,mean(y),sep = "-"),")"),collapse = "+")
pxy <- function(x,y) paste(x,"\\times",y,collapse = "+")
par(opar)
```

\[G(2.2,0.36)=`r sum((y-yh)^2)`\]

Se invece scegliessimo  $\beta_0^*=3.2,~\beta_1^*=-1.5$, otterremmo

\[G(3.2,-1.15)=`r sum((y-yh)^2)`\]

Siccome
\[G(2.2,0.36)=`r g1`<G(3.2,-1.15)=`r g2`\]
allora diremo che la retta $\beta_0^*=2.2,~\beta_1^*=0.36$ è _più vicina_, nel senso dei minimi quadrati, della retta $\beta_0^*=3.2,~\beta_1^*=-1.5$.

### Soluzioni dei minimi quadrati

La retta dei minimi quadrati è quella coppia di $\hat\beta_0$ e $\hat\beta_1$ tali che
\[G(\hat\beta_0,\hat\beta_1)<G(\beta_0,\beta_1),~~\forall(\beta_0,\beta_1)\neq(\hat\beta_0,\hat\beta_1)\]
ovvero
\begin{eqnarray*}
(\hat\beta_0,\hat\beta_1)=\operatorname*{argmin}_{(\beta_0,\beta_1)\in\mathbb{R}^2}G(\beta_0,\beta_1)
\end{eqnarray*}

:::: {.info data-latex=""}
::: {.proposition name="Stimatori dei Minimi Quadrati"}
Gli stimatori dei minimi quadrati $\hat\beta_0$ e $\hat\beta_1$ sono
\begin{eqnarray*}
\hat\beta_1 &=& \frac{\frac 1 n\sum_{i=1}^n{(x_i-\bar x)(y_i-\bar y)}}{\frac 1 n\sum_{i=1}^n(x_i-\bar x)^2}=\frac{\text{ cov}(x,y)}{\hat\sigma^2_X}\\
\hat\beta_0 &=&\bar y -\hat\beta_1\bar x
\end{eqnarray*}
dove
\[\bar y = \frac 1 n\sum_{i=1}^n y_i,\qquad \bar x=\frac 1 n \sum_{i=1}^n x_i\]
e
\[
\text{ cov}(x,y) = \frac 1 n\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)
\]
:::
::::

::: {.proof}
\begin{eqnarray*}
  G(\beta_{0}, \beta_{1}) &=& \frac 1n \sum_{i=1}^{n} \left( y_{i} - (\beta_{0} + \beta_{1} x_{i})   \right)^2\\ 
  \frac{\partial G(\beta_{0}, \beta_{1})} {\partial \beta_{0}} 
&=& -\frac{2} {n} \sum_{i=1}^n  \left( y_{i} - (\beta_{0} + \beta_{1} x_{i})
    \right)\\
&=& -\frac{2} n\left(\sum_{i=1}^ny_i-\sum_{i=1}^n\beta_0-\sum_{i=1}^n\beta_1 x_i\right) \\
&=&-\frac{2} n\left(n\bar y-n\beta_0-n\beta_1\bar x\right) \\
&=&-2\left(\bar y-\beta_0-\beta_1\bar x\right) \\
    \frac{\partial G(\beta_{0}, \beta_{1})}  {\partial \beta_{1}}
&=& -\frac{2} {n} \sum_{i=1}^n  x_i\left( y_{i} - (\beta_{0} + \beta_{1} x_{i})
    \right)\\
&=& -\frac{2} {n} \left(\sum_{i=1}^n  x_i y_{i}\right) - \left(\sum_{i=1}^n\beta_{0}x_i\right) 
- \left(\sum_{i=1}^n\beta_{1} x_{i}^2\right)\\
&=& -2 \left(\left(\frac 1n\sum_{i=1}^n  x_i y_{i}\right) - \beta_{0}\bar x
- \beta_{1}\left(\frac 1n\sum_{i=1}^n x^2_{i}\right)\right)\\
\end{eqnarray*}

E otteniamo il __sistema di equazioni normali__

\[
\left\{
\begin{array}{rl} 
 \frac{\partial G(\beta_{0}, \beta_{1})} {\partial \beta_{0}} &= 0  \\
  \frac{\partial G(\beta_{0}, \beta_{1})} {\partial \beta_{1}} &= 0
\end{array}\right.
\]

Dividendo per 2 e cambiando di segno, si ha
\[
\left\{
\begin{array}{rl} 
\left(\bar y-\beta_0-\beta_1\bar x\right)  &= 0\\
\left(\frac 1n\sum_{i=1}^n  x_i y_{i}\right) - \beta_{0}\bar x
- \beta_{1}\left(\frac 1n\sum_{i=1}^n x^2_{i}\right) &= 0
\end{array}\right.
\]
da cui
\[
    \hat\beta_0 = \bar y- \beta_{1} \bar{x}                  
\]

E sostituiamo

\begin{eqnarray*}
\left(\frac 1n\sum_{i=1}^n  x_i y_{i}\right) - (\bar y - \beta_1\bar x)\bar x
- \beta_{1}\left(\frac 1n\sum_{i=1}^n x^2_{i}\right) &=& 0\\
\beta_1\left(\frac 1n\sum_{i=1}^n x^2_{i}-\bar x^2\right) &=&
\frac 1n\sum_{i=1}^n x_i y_{i} - \bar x\cdot\bar y\\
\hat\beta_1 &=& \frac{\frac 1n\sum_{i=1}^n x_i y_{i} - \bar x\cdot\bar y}{\frac 1n\sum_{i=1}^n x^2_{i}-\bar x^2}
\end{eqnarray*}

Siccome 
\begin{eqnarray*}
   \text{ cov}(x,y) &=& \frac 1 n\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)\\
   &=& \frac 1 n\sum_{i=1}^n(x_i~y_i - y_i~\bar x - x_i~\bar y + \bar x\bar y)\\
   &=& \frac 1 n\sum_{i=1}^n x_i~y_i - \frac {\bar x} n\sum_{i=1}^n y_i - \frac {\bar y} n\sum_{i=1}^nx_i + \frac 1 n\sum_{i=1}^n\bar x\bar y\\
   &=& \frac 1 n\sum_{i=1}^n x_i~y_i-\bar x\bar y-\bar x\bar y+-\bar x\bar y\\
   &=& \frac 1 n\sum_{i=1}^n x_i~y_i-\bar x\bar y
\end{eqnarray*}

Concludendo la prova.
:::

## La covarianza

:::: {.info data-latex=""}
::: {.definition}
La Covarianza $\text{cov}(x,y)$ tra due variabili $x$ e $y$ è una misura della loro _covariazione_
\[\text{cov}(x,y)=\frac 1 n\sum_{i=1}^n{(x_i-\bar x)(y_i-\bar y)}\]
:::
::::

La covarianza è la _media degli scarti incrociati dalla media_
Come per la varianza esiste una formula semplificata:

:::: {.info data-latex=""}
::: {.proposition}
\[\text{cov}(x,y)=\frac 1 n\sum_{i=1}^nx_i~y_i-\bar x\bar y\]
:::
::::

:::: {.nota data-latex=""}
La covarianza è _la media dei prodotti degli scarti dalla media_ può essere riletta come _la media dei prodotti_ meno _il prodotto delle medie_.
::::

### Calcolo della covarianza

Nel caso in esame
\[\bar x=\frac 1 4(`r paste(x,collapse="+")`)=\frac{`r sum(x)`}{4}=`r mean(x)`\]
\[\bar y=\frac 1 4(`r paste(y,collapse="+")`)=\frac{`r sum(y)`}{4}=`r mean(y)`\]


Per la covarianza:
\begin{eqnarray*}
\text{cov}(x,y)&=&\frac 1 4\Big(`r dxy(x,y)`\Big)\\
&=& \frac 1 4(`r pxy(x,y)`)-`r mean(x)`\times`r mean(y)`\\
&=& \frac 1 4(`r sum(x*y)`)-`r mean(x)*mean(y)`\\
&=& `r mean((x-mean(x))*(y-mean(y)))`
\end{eqnarray*}


### Interpretazione della Covarianza

La covarianza non è direttamente leggibile perché ha un'unità di misura mista,
prodotto della unità di misura della $x$ e quella della $y$.

È interessante il segno della covarianza perché indica la pendenza della retta se la covarianza è positiva c'è **concordanza dei segni**, ad $x$ maggiori del loro baricentro ($(x_i-\bar x)>0$) corrispondo, _in media_, $y$ maggiori del loro baricentro $(y_i-\bar y)>0$. Mentre ad $x$ minori del loro baricentro ($(x_i-\bar x)<0$) corrispondo, _in media_, $y$ minori del loro baricentro $(y_i-\bar y)<0$. Se invece la covarianza è negativa c'è  **discordanza dei segni**, cioè ad $x$ maggiori del loro baricentro ($(x_i-\bar x)>0$) corrispondo, _in media_, $y$ minori del loro baricentro $(y_i-\bar y)<0$. Mentre ad $x$ minori del loro baricentro ($(x_i-\bar x)<0$) corrispondo, _in media_, $y$ maggiori del loro baricentro $(y_i-\bar y)>0$. 

```{r 17-regressione-I-33}
fig.def(3)
```


```{r 17-regressione-I-34}
set.seed(2)
opar <- par(mfrow=c(1,2),cex=cex)
x <- sort(rnorm(100))
x <- x-min(x)
y <- x+rnorm(100,0,.5)
y <- y - min(y)

mx <- mean(x)
my <- mean(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(cov(x,y)>0))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mx,expression(hat(mu)[X]))
axis(2,my,expression(hat(mu)[Y]))

text(mean(x)+c(-1,1),mean(y)+c(-1,1)*2,"+",cex=3,col=3)
text(mean(x)+c(1,-1),mean(y)+c(-1,1)*2,"-",cex=3,col=ared)

y <- -x+rnorm(100,0,.5)
y <- y - min(y)

mx <- mean(x)
my <- mean(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(cov(x,y)<0))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mx,expression(hat(mu)[X]))
axis(2,my,expression(hat(mu)[Y]))

text(mean(x)+c(-1,1),mean(y)+c(-1,1)*2,"+",cex=3,col=3)
text(mean(x)+c(1,-1),mean(y)+c(-1,1)*2,"-",cex=3,col=ared)

y <- rnorm(100,0,.5)
y <- y - min(y)

mx <- mean(x)
my <- mean(y)
```
```{r 17-regressione-I-35}
plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(cov(x,y) %~~% 0))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mx,expression(hat(mu)[X]))
axis(2,my,expression(hat(mu)[Y]))

text(mean(x)+c(-1,1),mean(y)+c(-1,1)*1,"+",cex=3,col=3)
text(mean(x)+c(1,-1),mean(y)+c(-1,1)*1,"-",cex=3,col=ared)

y <- (x-mean(x))^2+rnorm(100,0,.5)
y <- y - min(y)

mx <- mean(x)
my <- mean(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(cov(x,y) %~~% 0))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mx,expression(hat(mu)[X]))
axis(2,my,expression(hat(mu)[Y]))

text(mean(x)+c(-1,1),mean(y)+c(-1,1)*2,"+",cex=3,col=3)
text(mean(x)+c(1,-1),mean(y)+c(-1,1)*2,"-",cex=3,col=ared)

par(opar)
```

### Altre proprietà della covarianza

Simmetria
\[\text{cov}(x,y)=\text{cov}(y,x)\]
la dimostrazione è immediata
\[\text{cov}(x,y)=\frac 1 n\sum_{i=1}^n{(x_i-\bar x)(y_i-\bar y)}=\frac 1 n\sum_{i=1}^n{(y_i-\bar y)(x_i-\bar x)}=\text{cov}(y,x)\]

Caso particolare
\[\text{cov}(x,x)=\frac 1 n\sum_{i=1}^n{(x_i-\bar x)(x_i-\bar x)}=\frac 1 n\sum_{i=1}^n{(x_i-\bar x)^2}=\hat\sigma^2_X\]
la covarianza di una variabile con se stessa è la varianza.
La covarianza estende il concetto di varianza quando le variabili sono due.

### Campo di variazione della covarianza

\[-\hat\sigma_X\hat\sigma_Y\leq\text{cov}(x,y)\leq + \hat\sigma_X\hat\sigma_Y\]

```{r 17-regressione-I-36}
set.seed(2)
opar <-par(mfrow=c(1,2),cex=cex)
x <- sort(rnorm(100))
x <- x-min(x)
y <- x+rnorm(100,0,.5)
y <- y - min(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(list(0<cov(x,y)) < hat(sigma)[X]*hat(sigma)[y]))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mean(x),expression(bar(x)))
axis(2,mean(y),expression(bar(y)))

y <- x

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(cov(x,y) == hat(sigma)[X]*hat(sigma)[y]))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mean(x),expression(bar(x)))
axis(2,mean(y),expression(bar(y)))
```

```{r 17-regressione-I-37}
y <- -x+rnorm(100,0,.5)
y <- y - min(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(list(- hat(sigma)[X]*hat(sigma)[y]<cov(x,y))<0))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mean(x),expression(bar(x)))
axis(2,mean(y),expression(bar(y)))

y <- -x
y <- y - min(y)


plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(cov(x,y) == - hat(sigma)[X]*hat(sigma)[y]))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mean(x),expression(bar(x)))
axis(2,mean(y),expression(bar(y)))
par(opar)
```



Stimiamo di $\sigma_X$ e $\sigma_Y$

```{r 17-regressione-I-38}
x <- c(0:3)
y <- c(2,3.5,2.5,4)

quad <- paste(paste(x,collapse = "^2+"),"^2",sep="")

```


Calcolo di $\hat\sigma_X^2$
\begin{eqnarray*}
\hat\sigma_X^2&=&\frac 1 n\sum_{i=1}^n x_i^2-\hat\mu_X^2\\
&=&\frac 1 {4}(`r quad`)-`r mean(x)`^2\\
&=&`r s2c(x)`\\
\hat\sigma_X &=& `r sc(x)`
\end{eqnarray*}

```{r 17-regressione-I-39}
x <- c(0:3)
y <- c(2,3.5,2.5,4)

quad <- paste(paste(y,collapse = "^2+"),"^2",sep="")

```


Calcolo di $\hat\sigma_X^2$
\begin{eqnarray*}
\hat\sigma_Y^2&=&\frac 1 n\sum_{i=1}^n y_i^2-\hat\mu_Y^2\\
&=&\frac 1 {4}(`r quad`)-`r mean(y)`^2\\
&=&`r s2c(y)`\\
\hat\sigma_Y &=& `r sc(y)`
\end{eqnarray*}

### Calcolo in colonna


::: {.nota data-latex=""}
Molto più comodamente in colonna

\begin{center}
```{r 17-regressione-I-40}
Dato <- c(1:4,"Totale","Totale/n")
prn <- data.frame(Dato,x=c(x,0,0),y=c(y,0,0),x2 = c(x^2,0,0),y2 = c(y^2,0,0),w=c(x*y,0,0))
prn[5,2:6] <- colSums(prn[1:4,2:6])
prn[6,2:6] <- colMeans(prn[1:4,2:6])

names(prn)<-c("$i$","$x_i$","$y_i$","$x_i^2$","$y_i^2$","$x_i\\cdot y_i$") 

kable(prn,booktabs = T, escape = F, linesep = "")%>%
  row_spec(5, bold = T, color = "white", background = "gray")

mx <- mean(x)
my <- mean(y)
sx <- sc(x)
sy <- sc(y)
cv <- mean((x-mx)*(y-my))
b1 <- cv/(sx^2)
b0 <- my - b1*mx
```
\end{center}
\begin{eqnarray*}
\hat\sigma_X^2&=&\frac 1 n\sum_{i=1}^nx_i^2-\bar x^2=`r mean(x^2)`-`r mean(x)`^2=`r s2c(x)`\\
\hat\sigma_Y^2&=&\frac 1 n\sum_{i=1}^ny_i^2-\bar y^2=`r mean(y^2)`-`r mean(y)`^2=`r s2c(y)`\\
\text{cov}(x,y)&=&\frac 1 n\sum_{i=1}^nx_i~y_i-\bar x\bar y=`r mean(x*y)`-`r mean(x)`\cdot`r mean(y)`=`r cv`
\end{eqnarray*}
:::

### Calcolo di $\hat\beta_0$ e $\hat\beta_1$

La stima del coefficiente angolare è
\[\hat\beta_1 =\frac{\text{cov}(x,y)}{\hat\sigma_X^2}=\frac{`r mean((x-mean(x))*(y-mean(y)))`}{`r s2c(x)`}=`r b1`\]

La stima dell'intercetta è
\[\hat\beta_0=\bar y-\hat\beta_1\bar x=`r my`-`r b1`\cdot`r mx`=`r b0`\]

La retta stimata

```{r 17-regressione-I-41}
fig.def(3,3)
```

```{r 17-regressione-I-42}
x <- c(0:3)
y <- c(2,3.5,2.5,4)

plot(x,y,axes = F,pch=16,asp=1)
abline(lsfit(x,y),col=4)
axis(1,x,pos=NA)
axis(2,y,las=2,pos=0)
axis(2,b0,expression(hat(beta)[0]==2.25),las=2,pos = 0)

segments(0,b0,1,b0,lty=2,col="grey")
segments(1,b0,1,b0+b1,lty=2,col="grey")
#brackets(1.01,.7,1.01,.2,xpd = F)
text(1,b0+b1/2,expression(hat(beta)[1]==0.5),pos = 4)

```

## Proprietà della retta dei minimi quadrati


La retta dei minimi quadrati passa per il baricentro di ($x$,$y$) che è ($\bar x$, $\bar y$)
\begin{eqnarray*}
\hat\beta_0+\hat\beta_1\bar x &=&(\bar y-\hat\beta_1\bar x)+\hat\beta_1\bar x\\
       &=& \bar y
\end{eqnarray*}
```{r 17-regressione-I-43}
plot(x,y,axes = F,pch=16,asp=1)
abline(lsfit(x,y),col=4)
axis(1,x,pos=NA)
axis(1,mx,expression(hat(mu)[x]==1.5))
axis(2,y,las=2,pos=0)
axis(2,my,expression(hat(mu)[Y]==3),las=2,pos=0)
abline(v=mx,lty=2)
abline(h=my,lty=2)
```


Le previsioni, sulle $x$ osservate
\[\hat y_i=\hat\beta_0+\hat\beta_1x_i\]

Le stime degli errori
\[\hat\varepsilon_i = y_i-\hat y_i\]

Valgono le seguenti proprietà:

::: {.info data-latex=""}
$$
\begin{aligned}
y_i, & & \text{le $y$ osservate}\\
\hat y_i &= \hat \beta_0+\hat\beta_1x_i,&\text{le $y$ stimate}\\
\hat\varepsilon_i &= y_i-\hat y_i,&\text{gli errori stimati}\\
\bar y &= \frac 1 n\sum_{i=1}^n y_i, &\text{la media degli $y$}\\
\bar y &= \frac 1 n\sum_{i=1}^n \hat y_i, &\text{la media degli $\hat y$ coince con qeulla degli $y$}\\
0 &=\frac 1 n\sum_{i=1}^n\hat\varepsilon_i , &\text{la media degli scarti dalla retta è zero}
\end{aligned}
$$
:::

### Calcolo di $\hat y_i$ e $\hat\varepsilon_i$

```{r 17-regressione-I-44}
fig.def(3,3)
```


```{r 17-regressione-I-45}
yh <- b0 + b1*x
eh <- y  - yh

Dato <- c(1:4,"Totale","Totale/n")
prn <- data.frame(Dato,x=c(x,0,0),y=c(y,0,0),yh = c(yh,0,0),eh = c(eh,0,0))
prn[5,2:5] <- colSums(prn[1:4,2:5])
prn[6,2:5] <- colMeans(prn[1:4,2:5])

plot(x,y,axes=F,pch=16,xlim=c(0,3.5))
axis(1,x)
axis(2,y,las=2)
axis(2,yh,round(yh,2),las=2,col.axis=4)
abline(b0,b1,col=4)
segments(x,0,x,y,col=1:4,lty=2)
segments(x,y,0,y,col=1:4,lty=2)
names(prn)<-c("$i$","$x_i$","$y_i$","$\\hat y_i = 2.25 + 0.5 x_i$","$\\hat \\varepsilon_i$") 
segments(x,0,x,yh,col=1:4,lty=3)
segments(x,yh,0,yh,col=1:4,lty=3)
points(x,yh,pch=16,col=4)
arrows(x+.05,y,x+.05,yh,length = .05,code = 3)
text(x+.1,(y+yh)/2,round(eh,2),pos = 4)
kable(prn,booktabs = T, escape = F, linesep = "")%>%
  row_spec(5, bold = T, color = "white", background = "gray")

```


## Il coefficiente di Correlazione

Quanto si adatta bene la retta ai punti?
\begin{alignat*}{4}
-\hat\sigma_X\hat\sigma_Y&\leq~\text{cov}(x,y)&\leq& + \hat\sigma_X\hat\sigma_Y&\\
-\frac{\hat\sigma_X\hat\sigma_Y}{\hat\sigma_X\hat\sigma_Y}&\leq\frac{\text{cov}(x,y)}{\hat\sigma_X\hat\sigma_Y}&\leq& +\frac{\hat\sigma_X\hat\sigma_Y}{\hat\sigma_X\hat\sigma_Y}&\\
-1&\leq ~~~~~~~r &\leq& +1&
\end{alignat*}

::: {.info data-latex=""}
::: {.definition}
Il coefficiente $r$ 
\[r=\frac{\text{cov}(x,y)}{\hat\sigma_X\hat\sigma_Y}\]
è chiamato _coefficiente di correlazione_.
:::
::::

Nel nostro esempio
\[r=\frac{`r cv`}{`r sx`\cdot`r sy`}=`r cv/(sx*sy)`\]


$r$ misura _l'associazione lineare_ ovvero CRESCE in valore assoluto quando
i punti SI ADDENSANO intorno alla retta.

### Proprietà di $r$

::: {.info data-latex=""}
1. $-1 \le r \le 1$.  Il segno indica la direzione della relazione;
    + $r>0$, al crescere di $X$, _in media_, cresce $Y$;
    + $r<0$, al crescere di $X$, _in media_, decresce $Y$;
    + $r=1$, associazione perfetta diretta;
    + $r=-1$, associazione perfetta indiretta.

2. $r$ è un numero puro, ovvero è privo di unità di misura
3. è simmetrico: $r_{XY} = r_{YX} = r$
4. è invariante per cambiamenti di scala:
\[\text{se }W=a+bY,\text{allora }r_{X,W}=\text{sign}(b) r_{XY},\text{ dove la funzione sign}(b)=
\begin{cases}+1, &\text{se $b>0$}\\
             -1, &\text{se $b<0$}
\end{cases}\]
5. $r$ misura l'associazione lineare:
    + $r$ misura come i punti si addensano intorno alla retta.
    + $f(x)$ **non lineare** $r$ è parzialmente inutile
    + il valore di $r$, da solo, non è in grado di descrivere tutte le possibili relazioni
che si possono realizzare tra due variabili.
6. $r$ è più elevato se i dati sono aggregati in medie o percentuali
:::

**1.** La prima proprietà ci dice che $r$ non può mai essere minore di $-1$ e mai 
maggiore di $+1$ per costruzione

\[-1 \le r \le 1\]

```{r 17-regressione-I-46}
fig.def(3)
```

```{r 17-regressione-I-47}
set.seed(2)
opar <- par(mfrow=c(1,2),cex=cex)
x <- sort(rnorm(100))
x <- x-min(x)
y <- x+rnorm(100,0,.5)
y <- y - min(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(list(0<r) < +1))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mean(x),expression(bar(x)))
axis(2,mean(y),expression(bar(y)))

y <- x

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(r == +1))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mean(x),expression(bar(x)))
axis(2,mean(y),expression(bar(y)))
```

```{r 17-regressione-I-48}

y <- -x+rnorm(100,0,.5)
y <- y - min(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(list(- 1<r)<0))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mean(x),expression(bar(x)))
axis(2,mean(y),expression(bar(y)))

y <- -x
y <- y - min(y)


plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(r == - 1))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mean(x),expression(bar(x)))
axis(2,mean(y),expression(bar(y)))
par(opar)
```


**2.** La seconda proprietà ci dice che $r$ non dipende dalla scala di misura delle variabili
$X$ e $Y$. Mentre la covarianza ha un'unità di misura spuria, prodotto 
dell'unità di misura della $x$ ($u_X$) e della $y$ ($u_Y$), $r$ non
ha unità di misura perché è un rapporto tra la covarianza che porta l'unità di misura della $X$
moltiplicata quella della $Y$ e le standard deviation delle stesse.

\begin{eqnarray*}
\text{cov}(x,y)&=&\frac 1 n\sum_{i=1}^n{(x_i\cdot u_X-\bar x\cdot u_X)(y_i\cdot u_Y-\bar y\cdot u_Y)}\\
  &=&\frac 1 n\sum_{i=1}^n{(x_i-\bar x)u_X(y_i-\bar y)u_Y}\\
  &=&\frac 1 n\sum_{i=1}^n{(x_i-\bar x)(y_i-\bar y)u_X\cdot u_Y}  \\
  &=&\text{cov}(x,y)u_X\cdot u_Y
\end{eqnarray*}

Le standard deviation sono espresse nell'unità di misura delle variabili

\begin{eqnarray*}
\hat\sigma_Y &=& \sqrt{\frac 1 n\sum_{i=1}^n{(y_i\cdot u_Y-\bar y\cdot u_Y)^2}}\\
  &=& \sqrt{\frac 1 n\sum_{i=1}^n{((y_i-\bar y)u_Y)^2}}\\
  &=& \left(\sqrt{\frac 1 n\sum_{i=1}^n{(y_i-\bar y)^2}}\right)u_Y\\
  &=& \hat\sigma_Y u_Y\\
\hat\sigma_X &=& \hat\sigma_X u_X
\end{eqnarray*}

E quindi $r$
\[r=\frac{\text{cov}(x,y)}{\hat\sigma_X\hat\sigma_Y}=\frac{\text{cov}(x,y)u_X\cdot u_Y}{\hat\sigma_X\cdot u_X\hat\sigma_Y\cdot u_Y}=
\frac{\text{cov}(x,y){u_X}\cdot u_Y}{\hat\sigma_X\cdot u_X\hat\sigma_Y\cdot u_Y}\]

**3.** La terza proprietà deriva direttamente dalla simmetria della covarianza:
\[r_{XY}=\frac{\text{cov}(x,y)}{\hat\sigma_X\hat\sigma_Y}=\frac{\text{cov}(y,x)}{\hat\sigma_Y\hat\sigma_X}=r_{YX}\]


**4.** La quarta proprietà ci dice che, essendo un numero puro, $r$, 
non dipende dalle unità di misura di $X$ ed $Y$ e quindi
cambiarla non comporta alterazioni su $r$.

\[\text{se }W=a+bY,\text{allora }r_{X,W}=\text{sign}(b) r_{XY},\text{ dove la funzione sign}(b)=
\begin{cases}+1, &\text{se $b>0$}\\
             -1, &\text{se $b<0$}
\end{cases}\]

```{r 17-regressione-I-49}
set.seed(2)
par(mfrow=c(1,2),cex=cex)
x <- sort(rnorm(100))
x <- x-min(x)
y <- x+rnorm(100,0,.5)
y <- y - min(y)

plot(x,y,axes = F,pch=16)
title("Dati originari")
axis(1)
axis(2)
abline(lsfit(x,y),col=4)
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
text(1,4,paste("r =",round(cor(x,y),2)))

w <- 1 + 2*y
plot(x,w,axes = F,pch=16,ylab="w")
title("W = 1 + 2Y")

axis(1)
axis(2)
abline(lsfit(x,w),col=4)
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(w),lty=2,col="grey")
text(1,8,paste("r =",round(cor(x,w),2)))
```

```{r 17-regressione-I-50}
w <- 2 - 0.5 * y

plot(x,w,axes = F,pch=16,ylab="w")
title("W = 2 - 0.5  y")

axis(1)
axis(2)
abline(lsfit(x,w),col=4)
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(w),lty=2,col="grey")
text(1,0,paste("r =",round(cor(x,w),2)))

v <- 100 + 10 * x
w <- 500 - 100*y

plot(v,w,axes = F,pch=16)
title("V = 100 + 10 X \n W = 500 - y*100 ")

axis(1)
axis(2)
abline(lsfit(v,w),col=4)
abline(v=mean(v),lty=2,col="grey")
abline(h=mean(w),lty=2,col="grey")
text(110,100,paste("r =",round(cor(v,w),2)))
par(mfrow=c(1,1),cex=cex)
```

**5.** La quinta proprietà dice che $r$ misura l'associazione lineare, ovvero 
Il coefficiente di correlazione $r$ è una misura della distanza dei punti da una retta.

  - $r$ misura come i punti si addensano intorno alla retta.
  - $f(x)$ **non lineare** $r$ è parzialmente inutile
  - il valore di $r$, da solo, non è in grado di descrivere tutte le possibili relazioni
che si possono realizzare tra due variabili.

```{r 17-regressione-I-51}
set.seed(2)
par(mfrow=c(1,2),cex=cex)
x <- sort(rnorm(100))
x <- x-min(x)
y <- x+rnorm(100,0,.5)
y <- y - min(y)

mx <- mean(x)
my <- mean(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(r>0))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mx,expression(hat(mu)[X]))
axis(2,my,expression(hat(mu)[Y]))

text(mean(x)+c(-1,1),mean(y)+c(-1,1)*2,"+",cex=3,col=3)
text(mean(x)+c(1,-1),mean(y)+c(-1,1)*2,"-",cex=3,col=ared)

y <- -x+rnorm(100,0,.5)
y <- y - min(y)

mx <- mean(x)
my <- mean(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(r<0))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mx,expression(hat(mu)[X]))
axis(2,my,expression(hat(mu)[Y]))

text(mean(x)+c(-1,1),mean(y)+c(-1,1)*2,"+",cex=3,col=3)
text(mean(x)+c(1,-1),mean(y)+c(-1,1)*2,"-",cex=3,col=ared)
```

```{r 17-regressione-I-52}

y <- rnorm(100,0,.5)
y <- y - min(y)

mx <- mean(x)
my <- mean(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(r %~~% 0))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mx,expression(hat(mu)[X]))
axis(2,my,expression(hat(mu)[Y]))

text(mean(x)+c(-1,1),mean(y)+c(-1,1)*1,"+",cex=3,col=3)
text(mean(x)+c(1,-1),mean(y)+c(-1,1)*1,"-",cex=3,col=ared)

y <- (x-mean(x))^2+rnorm(100,0,.5)
y <- y - min(y)

mx <- mean(x)
my <- mean(y)

plot(x,y,axes = F,pch=16)
arrows(0,0,max(x),0,.05)
arrows(0,0,0,max(y),.05)
abline(lsfit(x,y),col=4)
title(expression(r %~~% 0))
abline(v=mean(x),lty=2,col="grey")
abline(h=mean(y),lty=2,col="grey")
axis(1,mx,expression(hat(mu)[X]))
axis(2,my,expression(hat(mu)[Y]))

text(mean(x)+c(-1,1),mean(y)+c(-1,1)*2,"+",cex=3,col=3)
text(mean(x)+c(1,-1),mean(y)+c(-1,1)*2,"-",cex=3,col=ared)
par(mfrow=c(1,1),cex=cex)
```



**6.** Infine $r$ è più elevato se i dati sono aggregati in medie o percentuali. Infatti se aggreghiamo, come nell'esempio qui sotto, i dati di tre paesi nelle loro medie, diminuiamo la variabilità.

```{r 17-regressione-I-53}
fig.def(3,3)
```


```{r 17-regressione-I-54}
set.seed(9)
x1 <- rnorm(n1 <- 15,10)
x2 <- rnorm(n2 <- 10,12)
x3 <- rnorm(n3 <- 12,13)

xx <- c(x1,x2,x3)
n  <- length(xx)
yy <- xx + rnorm(n,0,2)
xym <- t(matrix(c(mean(x1),mean(yy[1:n1]),mean(x2),mean(yy[(n1+1):(n1+n2)]),mean(x3),mean(yy[(n1+n2+1):(n)])),2))
plot(xx,yy,axes = F,xlab="math skills",ylab = "reading skills",type="n")
segments(xym[,1],xym[,2],xym[,1],0,lty=2)
segments(xym[,1],xym[,2],0,xym[,2],lty=2)
points(x1,yy[1:n1],col=1)
points(x2,yy[(n1+1):(n1+n2)],col=ared,pch=2)
points(x3,yy[(n1+n2+1):(n)],col=4,pch=0)
points(mean(x1),mean(yy[1:n1]),pch=16,cex=1.5)
points(mean(x2),mean(yy[(n1+1):(n1+n2)]),col=ared,pch=17,cex=1.5)
points(mean(x3),mean(yy[(n1+n2+1):(n)]),col=4,pch=15,cex=1.5)
legend(14,10,c("GR","SP","UK"),pch=c(1,2,0),col=c(1,2,4))

axis(1,xym[,1],c(expression(hat(mu)[x~GR]),expression(hat(mu)[x~SP]),expression(hat(mu)[x~UK])))
axis(1,c(9,11,14,15))
axis(1,xym[,1],c(expression(hat(mu)[x~GR]),expression(hat(mu)[x~SP]),expression(hat(mu)[x~UK])))
axis(2,xym[,2],c(expression(hat(mu)[y~GR]),expression(hat(mu)[y~SP]),expression(hat(mu)[y~UK])),las=2)
axis(2,c(6:8,10:11,14:17),las=2)

```


L'indice $r$ calcolato su tutti i dati è
\[r = `r cor(xx,yy)`\]


L'indice $r$ calcolato sulle tre medie è
\[r_{\text{Medie}} = `r cor(xym[,1],xym[,2])`\]

## Scomposizione della varianza

La scomposizione della varianza ci offre un quadro teorico per comprendere
come la variabilità iniziale della variabile di interesse $Y$ sia scomponibile
in due pezzi: la variabilità spiegata dal modello e quella residua (casuale).

Ricordiamo che:
\[\begin{aligned}
y_i &\phantom{=}\text{ dati } &\hat y_i &= \hat \beta_0+\hat\beta_1x_i,& \hat\varepsilon_i &= y_i-\hat y_i\\
\bar y &= \frac 1 n\sum_{i=1}^n y_i,& \bar y &= \frac 1 n\sum_{i=1}^n \hat y_i &
0  &=\frac 1 n\sum_{i=1}^n\hat\varepsilon_i
\end{aligned}\]

::: {.definition name="Varianza Totale, Varianza Spiegata e Varianza Residua"}
La varianza di $y$ (senza osservare $x$) è
\[\hat\sigma_Y^2=\frac{\sum_{i=1}^n(y_i-\bar y)^2}n=\frac{TSS}{n},\qquad\text{$TSS$ Total Sum of Squares}\]

La varianza di $\hat y$ è
\[\hat\sigma_{\hat Y}^2=\frac{\sum_{i=1}^n(\hat y_i-\bar y)^2}n=\frac{ESS}{n},\qquad\text{$ESS$ Explained Sum of Squares}\]

La varianza di $\hat \varepsilon$ è
\[\hat\sigma_{\varepsilon}^2=\frac{\sum_{i=1}^n(\hat \varepsilon_i-0)^2}n=\frac{\sum_{i=1}^n\hat \varepsilon_i^2}n=\frac{RSS}{n},\qquad\text{$RSS$ Residual Sum of Squares}\]
:::

:::: {.info data-latex=""}
::: {.proposition}
Vale la seguente relazione
\[TSS = ESS + RSS\]
:::
::::

La variabilità totale di $y$ (quella osservata senza $x$) e scomponibile nella somma di due parti

:::: {.info data-latex=""}
\[
\left\{\begin{array}{cc}
\text{varibilità di $y$}\\
\text{intorno alla sua media}
\end{array}\right\} =
\left\{\begin{array}{cc}
\text{varibilità della retta}\\
\text{intorno alla media}
\end{array}\right\} +
\left\{\begin{array}{cc}
\text{varibilità delle $y$}\\
\text{intorno alla retta}
\end{array} \right\}
\]
::::

Dividendo ogni membro per $TSS$, si ottiene
\begin{eqnarray*}
 TSS &=& ESS + RSS \\
 \frac{TSS}{TSS} &=& \frac{ESS}{TSS} + \frac{RSS}{TSS}\\
  1 &=& \frac{ESS}{TSS} + \frac{RSS}{TSS}\\
 \frac{RSS}{TSS} &=& 1 - \frac{ESS}{TSS}\\
 \frac{RSS}{TSS} &=& 1- r^2,\quad r^2=\frac{ESS}{TSS}
\end{eqnarray*}


## Il coefficiente di determinazione lineare $R^2$

È un indicatore sintetico che indica la quota di varianza spiegata dal modello. 
Nel caso della regressione lineare semplice

:::: {.info data-latex=""}
::: {.definition name="Indice di Determinazione Lineare"}
Si definisce 
\[R^2=\left(\frac{ESS}{TSS}\right)=r^2=\left(\frac{\text{cov}(x,y)}{\hat\sigma_x\hat\sigma_y}\right)^2\]
l'indice di determinazione lineare ed è, nel contesto della regressione lineare semplice, il quadrato dell'indice di correlazione
\[0\leq R^2\leq 1\]
:::
::::

Se $R^2=1$, allora $r=-1$ oppure $r=+1$, associazione lineare perfetta, 100% della variabilità spiegata, se $R^2=0$, allora $r=0$ associazione lineare nulla, 0% della variabilità spiegata, se $R^2>0.75$, allora considereremo l'associazione lineare _soddisfacente_.

### Interpretazione di $r^2$

La variabilità di $y$ viene scomposta in due, la componente spiegata dalla retta e della residua.
Caso limite uno: la retta spiega tutta la variabilità di $y$:
\[TSS = ESS\Rightarrow RSS=0\Rightarrow r^2=1\]
i punti sono allineati su una retta

```{r 17-regressione-I-55}
fig.def(3)
```


```{r 17-regressione-I-56}
set.seed(1)
par(mfrow=c(1,2),cex=cex)
xdat <- rnorm(25,5)
xdat <- xdat - min(xdat)+.5
ydat <- 2 + xdat/2
plot(xdat,ydat,xlab = "x", ylab = "y",pch=16,xlim=c(0,max(xdat)),axes = F)
axis(4,mean(ydat),expression(hat(mu)[Y]),las=2)
segments(xdat,ydat,0,ydat,lty=2,col="grey")
abline(h=mean(ydat),lty=2)
abline(2,.5,col=4)
points(rep(0,times = 25),ydat,pch=4,col=ared)
axis(1)
axis(2,las=2,pos = -.5)
xdat <- rnorm(25,5)
xdat <- xdat - min(xdat)+.5
ydat <- 10 - xdat
plot(xdat,ydat,xlab = "x", ylab = "y",pch=16,xlim=c(0,max(xdat)),axes = F)
axis(4,mean(ydat),expression(hat(mu)[Y]),las=2)
segments(xdat,ydat,0,ydat,lty=2,col="grey")
abline(h=mean(ydat),lty=2)
abline(10,-1,col=4)
points(rep(0,times = 25),ydat,pch=4,col=ared)
axis(1)
axis(2,las=2,pos = -.5)
par(mfrow=c(1,1),cex=cex)
```

Se la retta è in grado di assorbire tutta la variabilità di $y$ significa che tutti 
i punti sono allineati sul di essa.

Caso limite due: la retta **non** spiega nulla della variabilità di $y$
\[TSS = RSS\Rightarrow ESS=0\Rightarrow r^2=0\]
la retta è orizzontale e coincide con $\hat\mu_Y$

```{r 17-regressione-I-57}

set.seed(2)
par(mfrow=c(1,2),cex=cex)
xdat <- rnorm(25,5)
xdat <- xdat - min(xdat)+.5
ydat <- rnorm(25,5)
plot(xdat,ydat,xlab = "x", ylab = "y",pch=16,xlim=c(0,max(xdat)),axes = F)
axis(4,mean(ydat),expression(hat(mu)[Y]),las=2)
segments(xdat,ydat,xdat,mean(ydat),lty=2,col="grey")
segments(xdat,ydat,0,(ydat),lty=2,col="grey")
abline(h=mean(ydat),col=4)
points(rep(0,times = 25),ydat,pch=4,col=ared)
axis(1)
axis(2,las=2)
xdat <- rnorm(25,5)
xdat <- xdat - min(xdat)
xdat <- (xdat/max(xdat))*2*pi
ydat <- sin(xdat)
plot(xdat,ydat,xlab = "x", ylab = "y",pch=16,xlim=c(0,max(xdat)),axes = F)
axis(4,mean(ydat),expression(hat(mu)[Y]),las=2)
segments(xdat,ydat,xdat,mean(ydat),lty=2,col="grey")
segments(xdat,ydat,0,(ydat),lty=2,col="grey")
abline(h=mean(ydat),col=4)
points(rep(-.1,times = 25),ydat,pch=4,col=ared)
axis(1)
axis(2,las=2)
par(mfrow=c(1,1),cex=cex)
```


Se la retta è parallela all'asse dell $x$ non spiega nulla della variabilità di $y$.

### Scomposizione della varianza sui dati di esempio


```{r 17-regressione-I-58}
x <- c(0:3)
y <- c(2,3.5,2.5,4)

yh <- b0 + b1*x
eh <- y  - yh

x <- c(0:3)
y <- c(2,3.5,2.5,4)
#y <- rnorm(4)
w <- y[c(4,1,3,2)]
#w <- c(4,2,2.5,3.5)

mx <- mean(x)
vx <- mean(x^2)-mean(x)^2

my <- mean(y)
vy <- mean(y^2)-mean(y)^2

mw <- mean(w)
vw <- mean(w^2)-mean(w)^2

co <- mean(x*y) - mx*my
cw <- mean(x*w) - mx*mw
r  <- co/sqrt(vy*vx)
rw <- cw/sqrt(vw*vx)
b1 <- co/vx
bw1<- cw/vx
b0 <- my - b1*mx
bw0<- mw - bw1*mx

ys <- b0 + b1 * x
es <- y - ys
rg <- ys - my

ws <- bw0 + bw1 * x
ew <- w - ws
rgw<- ws - mw


par(mfrow=c(1,2),cex=cex)

plot(x,y,axes=F,pch=16,xlim=c(0,4))
axis(1,x)
axis(2,y,las=2)
axis(2,my,expression(hat(mu)[Y]==3),las=2,col.axis=4)
abline(h=my,col=4)
segments(x,y,x,my,col=1:4,lty=2)
segments(x,y,0,y,col=1:4,lty=2)
arrows(x+.05,y,x+.05,my,length = .05,code = 3)
text(x+.1,(y+my)/2,round(y-my,2),pos = 4)
text(x+.1,(my+y)/2-.25,paste("(",(y-my)^2,")",sep = ""),pos = 4,col=ared)
title("Total")


plot(x,y,axes=F,pch=16,col=1,xlim=c(0,4))
abline(lsfit(x,y),col=4)
abline(mean(y),0,lty=3)
segments(mean(x),min(y)-10,mean(x),max(y)+10,lty=3)
segments(x,my,x,ys,lty=2)
segments(-1,ys,x,ys,lty=2)
title("Explained")
axis(1,x)
axis(2,c(y),las=2)
axis(2,c(ys),las=2,col.axis=4)
axis(2,my,expression(hat(mu)[Y]==3),las=2,col.axis=4)
arrows(x+.05,ys,x+.05,my,length = .05,code = 3)
text(x,(ys+my)/2,rg,pos = 4)
text(x,(ys+my)/2-.25,paste("(",(ys-my)^2,")",sep = ""),pos = 4,col=ared)
```

```{r 17-regressione-I-59}
plot(x,y,axes=F,pch=16,xlim=c(0,4))
axis(1,x)
axis(2,y,las=2)
axis(2,yh,round(yh,2),las=2,col.axis=4)
abline(b0,b1,col=4)
segments(x,0,x,y,col=1:4,lty=2)
segments(x,y,0,y,col=1:4,lty=2)
segments(x,0,x,yh,col=1:4,lty=3)
segments(x,yh,0,yh,col=1:4,lty=3)
points(x,yh,pch=16,col=4)
arrows(x+.05,y,x+.05,yh,length = .05,code = 3)
text(x+.1,(y+yh)/2,round(eh,2),pos = 4)
text(x+.1,(yh+y)/2-.15,paste("(",es^2,")",sep = ""),pos = 4,col=ared)
title("Residual")

par(mfrow=c(1,1),cex=cex)



sum((y-my)^2)  -> TSS
sum((ys-my)^2) -> ESS
sum(es^2)      -> RSS 

sum((w-mw)^2)  -> TSSw
sum((ws-mw)^2) -> ESSw
sum(es^2)      -> RSSw

n <- 4
se2 <- (n/(n-2))*vy*(1-r^2) 
vb0 <- se2 * (1/n+mx^2/(n*vx)) 
vb1 <- se2 / (n*vx) 

Dato <- c(1:4,"Totale")
prn <- data.frame(Dato,y=c((y-my)^2,0),yh = c((ys-my)^2,0),eh = c(es^2,0))
prn[5,2:4] <- colSums(prn[1:4,2:4])

names(prn)<-c("$i$","$(y_i-\\bar y)^2$","$(\\hat y_i-\\bar y)^2$ ","$\\hat \\varepsilon_i^2$") 
```

```{r 17-regressione-I-60}
kable(prn,booktabs = T, escape = F, linesep = "") %>%
    row_spec(5, bold = T, color = "white", background = "gray")
```

\begin{eqnarray*}
 TSS &=& ESS + RSS \\
 `r TSS`&=&`r ESS`+`r RSS`\\
 \frac{RSS}{TSS} &=& 1- r^2\\
 \frac{`r RSS`}{`r TSS`} &=& 1- `r r`^2\\ 
 `r 1-r^2` &=& 1- `r r^2`
\end{eqnarray*}

Ovvero la retta di regressione di $y$ dato $x$ spiega il `r r^2*100`% della variabilità totale della y.

__Osservazione.__ Se volessi studiare la relazione tra $X$ e $W$ nella tabella qui sotto:

```{r 17-regressione-I-61}
Tipo <- c(1:4,"Totale")
axyw <- data.frame(Tipo,x=c(x,0),y=c(y,0),w=c(w,0))
names(axyw) <- c("$i$","$x_i$","$y_i$","$w_i$")
axyw[5,2:4] <- colSums(axyw[1:4,2:4])
#kable(axyw, format = "latex", booktabs = T)%>%kable_styling(latex_options = "striped")
kable(axyw,booktabs = T, escape = F, linesep = "")%>%
   row_spec(5, bold = T, color = "white", background = "gray")
```

Osservo dapprima Le variabili $y$ e $w$ hanno la stessa media e la stessa varianza, infatti
$\sum_i y_i = \sum_i w_i=`r sum(y)`$ e $\sum_i y_i^2=\sum_i w_i^2=`r sum(y^2)`$. 
Se osservati rispetto ai soli valori, in ipotesi IID, sono fenomeni indistinguibili.

```{r 17-regressione-I-62}
par(mfrow=c(1,2),cex=cex)

plot(x,y,axes=F,pch=16,col=4,xlab = "i")
abline(mean(y),0,lty=3)
segments(x,my,x,y,lty=2)
title("Varianza Totale y")
axis(1,x,1:4)
axis(2,y)
axis(2,mean(w),expression(hat(mu)[Y]==3))
text(x,(my+y)/2,y-my,pos = c(4,4,4,2))
text(x,(my+y)/2-.1,paste("(",(y-my)^2,")",sep = ""),pos = c(4,4,4,2),col=ared)

plot(x,w,axes=F,pch=16,col=4,xlab = "i")
abline(mean(w),0,lty=3)
segments(x,mean(w),x,w,lty=2)
title("Varianza Totale w")
axis(1,x,1:4)
axis(2,w)
axis(2,mean(w),expression(hat(mu)[W]==3))
text(x,(my+w)/2,w-my,pos = c(4,4,4,2))
text(x,(my+w)/2-.1,paste("(",(w-my)^2,")",sep = ""),pos = c(4,4,4,2),col=ared)

par(mfrow=c(1,1),cex=cex)

```


La varianza di $y$ è dunque
\[\hat\sigma_Y^2=\frac 1 n\sum_i (y_i - \bar y)^2 =\frac 1 n\sum_i y_i^2 - \bar y^2 =`r mean(y^2)` - (`r mean(y)`)^2=`r vy`\]
e quella di $w$
\[\hat\sigma_W^2=\frac 1 n\sum_i (w_i - \bar w)^2 =\frac 1 n\sum_i w_i^2 - \bar w^2 =`r mean(w^2)` - (`r mean(w)`)^2=`r vy`\]

Ma la variabile $X$ non spiega nello stesso modo $Y$ e $W$. Infatti se osserviamo la relazione tra $x$ ed $Y$ e la relazione tra $x$ e $w$ ci accorgiamo che i fenomeni sono diversi.

```{r 17-regressione-I-63}

par(mfrow=c(1,2),cex=cex)
plot(x,y,axes=F,pch=16,col=4)
title("retta Y|X")
abline(lsfit(x,y),col=ared)
abline(mean(y),0,lty=3)
segments(mean(x),min(y)-10,mean(x),max(y)+10,lty=3)
segments(x,0,x,y,lty=2)
segments(-1,y,x,y,lty=2)
axis(1,x)
axis(2,y)
axis(2,mean(w),expression(hat(mu)[W]))

rxw <- lsfit(x,w)
plot(x,w,axes=F,pch=16,col=4)
title("retta W|X")
abline(mean(y),0,lty=3)
segments(mean(x),min(y)-10,mean(x),max(y)+10,lty=3)
abline(rxw,col=ared)
segments(x,0,x,w,lty=2)
segments(x,0,x,w,lty=2)
segments(-1,w,x,w,lty=2)
axis(1,x)
axis(2,w)
axis(2,mean(w),expression(hat(mu)[W]))
par(mfrow=c(1,1),cex=cex)

```


- La retta $y|x$ assorbe il `r r^2*100`% della variabilità di $y$
- La retta $w|x$ assorbe il `r rw^2*100`% della variabilità di $w$

## Stima di $\sigma_\varepsilon^2$

Il parametro $\sigma_\varepsilon^2$ rappresenta la variabilità dei punti intorno alla retta. 
La stima di $\sigma_\varepsilon^2$ deriva dalla scomposizione della varianza
\begin{eqnarray*}
\frac{RSS}{TSS} &=& 1-r^2 \\
RSS &=& TSS(1-r^2)\\
\frac{RSS}{n} &=& \frac{TSS}{n}(1-r^2) \\
\hat\sigma_\varepsilon^2 &=& \hat\sigma_Y^2(1-r^2)
\end{eqnarray*}

In modo analogo alla stima di $\sigma^2$ in un modello normale, $\hat\sigma_\varepsilon^2$ 
**non** è stima corretta di $\sigma_\varepsilon^2$ e si dimostra che
\[E(\hat\sigma_\varepsilon^2)=\frac{n-2}n \sigma_\varepsilon^2\]
si quindi corregge con:
\[S_\varepsilon^2=\frac n{n-2}\hat\sigma_\varepsilon^2\]


## Statistiche Sufficienti del Modello di Reegressione

Se l'ipotesi di normalità dei residui viene ritenuta valida si può dimostrare che le
stime di massima verosimiglianza per $\beta_0$, $\beta_1$ e $\sigma_\varepsilon^2$ coincidono
con quelle dei minimi quadrati $\hat\beta_0$, $\hat\beta_1$ e $\hat\sigma_\varepsilon^2$.
Tutta l'informazione sul modello di regressione lineare semplice è contenuta nelle
seguenti statistiche
\[\sum_{i=1}^n x_i,~~\sum_{i=1}^n y_i,~~\sum_{i=1}^n x_i^2,~~\sum_{i=1}^ny_i^2,~~ \sum_{i=1}^n x_i y_i\]

```{r 17-regressione-I-64}
Dato <- c(1:4,"Totale","Totale/n")
prn <- data.frame(Dato,x=c(x,0,0),y=c(y,0,0),x2 = c(x^2,0,0),y2 = c(y^2,0,0),w=c(x*y,0,0))
prn[5,2:6] <- colSums(prn[1:4,2:6])
prn[6,2:6] <- colMeans(prn[1:4,2:6])

names(prn)<-c("$i$","$x_i$","$y_i$","$x_i^2$","$y_i^2$","$x_i\\cdot y_i$") 

kable(prn,booktabs = T, escape = F, linesep = "")%>%
  row_spec(5, bold = T, color = "white", background = "gray")
```


Ricapitolando nel nostro esempio:
\begin{alignat*}{3}
 \bar x & =  \frac 1 n \sum_{i=1}^n x_i  = `r mx` &
\hat\sigma_X^2 & =  \frac 1 n \sum_{i=1}^n x_i^2 - \bar x^2  = `r vx` &\\
 \bar y & =  \frac 1 n \sum_{i=1}^n y_i   = `r my` &
\hat\sigma_Y^2 & =  \frac 1 n \sum_{i=1}^n y_i^2 - \bar y^2  = `r vy` &\\
 \text{cov}(x,y) & = \frac 1 n \sum_{i=1}^n x_iy_i -\bar x\bar y  = `r co` & 
r & = \frac{\text{cov}(x,y)}{\hat\sigma_X \hat\sigma_Y }  = `r r` &\\
\hat\beta_1 & = \frac{\text{cov}(x,y)}{\hat\sigma_X^2} = `r b1` & 
\hat\beta_0 & = \bar y  - \hat\beta_1\bar x = `r b0`. &\\
\hat\sigma_\varepsilon^2 & = \hat\sigma_Y^2(1-r^2)=`r vy*(1-r^2)` &
S_\varepsilon^2 & = \frac{n}{n-2}\hat\sigma_\varepsilon^2 = `r se2`\\
\hat\sigma_\varepsilon & = \hat\sigma_Y\sqrt{(1-r^2)}=`r sqrt(vy*(1-r^2))` & \qquad
S_\varepsilon & = \sqrt{\frac{n}{n-2}}\hat\sigma_\varepsilon = `r sqrt(se2)`\\
\end{alignat*}

<!--chapter:end:17-regressione-I.Rmd-->


```{r setupreg2, include=FALSE}


source("intro.R")

s2c <- function(x) (mean(x^2)-mean(x)^2)
sc  <- function(x) (sqrt(s2c(x)))

x <- c(0:3)
y <- c(2,3.5,2.5,4)
w <- y[c(4,1,3,2)]

mx <- mean(x)
vx <- mean(x^2)-mean(x)^2
sx <- sqrt(vx)

my <- mean(y)
vy <- mean(y^2)-mean(y)^2
sy <- sqrt(vy)

mw <- mean(w)
vw <- mean(w^2)-mean(w)^2

co <- mean(x*y) - mx*my
cw <- mean(x*w) - mx*mw
r  <- co/sqrt(vy*vx)
rw <- cw/sqrt(vw*vx)
b1 <- co/vx
bw1<- cw/vx
b0 <- my - b1*mx
bw0<- mw - bw1*mx

ys <- b0 + b1 * x
es <- y - ys
rg <- ys - my

ws <- bw0 + bw1 * x
ew <- w - ws
rgw<- ws - mw


sum((y-my)^2)  -> TSS
sum((ys-my)^2) -> ESS
sum(es^2)      -> RSS 

sum((w-mw)^2)  -> TSSw
sum((ws-mw)^2) -> ESSw
sum(es^2)      -> RSSw

n <- 4
se2 <- (n/(n-2))*vy*(1-r^2) 
vb0 <- se2 * (1/n+mx^2/(n*vx)) 
vb1 <- se2 / (n*vx) 
```

# Inferenza e Diagnostica sul Modello di Regressione Lineare

## Teorema di Gauss-Markov

:::: {.info data-latex=""}
::: {.theorem name="Gauss-Markov"}
Sotto gli assunti dallo 0 al 5,
gli stimatori dei minimi quadrati sono corretti
\[E(\hat\beta_1)=\beta_1,\qquad E(\hat\beta_0)=\beta_0\]

La loro varianza è:

\begin{eqnarray*}
    V(\hat\beta_{1}) &=& \frac{\sigma_{\varepsilon}^{2}} {n \hat{\sigma}^{2}_{X}} \\
    V(\hat\beta_{0}) &=& \sigma_{\varepsilon}^{2} \left( \frac{1} {n}  +  \frac{\bar{x}^{2}} {n \hat{\sigma}^{2}_{X}} \right)   \\
    \mbox{cov}(\hat\beta_{0}, \hat\beta_{1}) &=& - \sigma_{\varepsilon}^{2} \frac{\bar{x}} {n \hat{\sigma}^{2}_{X}}
 =  - \bar{x} V(\hat\beta_{1})
\end{eqnarray*}

Gli stimatori $\hat\beta_{0}$ e $\hat\beta_{1}$ di $\beta_{0}$ e $\beta_{1}$
sono _BLUE (Best Linear Unbiased Estimators)_.
:::
::::

## La previsione $\hat Y$

Definiamo la previsione di $Y$ per un valore $x$ qualunque
\[\hat Y_{(X=x)}=\hat\beta_0+\hat\beta_1x\]

Se $X=x_i$ allora
\[\hat Y_{(X=x_i)}=\hat Y_i\]

La previsione è _corretta_
\[E\left(\hat Y_{(X=x)}\right)=E(\hat\beta_0+\hat\beta_1x)=E(\hat\beta_0)+E(\hat\beta_1)x=\beta_0+\beta_1x\]

La varianza
\[V(\widehat{Y}_{(X=x)}) = \sigma_{\varepsilon}^{2} \left( \frac{1} {n}  +  \frac{(x - \bar{x})^{2}} {n \hat{\sigma}^{2}_{X}} \right)\]


## Standard Errors e Stima degli SE

Gli Errori Standard (ES) delle stime sono le radici quadrate
delle varianze degli stimatori
\begin{eqnarray*}
SE(\hat\beta_{0})           &=& \sqrt{\hat\beta_{0}}           \\
                            &=& \sqrt{\sigma_{\varepsilon}^{2}\left( \frac{1} {n} +
   \frac{\bar{x}^{2}} {n\hat{\sigma}^{2}_{X}} \right)}\\
SE(\hat\beta_{1})           &=& \sqrt{V(\hat\beta_{1})}           \\
&=&\sqrt{\frac{\sigma_{\varepsilon}^{2}} {n\hat{\sigma}^{2}_{X}} }\\
SE(\widehat{Y}_{(X=x)}) &=& \sqrt{V(\widehat{Y}_{(X=x)})} \\
 &=& \sqrt{\sigma_{\varepsilon}^{2}\left( \frac{1} {n} +
   \frac{(x - \bar{x})^{2}} {n\hat{\sigma}^{2}_{X}} \right)}
\end{eqnarray*}

Ricordando che
\[S_{\epsilon}  = \sqrt{\frac{n} {n-2}}\ \hat{\sigma}_{Y}\sqrt{1 - r^2}\]

Otteniamo le stime campionarie per gli Standard Errors

::: {.info data-latex=""}
\begin{eqnarray*}
\widehat{SE(\hat\beta_{0})} &=& \sqrt{S_{\varepsilon}^{2} \left( \frac{1} {n} +  \frac{\bar{x}^{2}} {n \hat{\sigma}^{2}_{X}} \right)}\\
\widehat{SE(\hat\beta_{1})}           &=& \sqrt{\frac{S_{\varepsilon}^{2}} {n\hat{\sigma}^{2}_{X}} }\\
\widehat{SE(\widehat{Y}_{X=x})}&=& \sqrt{S_{\varepsilon}^{2}\left( \frac{1} {n} +  \frac{(x - \bar{x})^{2}} {n\hat{\sigma}^{2}_{X}} \right)}
\end{eqnarray*}
:::

## Inferenza su $\beta_0$ e $\beta_1$ e su $\hat Y$

Se l'assunto 6. (normalità dei residui)  è rispettato, allora

::: {.info data-latex=""}
\[\hat\beta_1\sim N(\beta_1,V(\hat\beta_1)), ~~\hat\beta_0\sim N(\beta_0,V(\hat\beta_0)), ~~\hat Y_{(X=x)}\sim N\left(\beta_0+\beta_1 x,V(\hat Y_{(X=x)})\right)\]
:::

Fisso $n=10$, $x_1=0.1,x_2=0.2,...,x_{10}=1.0$, $\beta_0=0$, $\beta_1=1$ e $\sigma_\varepsilon^2=0.64$. Simulo 5 volte $n=10$ punti da 
\[Y_i = 0 + 1\cdot x_i + \varepsilon_i,~~\varepsilon_i\sim N(0,0.64)\]

```{r 18-regressione-II-3}
fig.def(2.5,3.5)
```

```{r 18-regressione-II-4}
set.seed(1)
sge <- .8
x <- (1:10)/10
y <- x+rnorm(10,0,sge)
plot(x,y,pch=16,cex=1,ylim=c(-1,3),lty=2,axes = F,xlab = "x",ylab = "y")
axis(1,x)
axis(2)
abline(lsfit(x,y),col=1,lty=2)
for(i in 1:4){
  y <- x+rnorm(10,0,sge)
  points(x,y,pch=i+1,cex=1,col=i+1)
  abline(lsfit(x,y),col=i+1,lty=2)
}
abline(0,1,col="DarkRed",lwd=1.5)

fig.def(2.6)
```

E ripeto le simulazioni ripetute 500 volte:

```{r 18-regressione-II-5}
set.seed(1)
sge <- .8
x <- (1:10)/10
y <- x+rnorm(10,0,sge)

V0 <- sge^2 * (1/10+mean(x)^2/(10*s2c(x))) 
V1 <- sge^2 / (10*s2c(x)) 
C12<- -mean(x)*V1


V <- matrix(c(V0,C12,C12,V1),2)
#fpar <- nbvpdf.2(0,1,V0,V1,C12)



b0gr <- seq(-3*sqrt(V0),+3*sqrt(V0),length.out = 101)
b1gr <- seq(1-3*sqrt(V1),1+3*sqrt(V1),length.out = 101)
b01  <- expand.grid(b0gr,b1gr)

dbe <- dmvnorm(b01,c(0,1),V)

#bde  <- apply(b01,1,function(x)fpar(x[1],x[2]))
bde  <- matrix(dbe,101)


par(mfrow=c(1,2),cex=cex)

plot(x,y,pch=16,cex=1,ylim=c(-1,3),lty=2,type="n",axes=F,xlab="x",ylab="y")
axis(1,x)
axis(2)
md <- lsfit(x,y)
abline(md,col="Grey")
ppp <- md$coefficients
for(i in 1:500){
  y <- x+rnorm(10,0,sge)
#  points(x,y,pch=16,cex=1,col=i+1)
  md <- lsfit(x,y)
  abline(md,col="Grey")
  ppp <- cbind(ppp,md$coefficients)

}
abline(0,1,col="DarkRed",lwd=2)
abline(v=mean(x),lty=2)
abline(h=mean(x),lty=2)

sey <-function(x,n) sqrt(sge^2*(1/n+(x-mean(x))^2/(n*s2c(x))))


curve(x+1.96*sey(x,n=10),add=T,lty=2,col="DarkRed",lwd=2)
curve(x-1.96*sey(x,n=10),add=T,lty=2,col="DarkRed",lwd=2)



image(b0gr,b1gr,bde,xlab =expression(hat(beta)[0]),ylab=expression(hat(beta))[1],col = rev(gray.colors(101)))
points(ppp[1,],ppp[2,],pch=16,cex=.5,col="darkblue")
contour(b0gr,b1gr,bde,xlab =expression(hat(beta)[0]),ylab=expression(hat(beta))[1],drawlabels = F,add = T,col="darkred",lty=2)
```

```{r 18-regressione-II-6}
curve(dnorm(x,0,sqrt(V0)),range(b0gr)[1],range(b0gr)[2],xlab = expression(hat(beta)[0]),ylab = expression(f(hat(beta)[0])))
points(ppp[1,],rep(0,times=501),pch=4,cex=.5)

curve(dnorm(x,1,sqrt(V1)),range(b1gr)[1],range(b1gr)[2],xlab = expression(hat(beta)[1]),ylab = expression(f(hat(beta)[1])))
points(ppp[2,],rep(0,times=501),pch=4,cex=.5)


par(mfrow=c(1,1),cex=cex)
```


La varianza di $\hat Y$ è l'_errore di previsione_
\[V(\widehat{Y}_{(X=x)}) = \sigma_{\varepsilon}^{2} \left( \frac{1} {n}  +  \frac{(x - \bar{x})^{2}} {n \hat{\sigma}^{2}_{X}} \right)\]



### Interpolazione e Estrapolazione


::: {.info data-latex=""}
Parliamo di _interpolazione_ dei punti se $\hat Y_{(X=x)}$ è calcolato per
\[\min\{x_i\}\leq x \leq\max\{x_i\}\]

Parliamo di _estrapolazione_ dei punti se $\hat Y_{(X=x)}$ è calcolato per
\[x<\min\{x_i\}~~~\text{oppure}~~~ x >\max\{x_i\}\]
:::

Voglio le vedere le previsioni per $x>1$

```{r 18-regressione-II-7}
fig.def(3,3.5)
```

```{r 18-regressione-II-8}
set.seed(1)
sge <- .8
x <- (1:100)/100
y <- x+rnorm(100,0,sge)
plot(x,y,pch=16,cex=1,ylim=c(-2,5),lty=2,type="n",xlim=c(0,2),xlab="x",ylab="y")
#abline(lsfit(x,y),col="Grey")
abline(0,1,col="DarkRed",lwd=2)
abline(-.2,(.5+.2)/.5,col=4)
text(1.5,2.1,expression(-0.2+1.4*x),col=4)
curve(x+1.96*sey(x,n=10),add=T,lty=2,col=ared)
curve(x-1.96*sey(x,n=10),add=T,lty=2,col=ared)
abline(v=1,lty=2)
text(0,-2,"interpolazione",pos = 4)
text(1,-2,"estrapolazione",pos = 4)
text(1,4,"errore di previsione",col=ared,pos=4)
text(1.5,1.2,expression(0+1*x),col="darkred")

fig.def(2.5,3.5)

```



### Intervalli di Confidenza per $\beta_0$, $\beta_1$ e $\hat Y$

```{r 18-regressione-II-9}
n <- 4
se2 <- (n/(n-2))*vy*(1-r^2) 
vb0 <- se2 * (1/n+mx^2/(n*vx)) 
vb1 <- se2 / (n*vx) 

# H0: b1 = 0 H1: b1 != 0

t0oss <- (b0 - 0)/sqrt(vb0)
t1oss <- (b1 - 0)/sqrt(vb1)
t.025 <- qt(1-.025,n-2)
t.005 <- qt(1-.005,n-2)
```


Si dimostra che
\begin{eqnarray*}
 \frac{\hat\beta_0-\beta_0}{\widehat{SE(\hat\beta_0)}}&\sim&t_{n-2}\\
 \frac{\hat\beta_1-\beta_1}{\widehat{SE(\hat\beta_1)}}&\sim&t_{n-2}\\
 \frac{\hat Y_{(X=x)}-(\beta_0+\beta_1 x)}{\widehat{SE(\hat Y_{(X=x)})}}&\sim&t_{n-2}
\end{eqnarray*}


E dunque gli IdC al livello $(1-\alpha)$ sono dati da
\begin{eqnarray*}
\hat\beta_0 &\pm& t_{n-2;\alpha/2}\widehat{SE(\hat\beta_0)}\\
\hat\beta_1 &\pm& t_{n-2;\alpha/2}\widehat{SE(\hat\beta_1)}\\
\hat Y_{(X=x)} &\pm& t_{n-2;\alpha/2}\widehat{SE(\hat Y_{(X=x)})}
\end{eqnarray*}


### Test per $\beta_0$, e $\beta_1$




Consideriamo i due seguenti sistemi di ipotesi
\[
\begin{cases}
H_0:\beta_0=\beta_{0,H_0}\\H_1:\text{da scegliere $\neq$, $>$ o $<$}
\end{cases}
\qquad
\begin{cases}
H_0:\beta_1=\beta_{1,H_0}\\H_1:\text{da scegliere $\neq$, $>$ o $<$}
\end{cases}
\]


Sapendo che
\[
\widehat{SE(\hat\beta_{0})}           = \sqrt{S_{\varepsilon}^{2}\left( \frac{1} {n} + \frac{\bar{x}^{2}} {n\hat{\sigma}^{2}_{X}} \right)} \qquad \widehat{SE(\hat\beta_{1})}           = \sqrt{\frac{S_{\varepsilon}^{2}} {n\hat{\sigma}^{2}_{X}} }\\
\]


::: {.info data-latex=""}
Sotto $H_0$

\begin{eqnarray*}
 \frac{\hat\beta_0-\beta_{0,H_0}}{\widehat{SE(\hat\beta_0)}} &\sim& t_{n-2} \\
 \frac{\hat\beta_1-\beta_{1,H_0}}{\widehat{SE(\hat\beta_1)}} &\sim& t_{n-2}
\end{eqnarray*}


otteniamo
\begin{eqnarray*}
t_{0,\text{obs}} &=& \frac{\hat\beta_0-\beta_{0,H_0}}{\widehat{SE(\hat\beta_0)}} \\
t_{1,\text{obs}} &=& \frac{\hat\beta_1-\beta_{1,H_0}}{\widehat{SE(\hat\beta_1)}}
\end{eqnarray*}

Che andranno lette nella direzione di $H_1$ con le solite regole
:::

### Esempio sui 4 punti

Tipicamente quando si fa un modello di regressione per prima cosa si testa la significatività dei 
coefficienti:
\[
\begin{cases}
H_0:\beta_0=0\\H_1:\beta_0\neq 0
\end{cases}
\qquad
\begin{cases}
H_0:\beta_1=0\\H_1:\beta_1\neq 0
\end{cases}
\]


Calcolo gli standard errors stimati

\[\begin{aligned}
V(\hat\beta_1)&=\frac{\sigma_\varepsilon^2}{n\sigma_X^2} &\widehat {SE(\hat\beta_1)}&=\sqrt{\frac{S_\varepsilon^2}{n\hat\sigma_X^2}}\\
              &                                          &&=\sqrt{\frac{`r se2`}{`r n`\times`r vx`}}\\
              &&&=`r sqrt(vb1)`\\
V(\hat\beta_0)&=\sigma_\varepsilon^2\left(\frac 1 n +\frac{\bar x^2}{n\sigma_X^2}\right) 
&\widehat {SE(\hat\beta_0)} &= \sqrt{S_\varepsilon^2\left(\frac 1 n +\frac{\bar x^2}{n\sigma_X^2}\right)}\\
                &&           &= \sqrt{`r se2`\left(\frac 1 {`r n`} +\frac{`r mx`^2}{`r n`\times `r vx`}\right)}\\
                  &&         &= `r sqrt(vb0)`
\end{aligned}\]

### Calcolo dei valori osservati e dei valori critici

otteniamo
\begin{eqnarray*}
t_{0,\text{obs}} &=& \frac{\hat\beta_0-0}{\widehat{SE(\hat\beta_0)}}=`r t0oss`\\
t_{1,\text{obs}} &=&  \frac{\hat\beta_1-0}{\widehat{SE(\hat\beta_1)}}=`r t1oss`
\end{eqnarray*}

dalle tavole ricaviamo 
  - $t_{n-2;0.025}=$ `r t.025`
  - $t_{n-2;0.005}=$ `r t.005`


`r if (abs(t0oss)<abs(t.025)) risp01 <- "non rifiuto $H_0$"  else risp01 <- "rifiuto $H_0$ " `
`r if (abs(t0oss)<abs(t.025)) segn01 <- "<"  else segn01 <- ">" `
`r if (abs(t0oss)<abs(t.005)) risp02 <- "non rifiuto $H_0$"  else risp02 <- "rifiuto $H_0$ " `
`r if (abs(t0oss)<abs(t.005)) segn02 <- "<"  else segn02 <- ">" `
`r if (abs(t1oss)<abs(t.025)) risp11 <- "non rifiuto $H_0$"  else risp11 <- "rifiuto $H_0$ " `
`r if (abs(t1oss)<abs(t.025)) segn11 <- "<"  else segn11 <- ">" `
`r if (abs(t1oss)<abs(t.005)) risp12 <- "non rifiuto $H_0$"  else risp12 <- "rifiuto $H_0$ " `
`r if (abs(t1oss)<abs(t.005)) segn12 <- "<"  else segn12 <- ">" `

e osserviamo che, per il coefficiente $\beta_0$:

  - $|t_{0,\text{obs}}|=$ |`r t0oss`| `r segn01` `r t.025` $=t_{n-2;0.025}$ e quindi `r risp01` al livello di significatività $\alpha=0.05$.
  - $|t_{0,\text{obs}}|=$ |`r t0oss`| `r segn02` `r t.005` $=t_{n-2;0.005}$ e quindi `r risp02` al livello di significatività $\alpha=0.01$.

per il coefficiente $\beta_1$

  - $|t_{1,\text{obs}}|=$ |`r t1oss`| `r segn11` `r t.025` $=t_{n-2;0.025}$ e quindi `r risp11` al livello di significatività $\alpha=0.05$.
  - $|t_{1,\text{obs}}|=$ |`r t1oss`| `r segn12` `r t.005` $=t_{n-2;0.005}$ e quindi `r risp12` al livello di significatività $\alpha=0.01$.


### Se $n=10$

Si supponga che a parità di statistiche $\bar x$, $\bar y$, $\hat\sigma_X$, $\hat\sigma_Y$, e $\text{cov}(X,Y)$, ma per $n=10$, si ottiene:

```{r 18-regressione-II-10}

x <- c(0:3)
y <- c(2,3.5,2.5,4)
n <- 10 



se2 <- (n/(n-2))*vy*(1-r^2) 
vb0 <- se2 * (1/n+mx^2/(n*vx)) 
vb1 <- se2 / (n*vx) 

# H0: b1 = 0 H1: b1 != 0

t0oss <- (b0 - 0)/sqrt(vb0)
t1oss <- (b1 - 0)/sqrt(vb1)
t.025 <- qt(1-.025,n-2)
t.005 <- qt(1-.005,n-2)
```

Calcolo gli standard errors stimati

\[\begin{aligned}
V(\hat\beta_1)&=\frac{\sigma_\varepsilon^2}{n\sigma_X^2} &\widehat {SE(\hat\beta_1)}&=\sqrt{\frac{S_\varepsilon^2}{n\hat\sigma_X^2}}\\
              &                                          &&=\sqrt{\frac{`r se2`}{`r n`\times`r vx`}}\\
              &&&=`r sqrt(vb1)`\\
V(\hat\beta_0)&=\sigma_\varepsilon^2\left(\frac 1 n +\frac{\bar x^2}{n\sigma_X^2}\right) 
&\widehat {SE(\hat\beta_0)} &= \sqrt{S_\varepsilon^2\left(\frac 1 n +\frac{\bar x^2}{n\sigma_X^2}\right)}\\
                &&           &= \sqrt{`r se2`\left(\frac 1 {`r n`} +\frac{`r mx`^2}{`r n`\times `r vx`}\right)}\\
                  &&         &= `r sqrt(vb0)`
\end{aligned}\]

Calcolo dei valori osservati e dei valori critici e ottengo

\begin{eqnarray*}
t_{0,\text{obs}} &=& \frac{\hat\beta_0-0}{\widehat{SE(\hat\beta_0)}}=`r t0oss`\\
t_{1,\text{obs}} &=&  \frac{\hat\beta_1-0}{\widehat{SE(\hat\beta_1)}}=`r t1oss`
\end{eqnarray*}

dalle tavole ricaviamo $t_{n-2;0.025}=$ `r t.025` e $t_{n-2;0.005}=$ `r t.005`


`r if (abs(t0oss)<abs(t.025)) risp01 <- "non rifiuto $H_0$"  else risp01 <- "rifiuto $H_0$ " `
`r if (abs(t0oss)<abs(t.025)) segn01 <- "<"  else segn01 <- ">" `
`r if (abs(t0oss)<abs(t.005)) risp02 <- "non rifiuto $H_0$"  else risp02 <- "rifiuto $H_0$ " `
`r if (abs(t0oss)<abs(t.005)) segn02 <- "<"  else segn02 <- ">" `
`r if (abs(t1oss)<abs(t.025)) risp11 <- "non rifiuto $H_0$"  else risp11 <- "rifiuto $H_0$ " `
`r if (abs(t1oss)<abs(t.025)) segn11 <- "<"  else segn11 <- ">" `
`r if (abs(t1oss)<abs(t.005)) risp12 <- "non rifiuto $H_0$"  else risp12 <- "rifiuto $H_0$ " `
`r if (abs(t1oss)<abs(t.005)) segn12 <- "<"  else segn12 <- ">" `

e osserviamo che, per il coefficiente $\beta_0$:

  - $|t_{0,\text{obs}}|=$ |`r t0oss`| `r segn01` `r t.025` $=t_{n-2;0.025}$ e quindi `r risp01` al livello di significatività $\alpha=0.05$.
  - $|t_{0,\text{obs}}|=$ |`r t0oss`| `r segn02` `r t.005` $=t_{n-2;0.005}$ e quindi `r risp02` al livello di significatività $\alpha=0.01$.

per il coefficiente $\beta_1$

  - $|t_{1,\text{obs}}|=$ |`r t1oss`| `r segn11` `r t.025` $=t_{n-2;0.025}$ e quindi `r risp11` al livello di significatività $\alpha=0.05$.
  - $|t_{1,\text{obs}}|=$ |`r t1oss`| `r segn12` `r t.005` $=t_{n-2;0.005}$ e quindi `r risp12` al livello di significatività $\alpha=0.01$.

## Il modello di regressione lineare multiplo

Si tratta di un'estensione del modello semplice ma con più variabili indipendenti
\[Y_i=\beta_0+\beta_1X_{i1}+\beta_1X_{i2}+...++\beta_1X_{ik}+\varepsilon_i\]

La nube dei punti non può più essere rappresentata e le soluzioni dei minimi quadrati
tanto meno. 
Il coefficiente di determinazione lineare $R^2$ soffre di grossi limiti all'aumentare del numero
di $x$. 
Per valutare quanto bene il modello si adatta ai dati si devo compiere analisi statistiche ulteriori.

## Analisi dei Residui

Il modello lineare poggia sugli assunti che abbiamo elencato. 
La violazione degli assunti invalida le procedure inferenziali 
La **analisi dei residui** è una serie di procedure diagnostiche per controllare
che gli assunti siano rispettati. 
Le procedure consistono nel produrre statistiche e grafici sui residui osservati
$\hat\varepsilon_i$

### Diagramma dei residui e retta dei residui

Un modo comune per visualizzare i residui consiste nel mettere in ordinata
le $x_i$ e in ascissa i $\hat\varepsilon_i$.
Per costruzione

\[\text{cov}(x,\hat\varepsilon)=0\]

e dunque la **retta dei residui**, che è la retta di regressione tra $x$ e 
$\hat\varepsilon$ è parallela all'asse delle $x$ e coincide con esso. 
Esempio sui 4 punti

```{r 18-regressione-II-11}
plot(x,es,axes=F,pch=16,col=4,ylab = expression(hat(epsilon)))
title("x vs errori")
abline(lsfit(x,es),col=ared)
abline(0,0,lty=3)
segments(x,es,x,0,lty=2)
axis(1,x)
axis(1,mean(x),expression(hat(mu)[X]))
axis(2,es)
axis(2,0)
```

essendo $\hat y_i$ combinazioni lineari degli $x_i$ se mettiamo gli $\hat y_i$ in ordinata
e gli $\varepsilon_i$ in ascissa otteniamo lo stesso grafico

```{r 18-regressione-II-12}
plot(ys,es,axes=F,pch=16,col=4)
title("y stimate vs errori")
abline(lsfit(ys,es),col=ared)
abline(0,0,lty=3)
segments(ys,es,ys,0,lty=2)
axis(1,ys)
axis(1,mean(ys),"my")
axis(2,es)
axis(2,0)

fig.def(3)
```

### Lettura del Diagramma dei residui

Se tutte le assunzioni sono rispettate i residui devono essere distribuiti
in modo uniforme intorno alla retta dei residui

```{r 18-regressione-II-13}

set.seed(2)
x <- sort(rnorm(100,100))
y <- 20+x/2+rnorm(100)

par(mfrow=c(1,2),cex=cex)
plot(x,y,pch=16,axes=F,xlab="x",ylab="y")
axis(1)
axis(2)
ml <- lsfit(x,y)
abline(ml,col=4,lwd=2)

plot(x,ml$residuals,pch=16,axes=F,xlab="x",ylab=expression(hat(epsilon)))
axis(1)
axis(2)
abline(h=0,lty=2)
title("retta dei residui vs x")
```


<!-- ```{r 18-regressione-II-14} -->
<!-- par(mfrow=c(1,2),cex=cex) -->
<!-- plot(x,ml$residuals,pch=16,axes=F,xlab="x",ylab=expression(hat(epsilon)),type="l") -->
<!-- axis(1) -->
<!-- axis(2) -->
<!-- abline(h=0,lty=2) -->
<!-- title("retta dei residui vs x") -->

<!-- plot(cbind(1,x)%*%ml$coefficients,ml$residuals,pch=16,axes=F,xlab=expression(beta[0]+beta[1]~x),ylab=expression(hat(epsilon))) -->
<!-- axis(1) -->
<!-- axis(2) -->
<!-- abline(h=0,lty=2) -->
<!-- title("retta dei residui vs la retta") -->
<!-- par(mfrow=c(1,1),cex=cex) -->

<!-- ``` -->


__Violazione dell'assunto 0 (i punti provengono da una relazione lineare)__

```{r 18-regressione-II-15}
set.seed(1)
x <- sort(rnorm(100,0,.8))
y <- 10+exp(x)+rnorm(100)

par(mfrow=c(1,2),cex=.5)
plot(x,y,pch=16,axes=F,xlab="x",ylab="y")
axis(1)
axis(2)
ml <- lsfit(x,y)
abline(ml,col=4,lwd=2)
#curve(10+exp(x)/2,add=T,lwd=2,col=ared)

plot(x,ml$residuals,pch=16,axes=F,xlab="x",ylab=expression(hat(epsilon)))
axis(1)
axis(2)
abline(h=0,lty=2)
title("retta dei residui vs x")
```


<!-- ```{r 18-regressione-II-16} -->
<!-- par(mfrow=c(1,2),cex=cex) -->
<!-- plot(x,ml$residuals,pch=16,axes=F,xlab="x",ylab=expression(hat(epsilon)),type="l") -->
<!-- axis(1) -->
<!-- axis(2) -->
<!-- abline(h=0,lty=2) -->
<!-- title("retta dei residui vs x") -->

<!-- plot(cbind(1,x)%*%ml$coefficients,ml$residuals,pch=16,axes=F,xlab=expression(beta[0]+beta[1]~x),ylab=expression(hat(epsilon))) -->
<!-- axis(1) -->
<!-- axis(2) -->
<!-- abline(h=0,lty=2) -->
<!-- title("retta dei residui vs la retta") -->
<!-- par(mfrow=c(1,1),cex=cex) -->

<!-- ``` -->

__Violazione degli assunti 2. e 4. (omoschedasticità e indipendenza tra $x$ e $\varepsilon$)__

```{r 18-regressione-II-17}
set.seed(1)
x <- sort(rnorm(100,20,.8))
y <- x+rnorm(100,0,rep(c(.5,1),each=50))

par(mfrow=c(1,2),cex=cex)
plot(x,y,pch=16,axes=F,xlab="x",ylab="y")
axis(1)
axis(2)
ml <- lsfit(x,y)
abline(ml,col=4,lwd=2)
#curve(10+exp(x)/2,add=T,lwd=2,col=ared)

plot(x,ml$residuals,pch=16,axes=F,xlab="x",ylab=expression(hat(epsilon)))
axis(1)
axis(2)
abline(h=0,lty=2)
title("retta dei residui vs x")
```


<!-- ```{r 18-regressione-II-18} -->
<!-- par(mfrow=c(1,2),cex=cex) -->
<!-- plot(x,ml$residuals,pch=16,axes=F,xlab="x",ylab=expression(hat(epsilon)),type="l") -->
<!-- axis(1) -->
<!-- axis(2) -->
<!-- abline(h=0,lty=2) -->
<!-- title("retta dei residui vs x") -->

<!-- plot(cbind(1,x)%*%ml$coefficients,ml$residuals,pch=16,axes=F,xlab=expression(beta[0]+beta[1]~x),ylab=expression(hat(epsilon))) -->
<!-- axis(1) -->
<!-- axis(2) -->
<!-- abline(h=0,lty=2) -->
<!-- title("retta dei residui vs la retta") -->
<!-- par(mfrow=c(1,1),cex=cex) -->

<!-- ``` -->

__Violazione dell'assunto 3. (indipendenza gli $\varepsilon$)__

```{r 18-regressione-II-19}
set.seed(1)
x <- sort(rnorm(100,20,.8))
y <- x+ arima.sim(list(order = c(2,0,2), ar = c(0.7, 0.13), ma = c(2, 3)), n = 100)

par(mfrow=c(1,2),cex=cex)
plot(x,y,pch=16,axes=F,xlab="x",ylab="y")
axis(1)
axis(2)
ml <- lsfit(x,y)
abline(ml,col=4,lwd=2)
#curve(10+exp(x)/2,add=T,lwd=2,col=ared)

plot(x,ml$residuals,pch=16,axes=F,xlab="x",ylab=expression(hat(epsilon)),type="l")
axis(1)
axis(2)
abline(h=0,lty=2)
title("retta dei residui vs x")
par(mfrow=c(1,1),cex=cex)

```

<!-- ```{r 18-regressione-II-20} -->
<!-- par(mfrow=c(1,2),cex=cex) -->
<!-- plot(x,ml$residuals,pch=16,axes=F,xlab="x",ylab=expression(hat(epsilon)),type="l") -->
<!-- axis(1) -->
<!-- axis(2) -->
<!-- abline(h=0,lty=2) -->
<!-- title("retta dei residui vs x") -->

<!-- plot(cbind(1,x)%*%ml$coefficients,ml$residuals,pch=16,axes=F,xlab=expression(beta[0]+beta[1]~x),ylab=expression(hat(epsilon))) -->
<!-- axis(1) -->
<!-- axis(2) -->
<!-- abline(h=0,lty=2) -->
<!-- title("retta dei residui vs la retta") -->
<!-- par(mfrow=c(1,1),cex=cex) -->

<!-- ``` -->

__Violazioni dell'assunto 6. (normalità dei residui)__

Per diagnosticare se i residui provengono da una normale ci sono diverse tecniche.
Per esempio l'istogramma dei residui: si costruisce l'istogramma di frequenza e lo si compara con la normale.
Se gli assunti sono rispettati ci aspetteremmo una situazione del genere

```{r 18-regressione-II-21}

set.seed(7)
x <- sort(rnorm(200,100))
y <- 20+x/2+rnorm(200,0,.5)

ml <- lsfit(x,y)

s <- sc(ml$residuals)

hist(ml$residuals,15,probability = T,xlab=expression(hat(epsilon)),main = "Istogramma dei Residui")
curve(dnorm(x,0,s),add=T,col=ared)

# qqnorm(ml$residuals)
```


Esempio di assunto **non** rispettato

```{r 18-regressione-II-22}

set.seed(9)
x <- sort(rnorm(1000,100))
y <- 20+x/2+rt(1000,3)

ml <- lsfit(x,y)

s <- sc(ml$residuals)

hist(ml$residuals,100,probability = T,xlab=expression(hat(epsilon)),main = "Istogramma dei Residui")
curve(dnorm(x,0,s),add=T,col=ared)

# qqnorm(ml$residuals)
```

### Normal QQ plot

Si tratta di un grafico che mette sull'asse delle x i _quantile_ (percentili in inglese) teorici della normale
e in ordinata i _quantile_ osservati dei residui sul campione.

Si crea una tabella dei percentili degli $\hat\varepsilon_{(1)}$
<div/>
 errori ordinati        | ordine del percentile | percentile teorico
 -----------------------|-----------------------|-------------------
$\hat\varepsilon_{(1)}$ | $1/n$                 | $z_{1/n}$
$\hat\varepsilon_{(2)}$ | $2/n$                 | $z_{2/n}$
$...$                   | $...$                 | $...$
dove $z_{i/n}$ è il percentile di un $Z\sim N(0,1)$:
\[z_{i/n}: P(Z\leq z_{i/n})=i/n\]

Se le $z_{i/n}$ e le $\hat\varepsilon_i$ giacciono su una retta,
allora gli errori si possono assumere normali, tanto più i punti si allontanano
tanto più l'ipotesi è violata.


```{r 18-regressione-II-23}
par(mfrow=c(1,2),cex=.5)
set.seed(7)
x <- sort(rnorm(200,100))
y <- 20+x/2+rnorm(200,0,.5)

ml <- lsfit(x,y)

s <- sc(ml$residuals)

# hist(ml$residuals,15,probability = T,xlab=expression(hat(epsilon)),main = "Istogramma dei Reisudi")
# curve(dnorm(x,0,s),add=T,col=ared)

qqnorm(ml$residuals/sc(ml$residuals),asp=1,main = "QQ plot: assunti rispettati")
abline(0,1)

x <- sort(rnorm(100,100))
y <- 20+x/2+rt(100,2)

ml <- lsfit(x,y)

s <- sc(ml$residuals)

# hist(ml$residuals,100,probability = T,xlab=expression(hat(epsilon)),main = "Istogramma dei Reisudi")
# curve(dnorm(x,0,s),add=T,col=ared)

qqnorm(ml$residuals/sc(ml$residuals),asp=1,main = "QQ plot: assunti non rispettati")
abline(0,1)

par(mfrow=c(1,1),cex=cex)
```


## Punti di leva, Outliers e punti influenti

Ci sono tre tipi di dati anomali, in particolare

- Outlier: osservazione con residuo anomale (sulle $y$)
- Leverage: (punto di leva), valore anomalo (sulle $x$)
- Influence Points: (punti influenti) osservazioni con
comportamento anomalo che influenzano
notevolmente i risultati


```{r 18-regressione-II-24}
set.seed(1)
x <- c(rnorm(10,10),15,11)
y <- x+rnorm(12,0,.2)
y[11] <- 11
y[12] <- 13

plot(x,y,pch=16)
points(x[11],y[11],pch=16,col=ared)
segments(x0 = x[11],y0= y[11],x1=mean(x),col=ared,lty=3)
text(14,11,"punto di leva",col=ared,pos=1)

md <- lsfit(x,y)$coefficients

points(x[12],y[12],pch=16,col=4)
segments(x0 = x[12],y0= y[12],y1=md[1]+md[2]*x[12],col=4,lty=3)
text(11.5,12,"outlier",col=4,pos=1)
abline(lsfit(x[1:10],y[1:10]),lty=5,col="darkred")
text(13,12,"retta esclusi i due\n punti di influenza",col="darkred")
abline(lsfit(x,y))
abline(v=mean(x),lty=2)
abline(h=mean(y),lty=2)
h <- 1/11 + (x-mean(x))^2/(11*s2c(x))
```


### Punti di leva

Possiamo misurare la distanza di ogni singola $x_i$ dalla propria media $\bar x$ con la seguente misura
chiamata _leva_
\[
h_{i} = \frac{1} {n} + \frac{(x_{i} - \bar{x})^{2}} {n \hat\sigma_{X}^{2}}
\]

Valori di $x$ con indice di leva alto sono _lontani_ dal centro. 
In particolare se
\[h_i>\frac 2 n\]
allora $x_i$ è un _punto di leva_. 
I Punti di leva possono avere effetto sul calcolo dei coefficienti di regressione.
I punti a alta leva (con $h_{i} > 2/n$) sono nei
valori estremi della variabile esplicativa e sono potenzialmente
influenti, nel senso che possono influenzare in misura rilevante
la pendenza della RdR.
Infatti, i punti di leva possono portare a risultati forvianti per esempio

```{r 18-regressione-II-25}
set.seed(1)
x <- c(rep(0,times=10),1)
y <- c(rnorm(10,20),21.5)

plot(x,y,pch=16)
abline(lsfit(x,y))

h <- 1/11 + (x-mean(x))^2/(11*s2c(x))
text(.95,21,"h=1>2/11=0.18",pos = 2)
text(.05,19.5,"h=0.1<2/11=0.18",pos=4)
points(x[11],y[11],pch=16,col=ared)
abline(v=mean(x),lty=2)
abline(h=mean(y),lty=2)

```

```{r 18-regressione-II-1, results='asis', echo=FALSE, out.extra=''}

tab <- data.frame(x,y,h)
names(tab) <- c("$x_i$","$y_i$","$h_i$")
kable(t(tab),booktabs = T, escape = F, linesep = "",col.names = NULL,row.names = T) %>%
  column_spec(length(x)+1, color = ared)
```


Ma non sempre un punto a leva alta è un punto influente

```{r 18-regressione-II-26}
set.seed(1)
x <- c(rnorm(10,10),15)
y <- x+rnorm(11,0,.5)
y[11] <- 14.5
plot(x,y,pch=16)
points(x[11],y[11],pch=16,col=ared)
abline(lsfit(x,y))
abline(lsfit(x[-11],y[-11]),lty=5,col="darkred")
abline(v=mean(x),lty=2)
abline(h=mean(y),lty=2)
h <- 1/11 + (x-mean(x))^2/(11*s2c(x))
```

```{r 18-regressione-II-2, results='asis', echo=FALSE, out.extra=''}

tab <- data.frame(x,y,h)
names(tab) <- c("$x_i$","$y_i$","$h_i$")
kable(t(tab),booktabs = T, escape = F, linesep = "",col.names = NULL,row.names = T,digits = 2) %>%
  column_spec(length(x)+1, color = ared)
```


### I residui Studentizzati

La studentizzazione è una specie di standardizzazione nella quale
si tiene conto anche dei valori di leva. 
I residui studentizzati sono dati da:
\[
\tilde{\varepsilon}_{i}  =  \frac{\hat{\varepsilon}_{i}}{S_{\varepsilon} \sqrt{1 - h_{i}}} \sim t_{n-2}
\]

Si preferiscono i residui studentizzati perché incorporano
le leve e sono più confrontabili. 
La distribuzione è $t$ con $n-2$ gradi di libertà, 
se per qualche $i$, $|\tilde{\varepsilon}_{i}|>t_{\alpha;n-2}$ allora siamo in presenza di punti anomali che diventano **punti influenti** per il calcolo di $\hat\beta_0$ e$\hat\beta_1$.

__Esempi__

```{r 18-regressione-II-27}
set.seed(1)

par(mfrow=c(1,2),cex=cex)
x <- c(rnorm(10,10),13)
y <- x+rnorm(11,0,.1)
y[11] <- 11

md <- lsfit(x,y)
plot(x,y,pch=16)
title("Dato anomale per x ma non per y")
points(x[11],y[11],pch=16,col=ared)
abline(md)
abline(lsfit(x[-11],y[-11]),lty=2,col=ared)
h <- 1/11 + (x-mean(x))^2/(11*s2c(x))

rr<- cor(x,y)

ss <- sd(y)*sqrt(1-rr^2)*sqrt(1-h)
etil <- md$residuals/ss

text(x[-11]+.25,y[-11],round(etil[-11],3),cex=.5)
text(x[11]-.25,y[11],round(etil[11],3),col=ared)


x <- c(rnorm(11,10))
y <- x+rnorm(11,0,.05)
y[5] <- 13

md <- lsfit(x,y)
plot(x,y,pch=16)
title("Dato anomalo per y ma non per x")

points(x[5],y[5],pch=16,col=ared)
abline(md)
abline(lsfit(x[-5],y[-5]),lty=2,col=ared)
h <- 1/11 + (x-mean(x))^2/(11*s2c(x))

rr<- cor(x,y)

ss <- sd(y)*sqrt(1-rr^2)*sqrt(1-h)
etil <- md$residuals/ss

text(x[-5]+.25,y[-5],round(etil[-5],3),cex=.5)
text(x[5]-.25,y[5],round(etil[5],3),col=ared)


```

```{r 18-regressione-II-28}

par(mfrow=c(1,2),cex=cex)


x <- c(rnorm(10,10),13)
y <- x+rnorm(11,0,.05)
y[11] <- 15

md <- lsfit(x,y)
plot(x,y,pch=16)

title("Dato anomalo per x e per y,\n in direzione della covarianza")

points(x[11],y[11],pch=16,col=ared)
abline(md)
abline(lsfit(x[-11],y[-11]),lty=2,col=ared)
h <- 1/11 + (x-mean(x))^2/(11*s2c(x))

rr<- cor(x,y)

ss <- sd(y)*sqrt(1-rr^2)*sqrt(1-h)
etil <- md$residuals/ss

text(x[-11]+.25,y[-11],round(etil[-11],3),cex=.5)
text(x[11]-.25,y[11],round(etil[11],3),col=ared)



x <- c(rnorm(10,10),15)
y <- x+rnorm(11,0,.05)
y[11] <- 6

md <- lsfit(x,y)
plot(x,y,pch=16)
title("Dato anomalo per x e per y,\n in direzione contraria della covarianza")

points(x[11],y[11],pch=16,col=ared)
abline(md)
abline(lsfit(x[-11],y[-11]),lty=2,col=ared)
h <- 1/11 + (x-mean(x))^2/(11*s2c(x))

rr<- cor(x,y)

ss <- sd(y)*sqrt(1-rr^2)*sqrt(1-h)
etil <- md$residuals/ss

text(x[-11]+.25,y[-11],round(etil[-11],3),cex=.5)
text(x[11]-.25,y[11],round(etil[11],3),col=ared)

par(mfrow=c(1,1),cex=cex)
```

__Esempio numerico__

```{r 18-regressione-II-29}
set.seed(1)

x <- c(rnorm(10,10),13)
y <- x+rnorm(11,0,.1)
y[11] <- 11

md <- lsfit(x,y)
h <- 1/11 + (x-mean(x))^2/(11*s2c(x))
rr<- cor(x,y)
ss <- sd(y)*sqrt(1-rr^2)*sqrt(1-h)
etil <- md$residuals/ss

tab1 <- data.frame(1:11,x,y,h,etil)

names(tab1) <- c("$i$","$x_i$","$y_i$","$h_i$","$\\tilde\\varepsilon_i$")


kable(t(tab1[,-1]),booktabs = T, escape = F, linesep = "",col.names = NULL,row.names = T,digits = 2) %>%
  column_spec(length(x)+1, color = ared)

plot(x,y,pch=16)
title("Dato anomale per x ma non per y")
points(x[11],y[11],pch=16,col=ared)
abline(md)
abline(lsfit(x[-11],y[-11]),lty=2,col=ared)
h <- 1/11 + (x-mean(x))^2/(11*s2c(x))

rr<- cor(x,y)

ss <- sd(y)*sqrt(1-rr^2)*sqrt(1-h)
etil <- md$residuals/ss

text(x[-11]+.25,y[-11],round(etil[-11],3),cex=.5)
text(x[11]-.25,y[11],round(etil[11],3),col=ared)

```

## Relazione tra $Y|X$ e $X|Y$

Fin'ora abbiamo considerato il modello
\[y_i = \beta_0+\beta_1+\varepsilon_i\]

Supponiamo di invertire il ruolo della $x$ con la $y$
\[x_i = \alpha_0+\alpha_1y_i+\delta_i\]

Le stime dei minimi quadrati sono analoghe
\[
\begin{aligned}
\hat\beta_1 &=\frac{\text{cov}(x,y)}{\hat\sigma_X^2} & \hat\alpha_1 &=\frac{\text{cov}(x,y)}{\hat\sigma_Y^2}\\
\hat\beta_0&=\bar y-\hat\beta_1\bar x & \hat\alpha_0 &=\bar x-\hat\alpha_1\bar y
\end{aligned}
\]

In generale
\[\hat\beta_0\neq\hat\alpha_0,\qquad\hat\beta_1\neq\hat\alpha_1\]

In particolare
\[\hat\beta_1=\hat\alpha_1, \text{ se e solo se }\hat\sigma_X^2=\hat\sigma_Y^2\]

Mentre
\[\hat\beta_0=\hat\alpha_0, \text{ se e solo se }\hat\beta_1=\hat\alpha_1, \text{ e se }\bar y=\bar x\]

### Relazione tra gli $\alpha$ i $\beta$ ed $r$

Essendo:
\begin{eqnarray*}
\hat\beta_1 &=&\frac{\text{cov}(x,y)}{\hat\sigma_X^2}\\
\hat\beta_1 &=&\frac{\hat\sigma_Y}{\hat\sigma_Y}\frac{\text{cov}(x,y)}{\hat\sigma_X^2}\\
\hat\beta_1 &=&\frac{\hat\sigma_Y}{\hat\sigma_X}\frac{\text{cov}(x,y)}{\hat\sigma_Y\hat\sigma_X}\\
\hat\beta_1 &=&\frac{\hat\sigma_Y}{\hat\sigma_X}r
\end{eqnarray*}

Quindi:
\begin{eqnarray*}
\hat\alpha_1 &=&\frac{\text{cov}(x,y)}{\hat\sigma_Y^2}\\
\hat\alpha_1 &=&\frac{\hat\sigma_X}{\hat\sigma_Y}r
\end{eqnarray*}


**Graficamente**

```{r 18-regressione-II-30}
fig.def(3.,3.)
```

```{r 18-regressione-II-31}
set.seed(23)
n <- 25
x <- rnorm(n,2)
y <- x+rnorm(n,10)

plot(x,y,pch=16,asp=sd(x)/sd(y))
abline(lsfit(x,y),col=4)
ystim <- cbind(1,x)%*%lsfit(x,y)$coefficients
xstim <- cbind(1,y)%*%lsfit(y,x)$coefficients
mxy <-(lsfit(y,x))
ygr <- seq(-20,250,length.out = 2)
xgr <- cbind(1,ygr)%*%mxy$coefficients
lines(xgr,ygr,col=10)
# segments(x,y,x,ystim,lty=2,col=4)
# segments(x,y,xstim,y,lty=2,col=ared)

#i <- 2
#i <- 3
#i <- 5
#i <- 8
i <- c(2,3,5,8,9)
segments(x[i],y[i],x[i],ystim[i],lty=2,col=4)
segments(x[i],y[i],xstim[i],y[i],lty=2,col=ared)

abline(v=mean(x),lty=2)
abline(h=mean(y),lty=2)
text(4,13,expression(y==hat(beta)[0]+hat(beta)[1]*x),col=4)
text(3.5,14,expression(x==hat(alpha)[0]+hat(alpha)[1]*y),col=ared)

fig.def(2.6)
```

### Regressione sulle variabili standardizzate

Se standardizziamo sia $x$ che $y$, otteniamo
\[z_{Xi}=\frac{x_i-\bar x}{\hat\sigma_X}\qquad z_{Yi}=\frac{y_i-\bar y}{\hat\sigma_Y}\]

Abbiamo eliminato l'unità di misura sia da $x$ che da $y$ e centrato la nube dei dati

```{r 18-regressione-II-32}
par(mfrow=c(1,2),cex=cex)
plot(x,y,axes=F,xlab="x",ylab = "y")
axis(1)
axis(2)
abline(v=mean(x),lty=2)
abline(h=mean(y),lty=2)

zx <- (x-mean(x))/sc(x)
zy <- (y-mean(y))/sc(y)

plot(zx,zy,axes=F,xlab=expression(z[X]),ylab = expression(z[Y]))
axis(1)
axis(2)
abline(v=0,lty=2)
abline(h=0,lty=2)


par(mfrow=c(1,1),cex=cex)
```



I dati standardizzati hanno media zero e varianza 1
\[\begin{aligned}
\frac 1 n \sum_{i=1}^n z_{Xi} &=0 & \frac 1 n \sum_{i=1}^n z_{Xi}^2 &=1\\
\frac 1 n \sum_{i=1}^n z_{Yi} &=0 & \frac 1 n \sum_{i=1}^n z_{Yi}^2 &=1\\
\end{aligned}
\]

Dalle proprietà del coefficiente di correlazione
\[r_{Z_X,Z_Y}=r_{X,Y}=r\]

E dunque
\[\begin{aligned}
r &=\frac{\text{cov}(z_X,z_Y)}{\hat\sigma_{Z_X}\hat\sigma_{Z_Y}}\\
 &= \frac{\text{cov}(z_X,z_Y)}{1\times 1}\\
 &= \text{cov}(z_X,z_Y)
\end{aligned}
\]

Si considerino i due modelli
\[z_{Yi}=\beta_{0Z}+\beta_{1Z}\cdot z_{Xi}+\varepsilon_{Zi}, \qquad z_{Xi}=\alpha_{0Z}+\alpha_{1Z}\cdot z_{Yi}+\delta_{Zi}\]

Allora
\[\begin{aligned}
\hat\beta_{1Z} &=\frac{\text{cov}(z_X,z_Y)}{\hat\sigma^2_{Z_X}} & \hat\alpha_{1Z} &=\frac{\text{cov}(z_X,z_Y)}{\hat\sigma^2_{Z_Y}}\\
 &=\frac{r}{1^2}=r & &=\frac{r}{1^2}=r\\
\hat\beta_{0Z} &=\bar z_{Y} - \hat\beta_{1Z} \bar z_X &\hat\alpha_{0Z} &=\bar z_X - \hat\alpha_{1Z} \bar z_Y\\
 &=  0 + r \cdot 0 =0 & &=  0 + r \cdot 0 =0
\end{aligned}
\]

**Graficamente**

```{r 18-regressione-II-33}
par(mfrow=c(1,2),cex=cex)
plot(x,y,axes=F,xlab="x",ylab = "y")
axis(1)
axis(2)
abline(v=mean(x),lty=2)
abline(h=mean(y),lty=2)
zx <- (x-mean(x))/sc(x)
zy <- (y-mean(y))/sc(y)
text(3,12,expression(y==hat(beta)[0]+hat(beta)[1]*x),col=4)
text(2,14,expression(x==hat(alpha)[0]+hat(alpha)[1]*y),col=ared)
abline(lsfit(x,y),col=4)
ystim <- cbind(1,x)%*%lsfit(x,y)$coefficients
xstim <- cbind(1,y)%*%lsfit(y,x)$coefficients
mxy <-(lsfit(y,x))
ygr <- seq(-20,250,length.out = 2)
xgr <- cbind(1,ygr)%*%mxy$coefficients
lines(xgr,ygr,col=10)


plot(zx,zy,axes=F,xlab=expression(z[X]),ylab = expression(z[Y]))
axis(1)
axis(2)
abline(v=0,lty=2)
abline(h=0,lty=2)
abline(lsfit(zx,zy),col=4)
ystim <- cbind(1,x)%*%lsfit(zx,zy)$coefficients
xstim <- cbind(1,y)%*%lsfit(zy,zx)$coefficients
mxy <-(lsfit(zy,zx))
ygr <- seq(-20,250,length.out = 2)
xgr <- cbind(1,ygr)%*%mxy$coefficients
lines(xgr,ygr,col=10)
text(1,0.2,expression(z[Y]==r*Z[X]),col=4)
text(1,1.5,expression(z[X]==r*Z[Y]),col=ared,pos=4)

par(mfrow=c(1,1),cex=cex)
```

<!--chapter:end:18-regressione-II.Rmd-->



```{r setupchi, include=FALSE}
rm(list = ls())

source("intro.R")
  
mumax <- 5
za2 <- round(qnorm(.975),2)
```

# Il Test Chi-Quadro 

## Test di Significatività pura

I test che abbiamo visto fin'ora prevedono l'ipotesi su un parametro
$\theta$. Ovvero tutti i test che abbiamo visto fin'ora sono del tipo

\[
\begin{cases}
H_0:\theta=\theta_0\\
H_1:\theta\in\Theta_1
\end{cases}
\]

I test di significatività pura rispondono a domande diverse, come per esempio testare se
due variabili, alla luce dei dati, sono o non sono indipendenti o se la distribuzione osservata
di una variabile è compatibile con un modello probabilistico. In un test di 
significatività pure solo l'ipotesi $H_0$ viene elicitata, mentre l'ipotesi $H_1$ non ha molto senso. 
Il test del $\chi^2$ consente di risolvere diversi test di significatività pura

## Associazione tra due variabili

 Abbiamo visto nel modello di regressione le statistiche che misurano
_l'associazione lineare_ tra due variabili quantitative. 
Abbiamo osservato che l'associazione lineare **non** l'unico tipo di associazione
osservabile tra $x$ ed $y$. 
Il modello di regressione lineare funziona solo se $x$ e $y$ sono quantitative.
Come misurare l'associazione tra $x$ e $y$ se sono entrambe categoriali?

### Le tavole di contingenza

Sono tabelle che consentono di vedere le _frequenze congiunte_ delle osservazioni a coppie

:::: {.example}
100 individui dalla città $A$, 100 dalla città $B$ e 100 dalla città $C$. 
Abbiamo misurato $X=$ Genere ($M$, $F$), $Y=$ abbonamento allo stadio (Sì, No).
In tutte e tre le città abbiamo osservato 50 $M$ e 50 $F$, e abbiamo osservato 50 abbonati
e 50 non abbonati.
\[\tiny
\begin{array}{c|rr|r}
 & \text{Città A} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
   M & 50 & 0 & \bf 50\\ 
   F & 0 & 50  &\bf 50\\ \hline
  \bf\text{Tot} &\bf 50 &\bf 50 &\bf 100\\ 
\end{array}
\qquad
\begin{array}{c|rr|r}
 & \text{Città B} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
  M & 35 & 15 & \bf50\\ 
  F & 15 & 35  &\bf50\\ \hline
  \bf\text{Tot} &\bf 50 &\bf 50 &\bf 100\\ 
\end{array}
\qquad
\begin{array}{c|rr|r}
 & \text{Città C} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
  M & 25 & 25 & \bf50\\ 
  F & 25 & 25  &\bf50\\ \hline
  \bf\text{Tot} &\bf 50 &\bf 50 &\bf 100\\ 
\end{array}
\]

 Nella città $A$ solo i maschi hanno l'abbonamento allo stadio 
 **perfetta** associazione tra Genere e Abbonamento. 
 Nella città $B$ molti maschi hanno l'abbonamento allo stadio ma non solo 
 c'è associazione tra Genere e Abbonamento. 
 Nella città $C$ non c'è distinzione tra genere e passione per il pallone 
  **Non** c'è associazione tra Genere e Abbonamento.

Ma, come vedremo nel prossimo esempio, non è sempre facile individuare dove non c'è associazione.
::::

:::: {.example name="Popolazione finita, estrazioni SR (Efficienza)"}

100 individui dalla città $A$, 100 dalla città $B$ e 100 dalla città $C$. 
Abbiamo misurato $X=$ Genere ($M$, $F$), $Y=$ abbonamento allo stadio (Sì, No).
In tutte e tre le città abbiamo osservato 60 $M$ e 40 $F$, e abbiamo osservato 60 abbonati
e 40 non abbonati.
\[\tiny
\begin{array}{c|rr|r}
 & \text{Città A} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
   M & 60 & 0 & \bf 60\\ 
   F & 0 & 40  &\bf 40\\ \hline
  \bf\text{Tot} &\bf 60 &\bf 40 &\bf 100\\ 
\end{array}
\qquad
\begin{array}{c|rr|r}
 & \text{Città B} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
  M & 40 & 20 & \bf60\\ 
  F & 20 & 20  &\bf50\\ \hline
  \bf\text{Tot} &\bf 60 &\bf 40 &\bf 100\\ 
\end{array}
\qquad
\begin{array}{c|rr|r}
 & \text{Città C} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
  M & 36 & 24 & \bf60\\ 
  F & 24 & 16  &\bf40\\ \hline
  \bf\text{Tot} &\bf 60 &\bf 40 &\bf 100\\ 
\end{array}
\]

 Nella città $A$ solo i maschi hanno l'abbonamento allo stadio **perfetta** associazione tra Genere e Abbonamento. Nella città $B$ molti maschi hanno l'abbonamento allo stadio ma non solo,  
 c'è associazione tra Genere e Abbonamento.
 Nella città $C$ non c'è distinzione tra genere e passione per il pallone 
  **non** c'è associazione tra Genere e Abbonamento ma non è evidente da vedere.
::::

### Un passo indietro: il concetto di indipendenza

 Se $X$ e $Y$ sono due VC **INDIPENDENTI** allora
\[P(X=x~\cap~Y=y)=P(X=x)P(Y=y)\]

 Se per esempio 
\[S_X=\{0,1\},\qquad S_Y=\{0,1\}\]

 e
\[P(X=1)=\pi_X\qquad P(Y=1)=\pi_y\]

 Sotto regime di indipendenza
\[
\begin{array}{c|ll|r}
 &Y=0 &Y=1 &\\
  \hline
  X=0 & \pi_{11}=P(X=0\cap Y=0) = (1-\pi_X)(1-\pi_Y)& 
  \pi_{12}=P(X=0\cap Y=1) =(1-\pi_X)\pi_Y & 1-\pi_X\\ 
  X=1 & \pi_{21}=P(X=1\cap Y=0)= \pi_X(1-\pi_Y)& 
  \pi_{22}=P(X=1\cap Y=1) =\pi_X\pi_Y & \pi_X\\ \hline
   & 1-\pi_Y & \pi_Y & 1\\ 
\end{array}
\]



 Sia $X$ la VC che registra testa nel lancio di una moneta perfetta e $Y$ una VC che 
registra la faccia numero 6 dal lancio di un dado perfetto

 e
\[P(X=1)=\frac 1 2\qquad P(Y=1)=\frac 1 6\]

 Sotto regime di indipendenza
\[
\begin{array}{c|ll|r}
 &Y=0 &Y=1 &\\
  \hline
  X=0 & `r .5*5/6` & `r .5*1/6` & \frac12\\ 
  X=1 & `r .5*5/6` & `r .5*1/6` & \frac12\\ 
\hline
   & \frac 56 &\frac16 & 1\\ 
\end{array}
\]

 L'indipendenza è imposta dal meccanismo di generazione di casualità
 Ovvero, se io lancio 100 volte il dado e la moneta mi aspetto, in media
di osservare 41.7 volte $X=0$ e $Y=0$, 41.7 volte $X=1$ e $Y=0$, 8.3 volte
$X=0$ e $Y=1$ e 8.3 volte $X=1$ e $Y=1$.
 Se dopo 100 lanci le frequenze osservate sono molto diverse dalle frequenze 
teoriche in regime di indipendenza, potrei dubitare dell'ipotesi di indipendenza

### Estensione a più di due modalità

 Se abbiamo due VC categoriali $X$ e $Y$ con supporto
\[S_X=\{x_1,...,x_I\} \qquad S_Y= \{y_1,...,y_J\}\] definiamo
le **probabilità congiunte**
\[\pi_{ij}=\text{la probabilità che $X=x_i$ e $Y=y_j$}\]

 La distribuzione di probabilità doppia è
\[
\begin{array}{c|cccccc|c}
 &y_1 &y_2 &\ldots &y_j &\ldots &y_J &\\
  \hline
  x_1 & \pi_{11} & \pi_{12} & \ldots &\pi_{1j} &\ldots &\pi_{1J} & \pi_{1\bullet}\\
  x_2 & \pi_{21} & \pi_{22} & \ldots &\pi_{2j} &\ldots &\pi_{2J} & \pi_{2\bullet}\\
\vdots& \vdots & \vdots &\ddots &\vdots & &\vdots &\vdots \\
  x_i & \pi_{i1} & \pi_{i2} & \ldots &\pi_{ij} &\ldots & \pi_{iJ}& \pi_{i\bullet}\\
\vdots& \vdots & \vdots & &\vdots &\ddots &\vdots &\vdots \\
  x_I & \pi_{I1} & \pi_{I2} & \ldots &\pi_{Ij} &\ldots &\pi_{IJ} &\pi_{I\bullet}\\
\hline
   & \pi_{\bullet 1}& \pi_{\bullet 2}& \ldots& \pi_{\bullet j}& \ldots& \pi_{\bullet J}& 1\\ 
\end{array}
\]

 Le **probabilità marginali** sono
\[\pi_{i\bullet}=\pi_{i1}+\ldots+\pi_{iJ},\qquad \pi_{\bullet j}=\pi_{1j}+\ldots+\pi_{Ij} \]


### Esempio

```{r 19-chi-quadro-1}
px <- c(.6,.4)
py <- c(.3,.5,.2)

pxy <- outer(px,py,"*")

n <- 150

nxy <- pxy*n
```

 Nella città $A$ ci sono `r nxy[1,1]` individui occupati con la licenza media , `r nxy[1,2]` individui occupati con la licenza superiore, `r nxy[1,3]` individui occupati laureati, `r nxy[1,3]` individui disoccupati con la licenza media , `r nxy[2,2]` individui disoccupati con la licenza superiore e `r nxy[2,3]` laureati
\[
\begin{array}{c|ccc|r}
         &M             &   S              &L  & \\   \hline
Occupato    & `r nxy[1,1]`     & `r nxy[1,2]`     & `r nxy[1,3]`     & `r sum(nxy[1,])`\\
Disoccupato & `r nxy[2,1]`     & `r nxy[2,2]`     & `r nxy[2,3]`     & `r sum(nxy[2,])`\\ \hline
         & `r sum(nxy[,1])` & `r sum(nxy[,2])` & `r sum(nxy[,3])` & `r n`
\end{array}
\]

 Qual è la probabilità di estrarre un soggetto che abbia un dato titolo di studio e una data condizione occupazionale?
\[
\begin{array}{c|ccc|c}
         &M & S &L  & \\   \hline
Occupato    & \pi_{11}=\frac {`r nxy[1,1]`}{`r n`}=`r pxy[1,1]`    
         & \pi_{12}=\frac{`r nxy[1,2]`}{`r n`}=`r pxy[1,2]`    
         & \pi_{13}=\frac{`r nxy[1,3]`}{`r n`}=`r pxy[1,3]` 
         & \pi_{1\bullet}=\frac{`r sum(nxy[1,])`}{`r n`}=`r sum(pxy[1,])`\\
Disoccupato & \pi_{21}=\frac {`r nxy[2,1]`}{`r n`}=`r pxy[2,1]`    
         & \pi_{22}=\frac{`r nxy[2,2]`}{`r n`}=`r pxy[2,2]`    
         & \pi_{23}=\frac{`r nxy[2,3]`}{`r n`}=`r pxy[2,3]` 
         & \pi_{2\bullet}=\frac{`r sum(nxy[2,])`}{`r n`}=`r sum(pxy[2,])`\\ \hline
         & \pi_{\bullet1}=\frac{`r sum(nxy[,1])`}{`r n`}=`r sum(pxy[,1])`    
         & \pi_{\bullet2}=\frac{`r sum(nxy[,2])`}{`r n`}=`r sum(pxy[,2])` 
         & \pi_{\bullet3}=\frac{`r sum(nxy[,3])`}{`r n`}=`r sum(pxy[,3])`
         & \frac {`r n`}{`r n`}=1.0
\end{array}
\]

 Titolo di studio e occupazione sono **indipendenti**?

 Sì, in quanto
\[\pi_{ij}=\pi_{i\bullet}\pi_{\bullet j},~\forall i,j\]

 Ad esempio
\[\pi_{11}=`r pxy[1,1]`=`r sum(pxy[1,])`\times`r sum(pxy[,1])`=\pi_{1\bullet}\pi_{\bullet 1}\]


 Ad esempio
\[\pi_{23}=`r pxy[2,3]`=`r sum(pxy[2,])`\times`r sum(pxy[,3])`=\pi_{2\bullet}\pi_{\bullet 3}\]




```{r 19-chi-quadro-2}
px <- c(.6,.4)
py <- c(.3,.5,.2)


n <- 150


nxy <- matrix(c(0,45,60,15,30,0),2)
# nxy <- cbind(nxy,rowSums(nxy))
# nxy <- rbind(nxy,colSums(nxy))

pxy <- nxy/n
```

 Nella città $B$ osserviamo
\[
\begin{array}{c|ccc|r}
         &M &S &L  & \\   \hline
Occupato    & `r nxy[1,1]`     & `r nxy[1,2]`     & `r nxy[1,3]`     & `r sum(nxy[1,])`\\
Disoccupato & `r nxy[2,1]`     & `r nxy[2,2]`     & `r nxy[2,3]`     & `r sum(nxy[2,])`\\ \hline
         & `r sum(nxy[,1])` & `r sum(nxy[,2])` & `r sum(nxy[,3])` & `r n`
\end{array}
\]

 Qual è la probabilità di estrarre un oggetto che abbia un dato titolo di studio e una data condizione occupazionale?
\[
\begin{array}{c|ccc|c}
         &M &S &L  & \\   \hline
Occupato    & \pi_{11}=\frac {`r nxy[1,1]`}{`r n`}=`r pxy[1,1]`    
         & \pi_{12}=\frac{`r nxy[1,2]`}{`r n`}=`r pxy[1,2]`    
         & \pi_{13}=\frac{`r nxy[1,3]`}{`r n`}=`r pxy[1,3]` 
         & \pi_{1\bullet}=\frac{`r sum(nxy[1,])`}{`r n`}=`r sum(pxy[1,])`\\
Disoccupato & \pi_{21}=\frac {`r nxy[2,1]`}{`r n`}=`r pxy[2,1]`    
         & \pi_{22}=\frac{`r nxy[2,2]`}{`r n`}=`r pxy[2,2]`    
         & \pi_{23}=\frac{`r nxy[2,3]`}{`r n`}=`r pxy[2,3]` 
         & \pi_{2\bullet}=\frac{`r sum(nxy[2,])`}{`r n`}=`r sum(pxy[2,])`\\ \hline
         & \pi_{\bullet1}=\frac{`r sum(nxy[,1])`}{`r n`}=`r sum(pxy[,1])`    
         & \pi_{\bullet2}=\frac{`r sum(nxy[,2])`}{`r n`}=`r sum(pxy[,2])` 
         & \pi_{\bullet3}=\frac{`r sum(nxy[,3])`}{`r n`}=`r sum(pxy[,3])`
         & \frac {`r n`}{`r n`}=1.0
\end{array}
\]


 Titolo di studio e occupazione sono **indipendenti**?

 No, in quanto
\[\pi_{ij}\neq\pi_{i\bullet}\pi_{\bullet j}\]

 Ad esempio
\[\pi_{11}=`r pxy[1,1]`\neq`r sum(pxy[1,])`\times`r sum(pxy[,1])`=\pi_{1\bullet}\pi_{\bullet 1}\]


 Ad esempio
\[\pi_{23}=`r pxy[2,3]`\neq`r sum(pxy[2,])`\times`r sum(pxy[,3])`=\pi_{2\bullet}\pi_{\bullet 3}\]


### Dalla popolazione al campione

Abbiamo usato le due città come se fossero l'intera popolazione.
 Non sempre possiamo valutare l'intera popolazione. 
 Spesso abbiamo l'osservazione di $n$ coppie di VC categoriali estratte dalla 
popolazione di riferimento.
 Da una popolazione dove $X$ ed $Y$ sono indipendenti, ci aspettiamo che $n$
loro realizzazioni fotografino l'indipendenza che c'è in popolazione.


### Notazione formale per le tavole di contingenza

 Se abbiamo due variabili categoriali $X$ e $Y$ con supporto
\[S_X=\{x_1,...,x_k\} \qquad S_Y= \{y_1,...,y_J\}\] definiamo
le **frequenze assolute congiunte**
\[n_{ij}=\text{il numero di volte che $X=x_i$ e $Y=y_j$}\]

 La tabella di contingenza si presenta
\[
\begin{array}{c|cccccc|c}
 &y_1 &y_2 &\ldots &y_j &\ldots &y_J &\\
  \hline
  x_1 & n_{11} & n_{12} & \ldots &n_{1j} &\ldots &n_{1J} & n_{1\bullet}\\
  x_2 & n_{21} & n_{22} & \ldots &n_{2j} &\ldots &n_{2J} & n_{2\bullet}\\
\vdots& \vdots & \vdots &\ddots &\vdots & &\vdots &\vdots \\
  x_i & n_{i1} & n_{i2} & \ldots &n_{ij} &\ldots & n_{iJ}& n_{i\bullet}\\
\vdots& \vdots & \vdots & &\vdots &\ddots &\vdots &\vdots \\
  x_I & n_{I1} & n_{I2} & \ldots &n_{Ij} &\ldots &n_{IJ} &n_{I\bullet}\\
\hline
   & n_{\bullet 1}& n_{\bullet 2}& \ldots& n_{\bullet j}& \ldots& n_{\bullet J}& n\\ 
\end{array}
\]


 Le **frequenze assolute marginali** sono
\[n_{i \bullet}=n_{i1}+\ldots+n_{iJ}\] 
\[n_{\bullet j}=n_{1j}+\ldots+n_{Ij}\]


Definiamo le **frequenze relative congiunte**
\[\hat\pi_{ij}=\frac{n_{ij}}n\]



\[
\begin{array}{c|cccccc|c}
 &y_1 &y_2 &\ldots &y_j &\ldots &y_J &\\
  \hline
  x_1 & \hat\pi_{11}=\frac{n_{11}}{n} & \hat\pi_{12}=\frac{n_{12}}{n} & \ldots &\hat\pi_{1j}=\frac{n_{1j}}{n} &\ldots &\hat\pi_{1J}=\frac{n_{1J}}{n} & \hat\pi_{1\bullet}=\frac{n_{1\bullet}}{n}\\
  x_2 & \hat\pi_{21}=\frac{n_{21}}{n} & \hat\pi_{22}=\frac{n_{22}}{n} & \ldots &\hat\pi_{2j}=\frac{n_{2j}}{n} &\ldots &\hat\pi_{2J}=\frac{n_{2J}}{n} & \hat\pi_{2\bullet}=\frac{n_{2\bullet}}{n}\\
\vdots& \vdots & \vdots &\ddots &\vdots & &\vdots &\vdots \\
  x_i & \hat\pi_{i1}=\frac{n_{i1}}{n} & \hat\pi_{i2}=\frac{n_{i2}}{n} & \ldots &\hat\pi_{ij}=\frac{n_{ij}}{n} &\ldots & \hat\pi_{iJ}=\frac{n_{iJ}}{n}& \hat\pi_{i\bullet}=\frac{n_{i\bullet}}{n}\\
\vdots& \vdots & \vdots & &\vdots &\ddots &\vdots &\vdots \\
  x_I & \hat\pi_{I1}=\frac{n_{I1}}{n} & \hat\pi_{I2}=\frac{n_{I2}}{n} & \ldots &\hat\pi_{Ij}=\frac{n_{Ij}}{n} &\ldots &\hat\pi_{IJ}=\frac{n_{IJ}}{n} &\hat\pi_{I\bullet}=\frac{n_{I\bullet}}{n}\\
\hline
   & \hat\pi_{\bullet 1}=\frac{n_{\bullet 1}}{n}& \hat\pi_{\bullet 2}=\frac{n_{\bullet 2}}{n}& \ldots& \hat\pi_{\bullet j}=\frac{n_{\bullet j}}{n}& \ldots& \hat\pi_{\bullet J}=\frac{n_{\bullet J}}{n}& 1\\ 
\end{array}
\]

 Le **frequenze relative marginali** sono
\[\hat\pi_{i\bullet}=\hat\pi_{i1}+\ldots+\hat\pi_{iJ}\]
\[\hat\pi_{\bullet j}=\hat\pi_{1j}+\ldots+\hat\pi_{Ij}\]


### Le frequenze sono stime dei $\pi$

\[\hat\pi_{ij}=\frac{n_{ij}}n, \qquad \hat\pi_{i\bullet}=\frac{n_{i\bullet}}n,\qquad \hat\pi_{\bullet j}=\frac{n_{\bullet j}}n\]

 $\hat\pi_{ij}$ è stima di $\pi_{ij}$, $\forall i,j$ 

 $\hat\pi_{i\bullet}$ è stima di $\pi_{i\bullet}$, $\forall i$

 $\hat\pi_{\bullet j}$ è stima di $\pi_{\bullet j}$, $\forall j$

 Se in popolazione $x$ ed $y$ sono indipendenti, allora
\[\pi_{ij}=\pi_{i\bullet}\pi_{\bullet j}\]

 E quindi, mi aspetterei
\[\hat\pi_{ij}\approx\hat\pi_{i\bullet}\hat\pi_{\bullet j}\]

Ovvero
\begin{eqnarray*}
n\hat\pi_{ij}&\approx&n~\hat\pi_{i\bullet}\hat\pi_{\bullet j}\\
n_{ij}  &\approx& n \frac{n_{i\bullet}}{n}\frac{n_{\bullet j}}{n}\\
        &\approx& \frac{n_{i\bullet}n_{\bullet j}}{n}\\
        &\approx& n_{ij}^*
\end{eqnarray*}
$n^*_{ij}$ è la frequenza assolute attese in caso di indipendenza. 
La domanda è: quanto si discostano le frequenze assolute osservate $n_{ij}$ dalle 
frequenze attese $n_{ij}^*$?




### Esempio (continua)

Ripartiamo dall'esempio di partenza, immaginando che nelle tre città quei 100 
individui siano un campione
\[\tiny
\begin{array}{c|rr|r}
 & \text{Città A} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
   M & 60 & 0 & \bf 60\\ 
   F & 0 & 40  &\bf 40\\ \hline
  \bf\text{Tot} &\bf 60 &\bf 40 &\bf 100\\ 
\end{array}
\qquad
\begin{array}{c|rr|r}
 & \text{Città B} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
  M & 40 & 20 & \bf60\\ 
  F & 20 & 20  &\bf50\\ \hline
  \bf\text{Tot} &\bf 60 &\bf 40 &\bf 100\\ 
\end{array}
\qquad
\begin{array}{c|rr|r}
 & \text{Città C} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
  M & 36 & 24 & \bf60\\ 
  F & 24 & 16  &\bf40\\ \hline
  \bf\text{Tot} &\bf 60 &\bf 40 &\bf 100\\ 
\end{array}
\]

Costruiamo le $n_{ij}^*$ 
\[\begin{array}{c|rr|r}
 & \text{Freq. th} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
  M & \frac{60 \cdot 60}{100}=36 & \frac{60 \cdot 40}{100}=24 & \bf60\\ 
  F & \frac{40 \cdot 60}{100}=24 & \frac{40 \cdot 40}{100}=16  &\bf40\\ \hline
  \bf\text{Tot} &\bf 60 &\bf 40 &\bf 100\\ 
\end{array}\]

 Nella città $C$ le frequenza osservate coincidono con quelle attese, **non** c'è
associazione tra genere e abbonamento allo stadio. 
 Quanto si discostano le città $A$ e $B$ dalla situazione di indipendenza?

## L'indice $\chi^2$

```{r 19-chi-quadro-3}
th <- outer(c(.6,.4),c(.6,.4))*100
t1 <- matrix(c(60,0,0,40),2)
t2 <- matrix(c(40,20,20,20),2)

chi1 <- (t1-th)^2/th
chi2 <- (t2-th)^2/th
```
 Il $\chi^2_\text{obs}$, che abbiamo già incontrato, è utilizzato qui per nominare un misura di distanza
\[\chi^2_\text{obs}=\sum_{i=1}^I\sum_{j=1}^J \frac{\left(n_{ij}-n_{ij}^*\right)^2}{n_{ij}^*}\]

\[\tiny
\begin{array}{c|rr|r}
 & \text{Città A} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
   M & \frac{(60 - 36)^2}{36} = `r (60-24)^2/36` & \frac{(0  - 24)^2}{24} = `r (0-24)^2/24` & `r sum(chi1[1,])`\\ 
   F & \frac{(0  - 24)^2}{24} = `r (0 -24)^2/24` & \frac{(40 - 16)^2}{16} = `r (40-16)^2/16`& `r sum(chi1[2,])`\\ \hline
    \bf\text{Tot} & `r sum(chi1[,1])`& `r sum(chi1[,2])` &\chi^2_{\text{obs},A}=`r sum(chi1)`\\ 
\end{array}
\qquad
\begin{array}{c|rr|r}
 & \text{Città B} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
  M & \frac{(40 - 36)^2}{36} = `r (40-36)^2/36` & \frac{(20 - 24)^2}{24} = `r (20-24)^2/24` &`r sum(chi2[1,])`\\ 
  F & \frac{(20 - 24)^2}{24} = `r (20-24)^2/24` & \frac{(20 - 16)^2}{16} = `r (20-16)^2/16` &`r sum(chi2[2,])` \\ \hline
    \bf\text{Tot} &`r sum(chi2[,1])` &`r sum(chi2[,2])` &\chi^2_{\text{obs},B}=`r sum(chi2)`\\ 
\end{array}
\]

 Ovviamente per la città $C$:
\[
\begin{array}{c|rr|r}
 & \text{Città C} & &\\ \hline
 &\text{Abbonato} &\text{Non Abbonato} &\bf\text{Tot}\\
  \hline
  M & \frac{(36 - 36)^2}{36} = `r (36-36)^2/36` & \frac{(24 - 24)^2}{24} = `r (24-24)^2/24` & 0\\ 
  F & \frac{(24 - 24)^2}{24} = `r (24-24)^2/24` & \frac{(16 - 16)^2}{16} = `r (16-16)^2/16`  & 0\\ \hline
    \bf\text{Tot} &0 &0 &\chi^2_{\text{obs},C}=0\\ 
\end{array}
\]

Nella città $A$ c'è una evidente associazione tra genere e abbonamento. 
 Nella città $C$ c'è evidente indipendenza tra genere e abbonamento.
 Nella città $B$ lo scostamento dalla situazione di indipendenza, è dovuto al caso
oppure effettivamente nella città $B$ c'è associazione?

## Test per l'ipotesi di indipendenza

 Chiedersi se $x$ e $y$ sono indipendenti significa fare un test in cui è facile scrivere
$H_0$ ma è **superfluo scrivere** $H_1$. 
 Se $H_0$ prescrive l'indipendenza, allora
\[H_0:\pi_{ij}=\pi_{i\bullet}\pi_{\bullet j}, \forall i,j\]

 $H_1$ non la scriviamo perché non può altro che essere la negazione di $H_0$. 
 Questi sono _Test di significatività pura_.

### La statistica test $\chi^2$

Se $H_0$ prescrive l'indipendenza, allora
\[H_0:\pi_{ij}=\pi_{i\bullet}\pi_{\bullet j}, \forall i,j\]

Si dimostra che, sotto $H_0$, prima di osservare i dati, l'indicatore $\chi^2$
vista come VC è:
\[\chi_{{VC}}^2\sim\chi^2_{gdl}\]

 I gradi di libertà sono
\[gdl=(I-1)\times(J-1)\]

 Si fissa $\alpha$ e si cerca
\[\chi_{gdl;\alpha}^2:P(\chi_{gdl}^2>\chi_{gdl;\alpha}^2)=\alpha\]


 Esempio $\alpha=0.05$, nell'esempio genere/abbonamento i $gdl$ sono
\[gdl=(2-1)\times(2-1)=1\]

 \[\chi_{1;0.05}^2= 3.8415\]


 \[\begin{aligned}
\chi^2_{\text{obs};A}&=`r sum(chi1)`> 3.8415; &\chi^2_{\text{obs};B}&=`r sum(chi2)`< 3.8415; &\chi^2_{\text{obs};C}&=0< 3.8415\\
&\text{Rifiuto }H_0 & & \text{non Rifiuto} H_0 & & \text{non Rifiuto}  H_0
\end{aligned}\]



### Esempio

```{r 19-chi-quadro-4}
px <- c(.6,.4)
py <- c(.3,.5,.2)

n <- 150

rw <- c(90,60)
cl <- c(45,75,30)
n12 <- c(20,50)
n123 <- c(n12,90-sum(n12))
m123 <- cl - n123

nxy <- rbind(n123,m123)

# nxy <- cbind(nxy,rowSums(nxy))
# nxy <- rbind(nxy,colSums(nxy))
# chisq.test(nxy)
pxy <- nxy/n
nth <- n*(outer(px,py))
nds <- (nxy-nth)^2/nth
```

 In una città sono stati estratti `r nxy[1,1]` individui occupati con la licenza media , `r nxy[1,2]` individui occupati con la licenza superiore, `r nxy[1,3]` individui occupati laureati, `r nxy[1,3]` individui disoccupati con la licenza media , `r nxy[2,2]` individui disoccupati con la licenza superiore e `r nxy[2,3]` laureati
\[
\begin{array}{c|ccc|r}
         &M &S &L  & \\   \hline
Occupato    & `r nxy[1,1]`     & `r nxy[1,2]`     & `r nxy[1,3]`     & `r sum(nxy[1,])`\\
Disoccupato & `r nxy[2,1]`     & `r nxy[2,2]`     & `r nxy[2,3]`     & `r sum(nxy[2,])`\\ \hline
         & `r sum(nxy[,1])` & `r sum(nxy[,2])` & `r sum(nxy[,3])` & `r n`
\end{array}
\]

 Vogliamo testare 
\[H_0:\pi_{ij}=\pi_{i\bullet}\pi_{\bullet j}\]

 Per vari livelli di $\alpha$
 Costruiamo i valori attesi sotto $H_0$
\[n_{ij}^*=\frac{n_{i\bullet}n_{\bullet j}}{n}\]
e quindi
\[
\begin{array}{c|ccc|r}
\bf\text{Osservati}         &M &S &L  & \\   \hline
Occupato    & `r nxy[1,1]`     & `r nxy[1,2]`     & `r nxy[1,3]`     & `r sum(nxy[1,])`\\
Disoccupato & `r nxy[2,1]`     & `r nxy[2,2]`     & `r nxy[2,3]`     & `r sum(nxy[2,])`\\ \hline
         & `r sum(nxy[,1])` & `r sum(nxy[,2])` & `r sum(nxy[,3])` & `r n`
\end{array}
\qquad
\begin{array}{c|ccc|r}
\bf\text{Teorici}         &M &S &L  & \\   \hline
Occupato    & `r nth[1,1]`     & `r nth[1,2]`     & `r nth[1,3]`     & `r sum(nth[1,])`\\
Disoccupato & `r nth[2,1]`     & `r nth[2,2]`     & `r nth[2,3]`     & `r sum(nth[2,])`\\ \hline
         & `r sum(nth[,1])` & `r sum(nth[,2])` & `r sum(nth[,3])` & `r n`
\end{array}\]

Costruiamo le distanze

\[\frac{\left(n_{ij}-n_{ij}^*\right)^2}{n_{ij}^*}\]

e quindi

\[\begin{array}{c|ccc|r}
         &M &S &L  & \\   \hline
Occupato    & `r nds[1,1]`     & `r nds[1,2]`     & `r nds[1,3]`     & `r sum(nds[1,])`\\
Disoccupato & `r nds[2,1]`     & `r nds[2,2]`     & `r nds[2,3]`     & `r sum(nds[2,])`\\ \hline
         & `r sum(nds[,1])` & `r sum(nds[,2])` & `r sum(nds[,3])` & `r sum(nds)`
\end{array}\]

 $gdl=(2-1)\times(3-1)=2$

 Dalle tavole
\[\chi_{2;0.05}^2=`r qchisq(1-0.05,2)`,\qquad \chi_{2;0.01}^2=`r qchisq(1-0.01,2)`\]

 E osserviamo che $\chi_{\text{obs}}^2=`r sum(nds)`>\chi_{2;0.05}^2=`r qchisq(1-0.05,2)`$ e quindi rifiuto $H_0$ al 5% e che $\chi_{\text{obs}}^2=`r sum(nds)`<\chi_{2;0.01}^2=`r qchisq(1-0.01,2)`$ e quindi non rifiuto $H_0$ al 1%.


### I gradi di libertà

:::: {.example name="Tabella $2\times 2$"}

 Supponiamo di voler riempire la tabella di contingenza coi totali fissati
\[
\begin{array}{c|cc|r}
         &Abbonato &Non~Abbonato   & \\   \hline
M    & n_{11}     & n_{12}    & 60\\
F & n_{21}     & n_{22}     & 40\\ \hline
         & 60 & 40 & 100
\end{array}
\]
abbiamo $gdl=(2-1)\times(2-1)=1$. Ho molte scelte per il valore $n_{11}$, per esempio
\[n_{11}=35\]
ho usato il mio unico grado di libertà, tutti gli altri $n_{ij}$ sono vincolati
\[
\begin{array}{c|cc|r}
         &Abbonato &Non~Abbonato   & \\   \hline
M    & 35     & 25    & 60\\
F & 25     & 15     & 40\\ \hline
         & 60 & 40 & 100
\end{array}
\]
::::

:::: {.example name="Tabella $2\times 3$"}


 Se la tabella è
\[
\begin{array}{c|ccc|r}
         &M &S &L  & \\   \hline
Occupato    & n_{11}     & n_{12}     & n_{13}     & `r sum(nxy[1,])`\\
Disoccupato & n_{21}     & n_{22}     & n_{23}     & `r sum(nxy[2,])`\\ \hline
         & `r sum(nxy[,1])` & `r sum(nxy[,2])` & `r sum(nxy[,3])` & `r n`
\end{array}
\]
abbiamo $gdl=(2-1)\times(3-1)=2$

 Ho molte scelte per il valore $n_{11}$, per esempio
\[n_{11}=30\]
ho usato il primo grado di libertà, $n_{21}=15$ è obbligato

 \[
\begin{array}{c|ccc|r}
         &M &S &L  & \\   \hline
Occupato    & 30     & n_{12}     & n_{13}     & `r sum(nxy[1,])`\\
Disoccupato & 15     & n_{22}     & n_{23}     & `r sum(nxy[2,])`\\ \hline
         & `r sum(nxy[,1])` & `r sum(nxy[,2])` & `r sum(nxy[,3])` & `r n`
\end{array}
\]

 Fisso arbitrariamente $n_{12}$, per esempio
\[n_{12}=40\]
e ho usato il secondo grado di libertà, tutti gli altri valori sono vincolati
\[
\begin{array}{c|ccc|r}
         &M &S &L  & \\   \hline
Occupato    & 30     & 40     & 20     & `r sum(nxy[1,])`\\
Disoccupato & 15     & 35     & 10     & `r sum(nxy[2,])`\\ \hline
         & `r sum(nxy[,1])` & `r sum(nxy[,2])` & `r sum(nxy[,3])` & `r n`
\end{array}
\]
::::

## Misure di Conformità

Spesso, abbiamo assunto che $X\sim\mathscr{L}(\theta)$, senza questionare la scelta di
$\mathscr{L}$. Ad esempio quando assumiamo $X\sim\text{Pois}(\lambda)$, sappiamo fare inferenza su 
  $\lambda$, ma tutti i risultati sono validi se l'assunzione di partenza è valida.
  Oppure quando assumiamo $X\sim N(\mu,\sigma^2)$, sappiamo fare inferenza su 
  $\mu,\sigma^2$, ma tutti i risultati sono validi se l'assunzione di partenza è valida.

IL $\chi^2$ misura la _conformità_ dei dati al modello scelto. 
Supponiamo, le $X_i$, $i=1,...,n$ VC categoriale con supporto 
\[S_X=\{x_1,...,x_K\}\]

La $\mathscr{L}(\pi_1,...,\pi_K)$ tale che
\[P(X_i=x_j)=\pi_j,~~~i=1,...,n~~~~j=1,...,K\]

Se estraggo un campione da IID da $\mathscr{L}(\pi_1,...,\pi_K)$, ovvero 
estraggo $n$ volte $X$ IID ottengo:

modalità   | freq. assoluta | freq. relativa
-----------|----------------|---------------
$x_1$      | $n_2$          | $\hat\pi_1=n_1/n$
$x_2$      | $n_2$          | $\hat\pi_2=n_2/n$
$\vdots$   | $\vdots$       | $\vdots$  
$x_K$      | $n_K$          | $\hat\pi_K=n_K/n$

Le $\hat\pi_j$ sono stime delle $\pi_j$. 
Se $\mathscr{L}(\pi_1,...,\pi_K)$ è vera mi aspetto
\[\hat\pi_j\approx\pi_j\]

Ovvero
\begin{eqnarray*}
  n\cdot\hat\pi_j &\approx& n\cdot\pi_j\\
 n_j &\approx& n_j^*
\end{eqnarray*}

Dove le $n_j^*$ sono le frequenze attese 

:::: {.example}

Lanciamo un dado 100 volte e otteniamo 

```{r 19-chi-quadro-5}
dd <- c(16,15,18,20,14,17)
f <- dd/100
prn <- data.frame(as.character(c(dd,100)),c(f,1))
rownames(prn) <-c(1:6,"tot.")
names(prn) <- c("$n_j$","$\\hat\\pi_j$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4)
```

Se $X$ è un dado perfetto, allora
\[P(X_i=j)=\frac 1 6,~j=1,...,6\]

Da un dado perfetto mi aspetterei, in media
\[\begin{aligned}
\hat\pi_j&\approx \frac 1 6 & n\cdot\hat\pi_j &\approx n\cdot\frac 1 6\\
&= 0.1667 & n_j &\approx 100\frac 1 6\\
     &&&\approx 16.67
\end{aligned}
\]

E quindi

```{r 19-chi-quadro-6}
dd <- c(16,15,18,20,14,17)
f <- dd/100
fs <- (1:6*0+1)*1/6
ns <- round(100*fs,3)
prn <- data.frame(as.character(c(dd,100)),c(f,1),c(fs,1),as.character(c(ns,100)))
rownames(prn) <-c(1:6,"tot.")
names(prn) <- c("$n_j$","$\\hat\\pi_j$","$\\hat\\pi_j^*$","$n_j^*$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4)
```
::::

:::: {.example}

Faccio lanciare il dado a un'altra persona e osservo su 100 lanci 

```{r 19-chi-quadro-7}
dd <- c(1,2,2,5,40,50)
f <- dd/100
fs <- (1:6*0+1)*1/6
ns <- round(100*fs,3)
prn <- data.frame(as.character(c(dd,100)),c(f,1),c(fs,1),as.character(c(ns,100)))
rownames(prn) <-c(1:6,"tot.")
names(prn) <- c("$n_j$","$\\hat\\pi_j$","$\\hat\\pi_j^*$","$n_j^*$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4)
```

Che è molto diversa da quella di prima

```{r 19-chi-quadro-8}
dd <- c(16,15,18,20,14,17)
f <- dd/100
fs <- (1:6*0+1)*1/6
ns <- round(100*fs,3)
prn <- data.frame(as.character(c(dd,100)),c(f,1),c(fs,1),as.character(c(ns,100)))
rownames(prn) <-c(1:6,"tot.")
names(prn) <- c("$n_j$","$\\hat\\pi_j$","$\\hat\\pi_j^*$","$n_j^*$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4)
```
::::

## Il $\chi^2$ come misura di conformità

Misuriamo la distanza tra le frequenze attese con quelle osservate con la misura del chi-quadro
\[\chi^2_\text{obs}=\sum_{j=1}^K\frac{\big(n_{ij}-n_{ij}^*\big)^2}{n_{ij}^*}\]


Nel primo caso

```{r 19-chi-quadro-9}
dd <- c(16,15,18,20,14,17)
n <- sum(dd)
f <- dd/n

fs <- (1:6*0+1)*1/6
ns <- (n*fs)
nc <- round((dd-ns)^2/ns,3)
ns <- round(n*fs,3)
dd <- as.factor(c(dd,n))
ns <- as.factor(c(ns,n))
nc <- c(nc,sum(nc))

prn <- data.frame(dd,ns,nc)
rownames(prn) <-c(1:6,"tot.")
names(prn) <- c("$n_j$","$n_j^*$","$\\frac {(n_j-n_j^*)^2}{n_j^*}$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4)
```

\[\chi_\text{obs}^2=`r nc[7]`\]

Nel secondo caso

```{r 19-chi-quadro-10}
dd <- c(1,2,2,5,40,50)
n <- sum(dd)
f <- dd/n

fs <- (1:6*0+1)*1/6
ns <- (n*fs)
nc <- round((dd-ns)^2/ns,3)
ns <- round(n*fs,3)
dd <- as.factor(c(dd,n))
ns <- as.factor(c(ns,n))
nc <- c(nc,sum(nc))

prn <- data.frame(dd,ns,nc)
rownames(prn) <-c(1:6,"tot.")
names(prn) <- c("$n_j$","$n_j^*$","$\\frac {(n_j-n_j^*)^2}{n_j^*}$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4) 
```

\[\chi_\text{obs}^2=`r nc[7]`\]


Ci chiediamo se il modello che stiamo usando sia supportato dai dati
\[H_0:X\sim\mathscr{L}\]

Se $X$ è discreta
\[H_0:\pi_j=\pi_{j0}, \forall j\]

Sotto $H_0$, prima di osservare i dati, l'indice di conformità $\chi^2_{VC}$,
visto come VC è distribuito come
\[\chi_{VC}^2\sim\chi^2_{K-1}\]


Osservo i dati e calcolo
\[\chi^2_\text{obs}=\sum_{j=1}^K\frac{\big(n_{ij}-n_{ij}^*\big)^2}{n_{ij}^*}\]

Scelgo $\alpha$, trovo $\chi_{K-1;\alpha}^2$ sulle tavole del chi-quadro e lo confronto
con $\chi^2_\text{obs}$

  - se $\chi^2_\text{obs}>\chi_{K-1;\alpha}^2$ **rifiuto** $H_0$ al lds $\alpha$
  - se $\chi^2_\text{obs}<\chi_{K-1;\alpha}^2$ **non rifiuto** $H_0$ al lds $\alpha$



### Esempio: Scostamento da una uniforme

Lanciamo un dado 60 volte

```{r 19-chi-quadro-11}
dd <- c(5,14,6,17,13,5)
n <- sum(dd)
f <- dd/n

fs <- (1:6*0+1)*1/6
ns <- (n*fs)
nc <- round((dd-ns)^2/ns,3)
ns <- round(n*fs,3)
dd <- as.factor(c(dd,n))
ns <- as.factor(c(ns,n))
nc <- c(nc,sum(nc))
#qchisq(1-.01,5)
prn <- data.frame(dd,ns,nc)
rownames(prn) <-c(1:6,"tot.")
names(prn) <- c("$n_j$","$n_j^*$","$\\frac {(n_j-n_j^*)^2}{n_j^*}$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4)
```

\[H_0:\pi_j=\frac 1 6, j=1,...,6\]



 \[\chi_\text{obs}^2=`r nc[7]`\]

Se $\alpha=0.05$, $\chi_{6-1;0.05}^2=`r qchisq(1-.05,5)`$,
allora $\chi^2_\text{obs}=`r nc[7]`>`r qchisq(1-.05,5)`=\chi_{6-1;0.05}^2$ **rifiuto** $H_0$ al 5%
Se $\alpha=0.01$, $\chi_{6-1;0.01}^2=`r qchisq(1-.01,5)`$,
allora $\chi^2_\text{obs}=`r nc[7]`<`r qchisq(1-.01,5)`=\chi_{6-1;0.01}^2$ **non rifiuto** $H_0$ al 1%

### Esempio: Scostamento da una popolazione

```{r 19-chi-quadro-12}
x <- c(15,9,24,2)
n <- sum(x)

fc <-x/sum(x)
fh <- c(.3,.3,.3,.1)
```
L'hotel $A$ ha a servizio `r n` addetti: `r x[1]` addetti alle pulizie, `r x[2]` receptionist, `r x[3]` personale di sala e `r x[4]` cuochi. L'hotel fa parte di
una catena alberghiera. Al controllo di gestione del personale il gruppo registra che nei loro hotel il 30% sono addetti alle pulizie,
il 30% sono receptionist, il 30% personale di sala e il 10% cuochi.
L'hotel $A$ ha una gestione del personale conforme a quella del gruppo, al lds del 5%?
\[H_0:\pi_{jA}=\pi_{jG}\]

Costruiamo le tabelle
```{r 19-chi-quadro-13}

f <- fc
dd <- x

fs <- fh
ns <- (n*fs)
nc <- round((dd-ns)^2/ns,3)
ns <- round(n*fs,3)
dd <- as.factor(c(dd,n))
ns <- as.factor(c(ns,n))
nc <- c(nc,sum(nc))
#qchisq(1-.01,5)
prn <- data.frame(dd,c(fs,1),ns,nc)
rownames(prn) <-c(c("A","R","S","C"),"tot.")
names(prn) <- c("$n_j$","$\\pi_j^*$","$n_j^*=n\\pi_j^*$","$\\frac {(n_j-n_j^*)^2}{n_j^*}$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4)
```

Se $\alpha=0.05$, $\chi_{4-1;0.05}^2=`r qchisq(1-.05,3)`$, allora $\chi^2_\text{obs}=`r nc[5]`>`r qchisq(1-.05,3)`=\chi_{4-1;0.05}^2$ **rifiuto** $H_0$ al 5%

### Esempio: scostamento da una Poisson

Si sono registrati i clienti in coda presso la cassa di un negozio in 200 occasioni
scelte a caso in vari momenti del giorno per un mese e si sono ottenuti i risultati riportati
in tabella.

```{r 19-chi-quadro-14}
x <-c(70,85,30,15)
cod <- c(0,1,2,">2")
prn=data.frame(cod,x)
names(prn) <- c("Clienti in coda", "volte")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4)

```
 
Si presume che i clienti in coda presso la cassa del negozio sia una Poisson(1). Verificare, al livello di significatività del 5%, se i dati osservati sono coerenti con l'ipotesi.

\[H_0:X\sim\text{Pois}(1)\]

Sotto $H_0$
\[\pi_j^*=P(X_i=x_j)=\frac \lambda{x_j!}e^{-\lambda},\]

\begin{eqnarray*}
   \pi_1^*=P(X_i=0)&=&\frac{1}{0!}e^{-1}=`r dpois(0,1)`\\
   \pi_2^*=P(X_i=1)&=&\frac{1}{1!}e^{-1}=`r dpois(1,1)`\\
   \pi_3^*=P(X_i=2)&=&\frac{1}{2!}e^{-1}=`r dpois(2,1)`\\
   \pi_4^*=P(X_i>2)&=&1-(\pi_1^*+\pi_2^*+\pi_3^*)=`r 1-sum(dpois(0:2,1))`\\
\end{eqnarray*}


Le frequenze attese

\[n_j^*=n\pi_j^*\]

 
```{r 19-chi-quadro-15}
x <-c(70,85,30,15)
n <- sum(x)
cod <- c(0,1,2,">2","Tot")
ps <- dpois(0:2,1)
ps <- c(ps,1-sum(ps))
ns <- ps*n
nc <- (x-ns)^2/ns
prn <- cbind(x,ps,ns,nc)
prn <- rbind(prn,colSums(prn))
prn <- data.frame(cod,prn)


names(prn) <- c("Clienti in coda","volte","$\\pi_j^*$","$n_j^*=n\\pi_j^*$","$\\frac {(n_j-n_j^*)^2}{n_j^*}$")
kable(t(prn),booktabs = T, escape = F,linesep = "", digits = 4)

```

$\alpha=0.05$, quindi $\chi_{4-1;0.05}^2=`r qchisq(1-.05,3)`$

allora $\chi^2_\text{obs}=`r sum(nc)`<`r qchisq(1-.05,3)`=\chi_{4-1;0.05}^2$ 
**non rifiuto** $H_0$ al 5%.

<!--chapter:end:19-chi-quadro.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r intro23, include=FALSE}
rm(list = ls())


source("intro.R")
options(digits=4)
opts_knit$set(global.par = TRUE)
```

\part{Appendice}

```{r 23-APPENDICE-1}
if (!html) cat("# (APPENDIX) Appendice {-} ")
```

# Richiami sugli Operatori Sommatoria e Produttorio

## Operatore Sommatoria

È una forma simbolica per rappresentare somme di un numero qualunque di addendi.
Si consideri un insieme di numeri indicizzati con $i$
\[
\{a_1,...,a_n\}
\]
Si definisce la _Sommatoria per $i$ che va da 1 fino ad $n$_
\[
\sum_{i=1}^n a_i = a_1+a_2+...+a_n
\]
$i$ ed $n$ sono chiamati _quantificatori_.

_Esempio_

Si consideri l'insieme
\[
S=\{a_1=30,a_2=15,a_3=21\}
\]
allora la _Sommatoria per $i$ che va da 1 fino ad 3_
\[
\sum_{i=1}^3 a_i = 30+15+21=`r 30+15+21`
\]

Si consideri l'insieme
\[
S=\{x_1 = 3.1,x_2 = 4.1, x_3 = 1.4,x_4=3.3,x_5=2.9\}
\]
allora la _Sommatoria per $a$ che varia in $S$_
\[
\sum_{i=3}^5  x_3 + x_4+x_5=x_3 = 1.4+3.3+2.9=`r 1.4+3.3+2.9`
\]

Un modo alternativo per indicare i quantificatori è il seguente. Sia $S$ un insieme di numeri
\[
S=\{a_1,...,a_n\}
\]
Si definisce la _Sommatoria di tutti gli $a$ in $S$_
\[
\sum_{a\in S} a = a_1+a_2+...+a_n
\]

Si consideri l'insieme
\[
S=\{y_1=3.0,y_2=1.5,y_3=2.1\}
\]
allora la _Sommatoria per $a$ che varia in $S$_
\[
\sum_{a\in S} a = 3.0+1.5+2.1=`r 3.0+1.5+2.1`
\]


**Proprietà della Sommatoria**

1. Se $k$ è una costatante, allora
\[
\sum_{1=1}^n k x_i = k\sum_{1=1}^n  x_i
\]

```{r 23-APPENDICE-2}
k <- 3.6
xx<-c(x_1 = 3.1,x_2 = 4.1, x_3 = 1.4,x_4=3.3,x_5=2.9)
xk <- paste(k,xx,sep = "\\times ",collapse = "+")
```

Infatti

\begin{eqnarray*}
\sum_{1=1}^n k x_i   &=&  k\cdot x_1 + k\cdot x_n\\
 &=& k(x_1+...+x_n)\\
 &=& k\sum_{i=1}^n x_i
\end{eqnarray*}

Si consideri l'insieme
\[
S=\{x_1 = 3.1,x_2 = 4.1, x_3 = 1.4,x_4=3.3,x_5=2.9\},
\]
Posto $k=3.6$

\begin{eqnarray*}
\sum_{1=1}^n k x_i   &=&  `r xk`\\
 &=& `r k`\times(`r paste(xx,collapse="+")`)\\
 &=& `r sum(xx)*k`
\end{eqnarray*}
  
2. Se consideriamo due insiemi di numeri $\{a_1,...,a_n\}$ e $\{b_1,...,b_n\}$, allora

\[
\sum_{1=1}^n (a_i + b_i) = \sum_{1=1}^n  a_i + \sum_{1=1}^n  b_i
\]


```{r 23-APPENDICE-3}
k <- 3.6
xx <-c(x_1 = 3.1,x_2 = 4.1, x_3 = 1.4)
yy <-c(y_1 = 1.9,y_2 = 6.3, y_3 = 5.1)
xy <- paste("(",xx,"+",yy,")",sep = " ",collapse = "+")
xs <- paste(xx,collapse =  "+")
ys <- paste(yy,collapse =  "+")
xxyy <- paste("(",xs,")","+","(",ys,")")
```

Posto $S_X=\{x_1 = 3.1,x_2 = 4.1, x_3 = 1.4\}$,
      $S_Y=\{y_1 = 1.9,y_2 = 6.3, y_3 = 5.1\}$

\begin{eqnarray*}
\sum_{1=1}^n k x_i   &=&  `r xk`\\
 &=& `r xxyy`\\
 &=& `r sum(xx+yy)`
\end{eqnarray*}

3. Se $k$ è una costante, allora

\[
\sum_{1=1}^nk=k+k+...+k=n\cdot k
\]

Posto $k=3.6$ e $n=4$, allora
\begin{eqnarray*}
\sum_{1=1}^n  k   &=&  \sum_{1=1}^4 3.6\\
 &=& 3.6+3.6+3.6+3.6\\
 &=& 4\times 3.6\\
 &=& `r 4*3.6`
\end{eqnarray*}

4. Se $k$ e $c$ sono due costanti, allora

\[
\sum_{1=1}^n(c+ k a_i) = n\cdot c+k\sum_{1=1}^n  a_i
\]

Posto $S_X=\{x_1 = 3.1,x_2 = 4.1, x_3 = 1.4\}$, $k=3.6$ e $c=0.5$, allora

\begin{eqnarray*}
\sum_{1=1}^n c + k x_i   &=&  \sum_{1=1}^3 0.5 + 3.6\times x_i  \\
 &=& `r paste(0.5,"+",3.6,"\\times",xx,collapse="+")`\\
 &=& 4\times 0.5 + 3.6 \times (`r xs`)\\
 &=& `r sum(.5+3.6*xx)`
\end{eqnarray*}


**Attenzione!**

\[
\sum_{1=1}^n (a_i \cdot b_i) \ne \left(\sum_{1=1}^n  a_i \right)\cdot\left(\sum_{1=1}^n  b_i\right)
\]


```{r 23-APPENDICE-4}
xx <-c(x_1 = 3.1,x_2 = 4.1, x_3 = 1.4)
yy <-c(y_1 = 1.9,y_2 = 6.3, y_3 = 5.1)
xy <- paste("(",xx,"\\times",yy,")",sep = " ",collapse = "+")
xs <- paste(xx,collapse =  "+")
ys <- paste(yy,collapse =  "+")
xxyy <- paste("(",xs,")","+","(",ys,")")
```

Posto $S_X=\{x_1 = 3.1,x_2 = 4.1, x_3 = 1.4\}$,
      $S_Y=\{y_1 = 1.9,y_2 = 6.3, y_3 = 5.1\}$

\begin{eqnarray*}
\sum_{1=1}^n x_i    &=&  `r sum(xx)`\\
\sum_{1=1}^n y_i    &=&  `r sum(yy)`\\
\sum_{1=1}^n x_i y_i   &=&  `r xy`\\
 &=& `r sum(xx*yy)`\\
 &\ne& `r sum(xx)`\times `r sum(yy)`=`r sum(xx)*sum(yy)`
\end{eqnarray*}



## Operatore Produttorio

```{r 23-APPENDICE-5} 
a <- c(1.1,0.9,1.3)
```
Siano $a_1,...,a_n$, $n$ numeri, $a_i\in\mathbb{R}$:
L'operatore sommatoria somma gli elementi 
\[\sum_{i=1}^n a_i=a_1+ a_2+ ...+ a_n\],
allo stesso modo, l'operatore _produttoria_ opera il prodotto dei dati 

::: {.definition name="Produttoria"}
L'operatore produttoria moltiplica gli elementi
\[\prod_{i=1}^n a_i=a_1\cdot a_2\cdot ...\cdot a_n\]
:::


Esempio: $a_1=1.1$, $a_2=0.9$, $a_3=1.3$ 
\begin{eqnarray*}
 \sum_{i=1}^n a_i &=& `r paste(a[1],'+',a[2],'+',a[3])` = `r sum(a)`\\
 \prod_{i=1}^n a_i &=& `r paste(a[1],'\\times',a[2],'\\times',a[3])` = `r prod(a)`
\end{eqnarray*}


# Richiami di Calcolo Combinatorio

Sia $n\in\mathbb{N}$ un numero naturale, si definisce $n$ fattoriale, il numero 
\[n!=n(n-1)(n-2)...3\cdot 2 \cdot 1\] 
conta in quanti modo posso rimescolare $n$ oggetti.

__Esempi__ $3!=3\cdot 2\cdot 1=6$, $10!=10\cdot 9\cdot... \cdot1 =`r factorial(10)`$, $52!=`r factorial(52)`$

__Nota.__ Per definizione $0!=1$

## Il coefficiente binomiale

Il coefficiente binomiale $\binom{n}{k}$ conta in quanti modi posso disporre $k$ oggetti indistinguibili in $n\ge k$ posti:
\[\binom{n}{ k}=\frac {n!}{k!(n-k)!}\]

Proprietà utili

\begin{eqnarray*}
  \binom{n}{ k} &=&\binom{n}{ n-k},\qquad\text{Per esempio:} \\
  \binom{5}{ 3} &=&\binom{5}{ 2}=`r choose(5,2)`,\\
  \binom{n}{ 0} &=&\binom{n}{ n} = 1 \\
  \binom{n}{ 1} &=&\binom{n}{ n-1}=n 
\end{eqnarray*}

In matematica $\binom{n}{ k}$ è il $k$-esimo elemento della $n$-esima riga del _triangolo di Tartaglia_.

```{r 23-APPENDICE-6}
N <- 8
rnam <- 0:(N-1)
cnam <- 0:(N-1)
trg <- outer(rnam,cnam,choose)
trg[trg==0]<- NA
dimnames(trg)[[1]]<-rnam
dimnames(trg)[[2]]<-rnam

kable(trg,row.names = T,align = 'c',
      booktabs = T, escape = F,linesep = "", digits = 4) 

```



# Richiami di Matematica

## Richiami sui logaritmi

Si definisce $\log x$ il logaritmo naturale di $x>0$
  - $\lim_{x\to 0}\log x = -\infty$
  - $\log e = 1$, dove $e$ è il numero di Nepero `r exp(1)`...

Una delle utilità dei logaritmi è di trasformare il logaritmo del prodotto 
in somma dei logaritmi dei fattori.

__Proprietà 1__
\[\log a\cdot b=\log a + \log b\]

ed esprime la potenza come coefficiente moltiplicativo:

__Proprietà 2__
\[\log a^b=b\log a\]

Consideriamo il prodotto di logaritmi:
\begin{eqnarray*}
 \log \prod_{i=1}^n a_i &=& \log a_1\cdot...\cdot a_n\\
                       &=& \log a_1+...+\log a_n\\
                       &=& \sum_{i=1}^n \log a_i
\end{eqnarray*}

Inoltre
\begin{eqnarray*}
 \log\prod_{i=1}^n a_i^{b_i} &=& \log a_1^{b_1}\cdot...\cdot a_n^{b_i}\\
                       &=& b_1\log a_1 + ... + a_n\log a_n\\
                       &=& \sum_{i=1}^n b_i\log a_i
\end{eqnarray*}



## Richiami di Analisi

### Note sulla cardinalità degli insiemi

In matematica la cardinalità di un insieme indica il numero dei suoi elementi.

L'insieme $E=\{a,b,c\}$ ha *cardinalità* finita $$\text{card} (E)=\# E = 3$$ L'insieme dei numeri $S=\{0,1,2,3,...,n\}$ ha cardinalità finita: $$\# S = n+1$$ L'insieme dei numeri naturali $$
\mathbb{N}=\{0,1,2,3,...\}
$$ ha **cardinalità infinita numerabile** $$\# \mathbb{N} = \aleph_0,\qquad \text{infinito numerabile}$$ L'insieme dei reali $$\mathbb{R}=\mathbb{Q}\cup\mathbb{I}, \qquad\text{i numeri reali sono dati dall'unione dei razionali $\mathbb{Q}$ e degli irrazionali $\mathbb{I}$}$$ ha **cardinalità infinita più che numerabile** $$\# \mathbb{R} = \aleph_1,\qquad \text{infinito più che numerabile}$$

### Funzioni Reali e loro derivate

Solitamente in analisi matematica si studia una generica funzione $f$, dove la variabile è $x$.
La maggior parte degli esercizi riguardano 
\[
f(x), x\in\text{Dominio di $f$}
\]
In particolare, se $f(x)=\log x, x>0$ sappiamo che
\[f'(x)=\frac{1}x\]

Nella teoria della verosimiglianza lasceremo la lettera $f$ assegnata alle funzioni di 
probabilità (nel caso discreto) e le funzioni di densità (nel caso continuo) e 
useremo lettere differenti per indicare la funzione. Allo stesso modo le etichette $x$
restano per individuare i dati e le variabili sono i parametri del modello.

Quindi scriveremo, per esempio,

\[
g(\theta)=\log \theta,~~~ \theta\in\Theta
\]

e leggeremo: $g$ è funzione di $\theta$ e se dobbiamo derivare la funzione
lo facciamo rispetto a $\theta$:
\[g'(\theta)=\frac{1}\theta\]

Ricordiamo qualche semplice regola di derivazione:

Se $f(\theta)=g(h(\theta))$
\[f'(\theta)=g'(h(\theta))h'(\theta)\]

Se $f(\theta)=\log h(\theta)$
\[f'(\theta)=\frac{h'(\theta)}{h(\theta)}\]


<!--chapter:end:23-APPENDICE.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r 24-Libro-1, echo=FALSE}
rm(list = ls())


source("intro.R")
opts_knit$set(echo=T,results = "markup")
virg <- "`"
options(digits = 4,nsmall=4) # quanti decimali stampo
cat(par()$cex,html)
fig.def(2.4)
```

# Com'è Realizzato il Libro

Per scrivere queste pagine mi sono avvalso di diverse tecnologie che grazie all'ambiente integrato
R-Studio (cita) è possibile combinare insieme con uno sforzo relativamente basso.
Tra i tanti software coinvolti ricordo

- R <cite>[1]</cite>
- R Markdown <cite>[2], [3], [4]</cite>

R è l'ambiente di calcolo col quale ho svolto tutti i calcoli e realizzato le figure.
R Markdown è un software che consente di mescolare pezzi di codici HTML, pezzi di codici `r ifelse(html,"$\\rm\\LaTeX$","\\LaTeX{}")`
e pezzi di codice R.
In sostanza lo stesso documento contiene:

- il contenuto del testo;
- i codici per produrre i calcoli e i grafici;
- i codici che consentono la formattazione del testo

per esempio il codice
````.markdown
```{r 24-Libro-2, echo=FALSE}`r ''`
# Questo è un blocco di codice in R
# fisso alcuni parametri ed eseguo alcuni conti
n <- 250 # fisso n = 250
data <- rnorm(n,10,1) # genero n dati da una normale di di media 10 e sd 1
mx <- mean(data) # chiamo mx la media dei dati
vx <- var(data) # chiamo vx la varianza dei dati
```
Abbiamo analizzato $n=`r virg`r n`$ individui, abbiamo osservato una media pari 
a $\bar x =`r virg`r mx`$ e una varianza pari a $\sigma^2=`r virg`r vx`$

```{r 24-Libro-3, echo=FALSE}`r ''`
# qui produco un grafico
hist(data)
```
````

produrrà il seguente risultato

```{r 24-Libro-4,echo=FALSE}
# Questo è un blocco di codice in R
# fisso alcuni parametri ed eseguo alcuni conti
n <- 250 # fisso n = 250
data <- rnorm(n,10,1) # genero n dati da una normale di di media 10 e sd 1
mx <- mean(data) # chiamo mx la media dei dati
vx <- var(data) # chiamo vx la varianza dei dati
```

::: {.minip data-latex=""}
Abbiamo analizzato $n=`r n`$ individui, abbiamo osservato una media pari a 
$\bar x =`r mx`$ e una varianza pari a $\sigma^2=`r vx`$.
```{r 24-Libro-54}
hist(data)
```
:::

Quindi cioè che viene prodotto all'interno dei blocchi di R (chiamati R chunks), 
è utilizzabile nel testo richiamando `` `r knitr::inline_expr("nome_comando")` ``
il testo finale restituirà il risultato di ``nome_comando``.

Le espressioni scritte tra ``$`` sono pezzi di codice \LaTeX che è un potentissimo
software di video scrittura, particolarmente adatto per scrivere formule matematiche.
Per esempio il codice

````.markdown
$$
\bar x = \frac 1n \sum_{i=1}^n x_i
$$
````

produce

$$
\bar x = \frac 1n \sum_{i=1}^n x_i
$$

L'obiettivo delle seguenti pagine è documentare la realizzazione del libro e non
quello di approfondire passo, passo i software utilizzati. Farò solo una breve presentazione
per stimolare i lettori più curiosi a cercare documentazioni più approfondite dei
principali ambienti che ho utilizzato.

## R: A Language and Environment for Statistical Computing

L'ambiente open source R è una collezione software per analizzare, manipolare e rappresentare 
dati. È sviluppato per tantissimi sistemi operativi tra cui Windows, Linux, Mac OS e 
tanti altri. Per maggiori informazioni su download e installazione rimando al sito.
[https://www.R-project.org/][https://www.R-project.org/]
nel quale si trovano diversi documenti di introduzione al software non solo inglese:

- https://cran.r-project.org/manuals.html
- https://cran.r-project.org/other-docs.html

R è un interprete interattivo, l'utente scrive un comando e una volta premuto invio
R restituisce il risultato. 

### R come calcolatrice

R è anzitutto una calcolatrice

```{r setup}
source("intro.R")
opts_knit$set(echo=T,results = "markup")
options(digits = 5,nsmall=5) # quanti decimali stampo
```

```{r 24-Libro-5,echo=TRUE,results='markup'}
1+1 # uno più uno 
3-4 # tre meno quattro
4*3 # quattro per tre
4/3 # 4 diviso 3
5^2 # cinque elevato alla due
log(2) # logaritmo naturale di due
exp(1) # e alla uno, il numero di Nepero
pi # pi greco, la costante trigonometrica
1/0 # 1 diviso zero restituisce  infinito
```

### Operatori Speciali

Alcuni caratteri speciali sono

- ``NA`` Not Available è dedicato ai dati mancanti
- ``Inf`` infinito
- ``NULL`` non presente, nullo
- ``TRUE`` o ``T`` vero
- ``FALSE`` o ``F`` falso

```{r 24-Libro-6,echo=TRUE,results='markup'}
NA+1      # sommare uno al dato mancante dà come risultato un dato mancante
Inf+1     # infinito più uno è sempre infinito
NULL+1    # sommare al nulla uno non restituisce nulla
TRUE + 1  # vero + 1 fa due
FALSE + 1 # falso + 1 fa uno
```

- ``a==b`` a è uguale a b?
- ``a!=b`` a è diverso a b?
- ``a>b`` a è maggiore di b?

```{r 24-Libro-7,echo=TRUE,results='markup'}
3 == 4   # 3 è uguale a 4?
3 != 4   # 3 è diverso da 4?
3 >  4   # 3 è maggiore di 4?
```

Gli operatori ``&`` e ``|`` svolgono il ruolo di ``and`` e ``or``.

```{r 24-Libro-8,echo=TRUE,results='markup'}
cond1 <- TRUE
cond2 <- FALSE

cond1 & cond2 # vera se sono vere entrambe
cond1 | cond2 # vera se almeno una delle due è vera
```

### Vettori e matrici

Con il comando ``c`` è possibile concatenare dei valori per creare un vettore, mentre 
si possono assegnare etichette agli oggetti con l'operatore ``<-``

```{r 24-Libro-9,echo=TRUE,results='markup'}
x <- c(2,1,9)
x
y <- c(2.3,1.4,2.8)
y
x+y # somma elemento per elemento
```

Alcune funzioni speciali aiutano a velocizzare la creazione di vettori, per
esempio ``1:10`` produce i numeri che vanno da uno a dieci.

```{r 24-Libro-10,echo=TRUE,results='markup'}
1:10
```

Alternativamente la funzione ``seq`` produce una sequenza di numeri dal minimo al massimo 
secondo alcuni criteri

```{r 24-Libro-11,echo=TRUE,results='markup'}
s1 <- seq(0,1,by=.1)     # sequenza da 0 ad 1 di passo 0.1
s2 <- seq(0,1,length=6)  # sequenza da 0 ad 1 di 6 numeri
s1
s2
```

La lunghezza di un vettore si ricava col comando ``lenght``

```{r 24-Libro-12,echo=TRUE,results='markup'}
length(s2)
```

Per estrarre elementi da un vettore usiamo le parentesi quadre: ``s2[2]`` restituisce il secondo elemento di ``s2`` mentre ``s2[1:4]`` restituisce i primi 4 elementi e ``s2[c(3,2,5)]``
il terzo, il secondo e il quinto

```{r 24-Libro-13,echo=TRUE,results='markup'}
s2[2]
s2[1:4]
s2[c(3,2,5)]
```

```{r 24-Libro-14,echo=TRUE,results='markup'}
x <- c(1.1,-3.4,4.2,5.1,-4.4,-3.9,2.5)
x > 0 # sequenza di TRUE e FALSE
which(x > 0) # in quale posizione sono gli x maggiori di zero?
x[x > 0] # gli x, ma solo quelli maggiori di zero
x[!(x > 0)] # gli x, ma solo quelli che NON sono maggiori di zero
```

È possibile costruire matrici con la funzione ``matrix``

```{r 24-Libro-15,echo=TRUE,results='markup'}
mat1 <- matrix(c(1,2,3,4),nrow = 2) # legge in colonna
mat1
mat2 <- matrix(c(1,2,3,4),nrow = 2,byrow = T) # legge per riga
mat2

mat1 + mat2 # restituisce la somma elemento per elemento
```

Le funzioni ``cbind`` ed ``rdbind`` consentono di unire una nuova colonna o una nuova riga
ad una matrice o vettore.

```{r 24-Libro-16,echo=TRUE,results='markup'}
x1 <- 1:3
x2 <- 2:4
cbind(x1,x2)
rbind(x1,x2)
```

Per indicizzare una matrice useremo sempre le parentesi quadre ma con 2 indici

```{r 24-Libro-17,echo=TRUE,results='markup'}
mat <- matrix(1:20,nrow = 5) 
mat
mat[1,1]
mat[2:3,3:4]
```

I vettori e le matrici non sono solo numeriche ma possono anche contenere 
caratteri, ovviamente le operazioni aritmetiche non sono più consentite

```{r 24-Libro-18,echo=TRUE,results='markup'}
mat <- matrix(c("testo 1","testo 2","testo 3","testo 4"),nrow = 2) 
mat
```

### Liste e dataframe

Una lista è una collezione di diversi oggetti di R

```{r 24-Libro-19,echo=TRUE,results='markup'}
mat1 <- matrix(c(1,2,3,4),nrow = 2) # legge in colonna
mat <- matrix(c("testo 1","testo 2","testo 3","testo 4"),nrow = 2) 
b <- c(NA,NA,NA)

lista <- list(mat1,mat,b)
lista
```

Una particolare tipo di lista è il ``data.frame`` che consente di creare una matrice dei
dati composta da colonne di diversa natura


```{r 24-Libro-20,echo=TRUE,results='markup'}
sesso <- c("M","M","M","F","F")
eta   <- c(32.2,45.6,65.3,34.1,43.2)
dati <- data.frame(sesso,eta)
dati
```

il simbolo del ``$`` aiuta a selezionare le colonne di interesse

```{r 24-Libro-21,echo=TRUE,results='markup'}
dati$sesso
dati$eta
```

### Classi e Oggetti

R è un linguaggio funzionale, ogni elemento è un oggetto che ha un classe
e metodi. Per esempio

```{r 24-Libro-22,echo=TRUE,results='markup'}
x <- 3
class(x)
x <- matrix(1:4,nrow = 2)
class(x)
x <- list(1:3,c("a","b"))
class(x)
```

::: {.att data-latex=""}
Alcuni oggetti possono assomigliarsi ma essere diversi ad esempio
```{r 24-Libro-23,echo=TRUE,results='markup'}
x1 <- 1;3
x2 <- 2:4
mat <- cbind(x1,x2)
dat <- data.frame(x1,x2)
mat
dat
class(mat)
class(dat)
```
:::

### I grafici

La libraria grafica di R è particolarmente ricca. La funzione di base per
realizzare un grafico è la funzione ``plot(x,y)``. La funzione, di default, disegna 
i punti di coordinate ``x`` e ``y``.

```{r 24-Libro-24,echo=TRUE,results='markup'}
x <- c(1,2,3,4)
y <- c(2,1,4,3)
plot(x,y)
```

La funzione ``plot`` è molto flessibile e può essere arricchita con molte opzioni

```{r 24-Libro-25,echo=TRUE,results='markup'}
x <- c(1,2,3,4)
y <- c(1.9,1.2,3.5,3.1)
plot(x,y,
     axes=F,      # non disegnare gli assi lo farò dopo
     pch = 16,    # codice 16, per il pallino chiuso
     col = ared, #colore rosso per i pallini
     xlab="Etichetta per la x",
     ylab="Etichetta per la y",
     type = "b",  # linea e punto
     lty  =2,     # stile di tratteggiatura
     main = "Titolo del grafico"
          )
# Un volta creato il grafico di base possiamo aggiungere 
axis(1) # asse delle x
axis(2,at = y) # asse delle y sui punti osservati
text(2,3,"Nelle coordinate x=1.5 e y=3 scrivo qualcosa")
text(3,1.9,"Scrivo più grande se cex = 1.5",cex=1.5)
```

Con le funzioni ``points`` e ``lines``  è possibile aggiungere al grafico esistente
punti e linee, rispettivamente. Mentre per disegnare funzioni la funzione ``curves``
aiuta molto

```{r 24-Libro-26,echo=TRUE,results='markup'}
curve(sin(x),from = -pi,to = pi,axes=F) # disegna sin(x) tra - pi e + pi 
curve(cos(x),col=ared,add=T)           # aggiunge il grafico di cos(x) in rosso
points((-3):3,cos((-3):3),pch=3)
lines((-3):3,cos((-3):3),col=iblue,lty=4)
axis(1)
axis(2)
```

Un'altra funzione di interesse è ``hist`` che produce istogrammi di densità

```{r 24-Libro-27,echo=TRUE,fig.width=4}
x <- c(2.3,2.5,3.5,3.6,4.6,5.9,6.9,9.8)
hist(x)
hist(x,breaks = c(0,3,4,6,10),probability = T,axes=F,col="white")
axis(1,at = c(0,3,4,6,10))
```

Per rappresentare dati in 3D la funzione ``persp`` disegna superfici in 3 dimensioni

```{r 24-Libro-28,echo=TRUE,fig.height=3.5}
x <- seq(-4,4,by=.1)
y <- seq(-4,4,by=.1)
z <- outer(x,y,function(x,y)exp(-x^2-y^2))
persp(x,y,z,theta = 25,phi = 25,ltheta = 25,border = NA,shade = 2.5)
```

Altre funzioni grafiche interessanti sono ``rect`` e ``polyogn``
che aggiungo ai grafici rettangoli e poligoni, rispettivamente

```{r 24-Libro-29,echo=TRUE,fig.height=3}
plot(c(0,10),c(0,10),xlab = "", ylab="",axes=F,type="n") # inizia un grafico vuoto 
rect(xleft = 1,ybottom = 2,xright = 5,ytop = 4)
rect(xleft = 2,ybottom = 3,xright = 6,ytop = 9,density = 20,col=iblue)
rect(xleft = .5,ybottom = 3.5,xright = 3.5,ytop = 4.5,col=ared)
polygon(x = c(6,6.5,7,10),y = c(1,7,5,6),density = 10,
        col="darkorange",angle = -45,border = "gray39",lwd=4)
```

Le proporzioni tra ``x`` ed ``y`` si possono forzare con l'opzione ``asp``

```{r 24-Libro-30,echo=TRUE,results='markup'}
plot(0:10,0:10,xlab = "", ylab="",axes=F) # asp non specificato
axis(1,pos = 0)
axis(2,pos = 0,las=2)
```

```{r 24-Libro-31,echo=TRUE,fig.height=2.2,fig.width=2}
plot(0:10,0:10,xlab = "", ylab="",axes=F,asp=1) # asp=1 
axis(1,pos = 0,at = seq(0,10,by=2))
axis(2,pos = 0,las=2)
```

```{r 24-Libro-32,echo=TRUE,fig.height=2,fig.width=2}
plot(0:10,0:10,xlab = "", ylab="",axes=F,asp=9/16) # asp=9/16 
axis(1,pos = 0)
axis(2,pos = 0,las=2)
```


```{r 24-Libro-33,echo=TRUE,fig.height=4,fig.width=2}
plot(0:10,0:10,xlab = "", ylab="",axes=F,asp=4/3) # asp=4/3 
axis(1,pos = 0)
axis(2,pos = 0,las=2,at = seq(0,10,by=2))
```

I parametri del grafico si possono settare attraverso ``par``

```{r 24-Libro-34,echo=TRUE,results='markup'}
set.seed(1)
par(mfrow=c(1,2),cex=0.6) # 1 riga e 2 colonne
dati <- rnorm(100) # 100 dati dalla normale
plot(
  sort(dati),(1:100)/100,xlab = "x", ylab="F(x)",axes=F,type="l"
  ) # FdR empirica
axis(1)
axis(2)

hist(
  dati,c(-4,-3,-1.5,-.5,0,.5,1.5,3,4),main="Istogramma di densità",col="white"
  ) # istogramma dei dati
```



### Le Funzioni in R

Essendo R un linguaggio funzionale ogni funzione (che è essa stessa un oggetto)
si alimenta di uno o più oggetti di determinate classi e restituisce uno o 
più oggetti di una determina classe. Per esempio abbiamo visto come la funzione 
``matrix`` abbia come input un vettore e come output una matrice.

```{r 24-Libro-35,echo=TRUE,results='markup'}
x <- 1:10
class(x)
y <- matrix(x,nrow = 2)
class(y)
```

Le funzioni di R hanno argomenti con etichette, se gli argomenti rispettano l'ordine 
delle etichette non c'è bisogno di chiamarli altrimenti vanno etichettati anche loro.
Per esempio la funzione ``matrix`` si aspetta almeno due argomenti i dati ``data``
e il numero di righe ``nrow`` o il numero di colonne ``ncol``

````.r
matrix(data,nrow,ncol,byrow = F)
````

```{r 24-Libro-36,echo=TRUE,results='markup'}
x <- 1:10
matrix(x, 2)                # è identico a
matrix(nrow = 2, data = x)  # ma è diverso da
matrix(data = x, ncol = 2)  #
matrix(2, x)                # produce un risultato senza senso
```

#### Le funzioni statistiche

Tra le funzioni di base ricordiamo ``mean(x)`` la media del vettore ``x``
``var(x)`` la varianza **corretta**, ``sd(x)`` la SD **corretta** , ``median(x)`` la mediana
e ``quantile(x,p)`` il percentile di ordine $p$. Per esempio

```{r 24-Libro-37,echo=TRUE,results='markup'}
x <- c(2.3,4.5,6.7,2.1,3.8,2.5,6.9,9.8)
mean(x)   # media
var(x)    # varianza corretta
sd(x)     # sd corretta
quantile(x,c(.25,.50,.75)) # i percentili per p=.25,.50,.75 => i quartili
```

Per trattare le variabili doppie nella regressione ci sono strumenti appositi,
le funzioni ``cov(x,y)`` e ``cor(x,y)`` calcolano la correlazione e la covarianza tra i vettori
``x`` ed ``y``

```{r 24-Libro-38,echo=TRUE,results='markup'}
x <- c(2.3,4.5,6.7,2.1,3.8,2.5,6.9,9.8)
y <- c(2.4,2.1,5.6,7.2,6.5,7.1,4.3,9.7)
cov(x,y)   # covarianza
cor(x,y)   # r
cor(x,y)^2 # r quadro
```

La funzione ``lsfit`` (Least Squared Fit => Stima dei Minimi Quadrati) consente
di calcolare $\hat\beta_0$ e $\hat\beta_1$ rapidamente

```{r 24-Libro-39,echo=TRUE,results='markup'}
x <- c(1,2,3,4)
y <- c(1.9,1.2,3.5,3.1)
modello <- lsfit(x,y)
modello$coefficients # coefficienti beta 0 e beta 1
modello$residuals    # i residui osservati gli epsilon i

plot(x,y,
     axes=F,      # non disegnare gli assi lo farò dopo
     pch = 16,    # codice 16, per il pallino chiuso
     col = ared, #colore rosso per i pallini
     xlab="Reddito",
     ylab="Consumo",
     lty  =2,     # stile di tratteggiatura
     main = "Relazione tra Reddito e Conumo"
          )
# Un volta creato il grafico di base possiamo aggiungere 
axis(1) # asse delle x
axis(2) # asse delle y 

# e aggiungere una retta di coefficienti beta 0 e beta 1 

abline(modello$coefficients,lwd=2,col=iblue) 

```


#### Le VC di maggiore interessa

In R sono tabulate tutte le distribuzioni di maggiore interesse, comprese, la binomiale, la Poisson,
la Normale, la t di studente e il chi quadro, insieme a moltissime altre. La sintassi 
è relativamente semplice il prefisso ``d`` indica la funzione di densità o di probabilità
il prefisso ``p`` indica la funzione di ripartizione, il prefisso ``q`` indica
l'inverso della funzione di ripartizione e il prefisso ``r`` genera numeri casuali
da quella distribuzione. 
Per esempio se $X\sim\text{Binom}(n=5;\pi=0.3)$ 

```{r 24-Libro-40,echo=TRUE,fig.width=5,fig.height=3}
dbinom(x = 3,size = 5,prob = 0.3) # è la probabilità che X=3
dbinom(x = 0:5,size = 5,prob = 0.3) #  tutte le probabilità per X=0, 1, ..., 5
plot(0:5,dbinom(x = 0:5,size = 5,prob = 0.3),type="h",lwd=2) # grafico
pbinom(q = 3,size = 5,prob = 0.3) # è la probabilità che X≤3
qbinom(p = 0.75,size = 5,prob = 0.3) # 75-esimo percentile di X
rbinom(n = 10,size = 5,prob = 0.3)  # simula 10 estrazioni da X
```

Per esempio se $X\sim\text{Pois}(\lambda=4.3)$ 

```{r 24-Libro-41,echo=TRUE,fig.width=5,fig.height=3}
dpois(x = 3,lambda = 4.3) # è la probabilità che X=3
dpois(x = 0:10,lambda = 4.3) #  le probabilità per X=0, 1, ..., 10
plot(0:10,dpois(x = 0:10,lambda = 4.3),type="h",lwd=2) # grafico
ppois(q = 3,lambda = 4.3) # è la probabilità che X≤3
qpois(p = .75,lambda = 4.3) # 75-esimo percentile di X
rpois(n = 10,lambda = 4.3)  # simula 10 estrazioni da X
```

Per esempio se $X\sim N(\mu=2.5,\sigma^2=(1.5)^2)$ 

```{r 24-Libro-42,echo=TRUE,results='markup'}
par(mfrow=c(1,2),cex=0.6) # metto una figura accanto all'altra
curve(dnorm(x,mean = 2.5,sd = 1.5),
      from = 2.5-4*1.5,
      to = 2.5+4*1.5,
      axes=F,
      ylab = expression(f(theta)),
      xlab = expression(theta))
axis(1,c(2.5-2*1.5,2.5-1.5,2.5,2.5+1.5,2.5+2*1.5))
segments(x0 = c(2.5-1.5,2.5+1.5),
         y0 = 0,
         x1 = c(2.5-1.5,2.5+1.5),
         y1 = dnorm(c(2.5-1.5,2.5+1.5),2.5,1.5),
         lty = 2)
curve(pnorm(x,mean = 2.5,sd = 1.5),
      from = 2.5-4*1.5,
      to = 2.5+4*1.5,
      axes=F,
      ylab = expression(F(theta)),
      xlab = expression(theta))
axis(1)
axis(2)

pnorm(3,mean = 2.5,sd = 1.5)      # P(X<3)
1 - pnorm(3,mean = 2.5,sd = 1.5)  # P(X>3)
pnorm(3,mean = 2.5,sd = 1.5)-pnorm(0.5,mean = 2.5,sd = 1.5) #  P(0.5 < X < 3)

rnorm(25,2.5,1.5) # 25 estrazioni da X
```

#### Funzioni tra stringhe

Una stringa è un vettore di testo, non numerico, la funzione ``paste`` consente 
di incollare testi.

```{r 24-Libro-43,echo=TRUE,results='markup'}
vettore_testo1 <- c("mela","pera","pesca","banana")
vettore_testo2 <- c("rossa","verde","gialla","gialla")
paste(vettore_testo1,vettore_testo2,sep=" è ")
paste("la",paste(vettore_testo1,vettore_testo2,sep=" è "),collapse = " e ")
x <- round(rnorm(5,10,1),2)
x
paste("La somma degli x è",paste(x,collapse = " + ")," = ", sum(x))
```

La funzione ``cat`` stampa su schermo o su file e consente l'utilizzo di alcuni caratteri speciali

```{r 24-Libro-44,echo=TRUE,results='markup'}
cat(vettore_testo1,vettore_testo2)
cat(vettore_testo1,"\t",vettore_testo2)
cat(vettore_testo1,"\n",vettore_testo2)
```

#### Cicli e Condizioni

I cicli si possono fare con ``for`` ma come vedremo si possono aggirare in molti modi

```{r 24-Libro-45,echo=TRUE,results='markup'}
for (i in 1:5){
  cat(i,"; ")
}
```

Mentre le condizioni si risolvono con ``if`` ``else``

```{r 24-Libro-46,echo=TRUE,results='markup'}
a <- 12
if (a>10) {
  cat("Hai vinto! \n")
} else {cat("Hai perso! \n")}
a <- 5
if (a>10) {
  cat("Hai vinto! \n")
} else {cat("Hai perso! \n")}

```

anche la funzione ``ifelse`` è di aiuto

```{r 24-Libro-47,echo=TRUE,results='markup'}
cond <- TRUE
ifelse(cond,"VERO","FALSO")
cond <- FALSE
ifelse(cond,"VERO","FALSO")
```

#### Funzioni per Ovviare ai Cicli
R è un interprete e i cicli rallentano molto il funzionamento dei programmi
alcune funzioni speciali quali ``apply``, ``tapply``, ``lapplay`` e ``sapply``
applicano a diversi tipi di oggetti una funzione.
``apply`` applica una funzione alle righe o alle colonne di una matrice

```{r 24-Libro-48,echo=TRUE,results='markup'}
mat <- matrix(rnorm(12),nrow = 3)
mat
apply(X = mat, MARGIN = 1,FUN = "sum") # applica a mat per righe la somma
apply(X = mat, MARGIN = 2,FUN = "mean") # applica a mat per colonne la media
```

``tapply`` applica una funzione solo su elementi che rispettano una condizione

```{r 24-Libro-49,echo=TRUE,results='markup'}
sesso <- c("M","M","M","F","F")
eta   <- c(32.2,45.6,65.3,34.1,43.2)
tapply(eta, sesso, median) # applica la mediana al gruppo M e al gruppo F
```

Le funzioni ``lapplay`` e ``sapply`` (versione semplificata delle prima) applicano una funzione
ad una lista o agli elementi di un vettore

```{r 24-Libro-50,echo=TRUE,results='markup'}
gruppo1 <- rnorm(5,9.4,1.1)  # gruppo1 5 estrazioni da una N( 9.4,1.1^2)
gruppo2 <- rnorm(9,12.2,1.2) # gruppo2 9 estrazioni da una N(12.2,1.2^2)
gruppo3 <- rnorm(7,11.7,0.9) # gruppo3 7 estrazioni da una N(11.7,0.9^2)
campione <- list(gruppo1,gruppo2,gruppo3)
# applica media e varianza ad ogni gruppo:
lapply(campione,FUN = function(x)c(mean(x),var(x))) 
sapply(campione,FUN = function(x)c(mean(x),var(x))) # come sopra ma in matrice
```

#### Funzioni personalizzate

Le funzioni si possono creare con la funzione ``function``

```{r 24-Libro-51,echo=TRUE,results='markup'}
funz1 <- function(x) x^2 # funz1 usa x come input e x^2 come output
funz1(3)
funz2 <- function(x,y) { # funz2 usa x e y come input 
  risultato <- ifelse(x>y,x+y,"x deve essere maggiore di y!")
  return(risultato)
} 
funz2(2,3)
funz2(3,1)

```

Possiamo per esempio creare la funzione che crei la varianza del campione e la SD non corretta

```{r 24-Libro-52,echo=TRUE,results='markup'}
var_pop <- function(x) {
  vpop <- mean(x^2)-mean(x)^2 # media dei quadrati meno il quadrato della media
  return(vpop)  
}
n <- 10
x <- rnorm(n,mean = 10,sd = 1)
var(x)     # varianza corretta
var_pop(x) # varianza NON corretta
n/(n-1)*var_pop(x) # correzione

sd_pop <- function(x) sqrt(var_pop(x)) # rad. quad. della varianza 
sd_pop(x)
```

Possiamo anche creare una funzione che calcola un intervallo di confidenza 
al livello $1-\alpha$, per un $\alpha$ qualunque

```{r 24-Libro-53,echo=TRUE,results='markup'}
## intervallo di confidenza per mu, sigma nota

idc_mu <- function(mu_obs,sigma,alpha=0.05){
z_alpha <- qnorm(1-alpha/2) # fisso z_alpha
SE      <- sigma/sqrt(n) # SE per mu = sigma diviso radice di n

cat("L'intervallo di confidenza al ",(1-alpha)*100,"% è dato da \n",
    "[",mu_obs," - ",z_alpha," x ",SE," ; ",mu_obs," + ",z_alpha," x ",SE,"] \n",
    "[",mu_obs-z_alpha*SE," ; ",mu_obs+z_alpha*SE,"]",
    sep="")
}

alpha   <- 0.01     # fisso alpha
mu_obs  <- 23.3     # fisso la media del campione
sigma   <- 2.13     # fisso il sigma di popolazione
n       <- 34       # fisso n

idc_mu(mu_obs = mu_obs, sigma = sigma, alpha = alpha)
```


[1] R Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.

[2]  JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone (2022). rmarkdown: Dynamic Documents for R. R package version 2.14. URL https://rmarkdown.rstudio.com.

[3] Yihui Xie and J.J. Allaire and Garrett Grolemund (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC.   ISBN 9781138359338. URL https://bookdown.org/yihui/rmarkdown.

[4] Yihui Xie and Christophe Dervieux and Emily Riederer (2020). R Markdown Cookbook. Chapman and Hall/CRC. ISBN  9780367563837. URL https://bookdown.org/yihui/rmarkdown-cookbook.

<!--chapter:end:24-Libro.Rmd-->

---
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

# Funzioni usate nel libro

Presento le funzioni che sono state create da me per risolvere vari problemi
di automazione, dalla creazione dei data set alla soluzione di alcuni problemi.

```{r 25-test-functions-1, include=FALSE}

rm(list = ls())


source("intro.R")

i1 <- 1
i2 <- 0
item <- function(){
it <- (paste(i1,".",letters[i2],sep = ""))
return(it)}
fih <- ifelse(html,4,3)
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE,fig.height = fih,results='asis')
opts_knit$set(echo=T,results='asis')
virg <- "`"
```

## Istogramma 

```{r 25-test-functions-2,results='asis'}
#############################################################################
#############################################################################
## Funzioni per generare i dati dell'esercizio 1
##
## genera_dati(brk,hhh=NULL,n,nnn=NULL,rand = T)
##
##  brk      intervalli (breaks)
##  hhh      aspetto presunto
##  n        numero totale individui
##  nnn      alternativo ad hhh, frequenza da riportare ad n
##  rand     i numeri sono casuali?
##
## tabl(x,...)                   shortcut personalizzato a kable
##
##  x        oggetto da stampare in tabella
##
## ls2e(stat_base(samp,brk))   crea diversi oggetti
##  dat2     tabella con intestazioni semplici
##  dat3     tabella con intestazioni da stampa
##  H.int(x) densità percentuale
##  F.int(x) Funzione di ripartizione
##  Q.int(p) Inversa della FdR
##    x      vettore di valori
##    p      vettore di frequenze 
##  histp(axes=T,...) istogramma
##  h.int(x1,x2,...)  evidenzia istogramma
##    x1     limite inferiore
##    x2     limite superiore

set.seed(2)                      # per ottenere sempre la stessa simulazione
n <- 60                          # ampiezza campionaria

brk  <- c(0,1.5,3,5,7.5,15)      # intervalli (breaks)
hhh  <- c( 2,11,10, 2,1)         # aspetto presunto istogramma

nomex <- "Nome della X"          # nome della X
samp <- genera_dati(
  brk = brk,hhh = hhh,n = n)     # genera i dati dall'istogramma

ls2e(stat_base(samp,brk))      # crea il data set e la tabella dat3

tabl(dat3)

H.int(2:3)            # Calcolo della Densità percentuale
F.int(2:3)            # Calcolo della Ripartizione
Q.int((0:4)/4)        # Inverse della Ripartizione
histp(axes=T)         # Istogramma
h.int(2,8,col=ared,   # Aree selezionate
      density = 25)   
```
```{r 25-test-functions-3,results='asis'}
percentile(p = 0.45)       # Calcolo percentile
F_print(2,"<")             # calcolo della prop inferiore a 2
F_print(2,">")             # calcolo della prop superiore a 2
F_print(8,">")
F_print(x = 2,verso = "",x2 = 8) # intervallo 2-8
media_(1:4)                     # media dei dati 1,2,3,4
var_(1:4)                       # varianza dei dati 1,2,3,4
stat_(1:4)                      # media e varianza insieme (new)
stat_(1:4,p = c(2,4,5,1))       # vettore dei pesi p
stat_(rep(1:4,times=c(2,4,5,1)),semp = T) # in frazione

```


## Probabilità

### Tavole della somma
```{r 25-test-functions-4, results='asis'}
# Somma di due dadi
c1 <- 6
c2 <- 6
re1 <- (two_way(S_1 = 1:c1,S_2 = 1:c2,
                num1 = rep(1,times=c1),num2 = rep(1,times=c2),
                size = "\\footnotesize"))

# Differenza di due dadi
res<-two_way(S_1 = 1:c1,S_2 = 1:c2,size = "\\footnotesize",
             num1 = numeric(c1)+1,num2 = numeric(c2)+1,op = `-`)
res[[1]]
names(res)


```

### Binomiale
```{r 25-test-functions-5, results='asis'}
bin_dis(x1 = 2,n = 5,pp = 0.34)
bin_dis(x1 = 4,n = 5,pp = 0.34,verso = "\\geq")
bin_dis(x1 = 2,n = 5,pp = 0.34,comp = T)
bin_dis(x1 = 2,n = 5,pp = 0.34,sing = T)
```

### Poisson
```{r 25-test-functions-6, results='asis'}
pois_dis(x1 = 2,ll = 1.5)
pois_dis(x1 = 2,ll = 1.5,verso = "\\geq")
pois_dis(x1 = 2,ll = 1.5,sing = T)

```

### Normale
```{r 25-test-functions-7, results='asis'}
norm_int(x1 = 1,verso = "<",mm = 3,ss = 2.2,vnam = "\\theta",
             mu = "\\mu_\\theta",sigma = "\\sigma_\\theta")
norm_int(x1 = 4,verso = "<",mm = 3,ss = 2.2,vnam = "X",
             mu = "\\psi",sigma = "\\tau")
norm_int(x1 = 1,verso = ">",mm = 3,ss = 2.2,vnam = "Y",
             mu = "\\mu_Y",sigma = "\\sigma_Y")
norm_int(x1 = 4,verso = ">",mm = 3,ss = 2.2,)
norm_int(x1 = 1,verso = ">",mm = -3,ss = 2.2)
norm_int(x1 = 1,x2=2,mm = 3,ss = 2.2,verso = NULL)
norm_int(x1 = 1,x2=2,mm = -3,ss = 2.2,verso = NULL)
norm_int(x1 = -1,x2=2,mm = -3,ss = 2.2,verso = NULL)
```

### TLC

```{r 25-test-functions-8,results='asis'}
tlc(tipo = "somma",x1 = 90,x2 = 110,verso = NULL,mu = 1,s2 = 1,n = 100)
tlc(tipo = "media",x1 = 9,x2 = 11,verso = NULL,mu = 10,s2 = 1,n = 100)
tlc(tipo = "prop",x1 = .1,verso = ">",mu = .2,n = 50)
```

## Inferenza

### Intervalli di Confidenza
```{r 25-test-functions-9, results='asis'}
idc(xm = 10,sd = 1.1,alpha = .05,n = 15,dist_ = "z")
idc(xm = 10,sd = 1.1,alpha = .05,n = 15,dist_ = "t")
idc(xm = 10,alpha = .05,n = 15,dist_ = "z")
idc(xm = 7.4,sd = sqrt(7.4),alpha = .05,n = 75,dist_ = "z",mus = "\\lambda",
        ss = "\\sqrt\\lambda")
```


### Test
```{r 25-test-functions-10, results='asis'}
ztest_mu(muh = 0,s = 1,10,mu0 = 1,h1 = "\\neq",alpha = 0.05)
ztest_mu(muh = 0,s = 1,10,mu0 = 1,h1 = "\\neq")
ztest_mu(muh = 0,s = 1,10,mu0 = 1,h1 = "\\neq",pv_only = T)
ttest_mu(muh = 0,sh = 1,n = 10,mu0 = 1,h1 = "<",alpha = 0.01)
ttest_mu(muh = 0,sh = 1,n = 10,mu0 = 1,h1 = "<")
ttest_mu(muh = 0,sh = 1,n = 10,mu0 = 1,h1 = "<",pv_only = T,um="cm")
ztest_pi(sn = 60,n = 100,p0 = .5,h1 = ">",alpha = 0.05)
test_2c(mu1 = 11,mu2 = 12,s1h = 1.1,s2h = 1.2,n1 = 10,n2 = 12,
            h1 = "\\neq",et = T)
test_2c(mu1 = 11,mu2 = 12,s1h = 1.1,s2h = 1.2,n1 = 10,n2 = 12,
            h1 = "\\neq",alpha = .05,et = F)
test_2c(mu1 = 11,mu2 = 12,s1h = F,s2h = NULL,n1 = 50,n2 = 60,
            h1 = "\\neq",alpha = .05,et = T)
ttest_2c_et(mu1 = 11,mu2 = 12,s1h = 1.1,s2h = 1.2,n1 = 10,n2 = 12,
                h1 = "\\neq",alpha = .05)
ttest_2c_om(mu1 = 11,mu2 = 12,s1h = 1.1,s2h = 1.2,n1 = 10,n2 = 12,
                h1 = "\\neq",rbow = T)
ztest_2c_pi(s1 = 120,s2 = 130,n1 = 250,n2 = 260,h1 = "<",alpha = .01)
```


### Regressione
```{r 25-test-functions-11,results='asis'}
set.seed(12)                 # ripete le stesse generazioni casuali
n <- 100                     # fisso n
x <- rnorm(n,10)             # genero x
y <- x+rnorm(n,0,1)          # genero y
ls2e(regr(x = x,y = y))    # produco le statistiche di base

calcolo_beta()
calcolo_beta(semplice = T)
residuo(x[12],y[12])
se_beta0()
se_beta1()
ttest_beta(cof = 0,bj0 = 0,h1 = "<",alpha = 0.01)
ttest_beta(cof = 1,bj0 = 0,h1 = "\\neq",alpha = 0.01)
```

## Esempi


#### Esercizio 1


````.markdown
```{r 25-test-functions-12, echo=FALSE}`r ' '`
set.seed(1)                      # per ottenere sempre la stessa simulazione
n <- 250                         # ampiezza campionaria

brk  <- c(0,15,30,50,100,250)          # intervalli (breaks)
hhh  <- c( 20,120,100, 50,10)       # aspetto presunto istogramma

nomex <- "Spesa"
samp <- genera_dati(brk = brk,hhh = hhh,n = n)

ls2e(stat_base(samp,brk))        # crea il data set e la tabella dat3
```

Su un campione di $`r virg`r n`$ famiglie della provincia di Modena è stato 
rilevata la spesa mensile in telecomunicazioni (in euro), qui di seguito la 
distribuzione delle frequenze relative:

```{r 25-test-functions-13, echo=FALSE}`r ' '`
kable(dat3[,c(1,2,4)]) %>%
  kable_styling(full_width = F)
```

`r i2 <- i2+1;item()` (**Punti 14**) Disegnare l'istogramma di densità 
percentuale.

**Soluzione**

```{r 25-test-functions-14, echo=FALSE}`r ' '`
kable(dat3) %>%            # Stampa la tabella
  kable_styling(full_width = F)
histp(axes = T)
h.int(60,250,density=20)
```

`r i2 <- i2+1;item()` (**Punti 3**) Qual è la percentuale di famiglie con 
spesa superiore a 60 euro?

**Soluzione**

```{r 25-test-functions-15, echo=FALSE}`r ' '`
F_print(60,verso=">")
```
````
_________________________________________________________

```{r 25-test-functions-16,echo=FALSE}
set.seed(1)                      # per ottenere sempre la stessa simulazione
n <- 250                         # ampiezza campionaria

brk  <- c(0,15,30,50,100,250)          # intervalli (breaks)
hhh  <- c( 20,120,100, 50,10)       # aspetto presunto istogramma

nomex <- "Spesa"
samp <- genera_dati(brk = brk,hhh = hhh,n = n)

ls2e(stat_base(samp,brk))        # crea il data set e la tabella dat3
i1 <- 1
i2 <- 0

```
\newpage
Su un campione di $`r n`$ famiglie della provincia di Modena è stato rilevata la spesa mensile in telecomunicazioni (in euro), qui di seguito 
la distribuzione delle frequenze relative:

```{r 25-test-functions-17,echo=FALSE,}
kable(dat3[,c(1,2,4)],booktab=T,            # Stampa la tabella  
      escape = F,linesep = "") %>%            
  kable_styling(full_width = F,latex_options = "HOLD_position")
```

`r i2 <- i2+1;item()` (**Punti 14**) Disegnare l'istogramma di densità percentuale.

**Soluzione**

```{r 25-test-functions-18,echo=FALSE}
kable(dat3[,c(1:7,12)],booktab=T,            # Stampa la tabella  
      escape = F,linesep = "") %>%            
  kable_styling(full_width = F,latex_options = "HOLD_position")
histp(axes = T)
h.int(60,250,density=20)
```

`r i2 <- i2+1;item()` (**Punti 3**) Qual è la percentuale di famiglie con spesa superiore a 60 euro?

**Soluzione**

```{r 25-test-functions-19,results='asis',echo=FALSE}
F_print(60,verso=">")
```


```{r 25-test-functions-20,echo=FALSE}
# preparo i parametri
s1 <- 27
n1 <- 37
s2 <- 30
n2 <- 45
alpha <- 0.05
h1 <- "\\neq"
i1 <- 5
i2 <- 0
```

_________________________________________________________

````.markdown
```{r 25-test-functions-21, echo=FALSE}`r ' '`
# preparo i parametri
s1 <- 27
n1 <- 37
s2 <- 30
n2 <- 45
alpha <- 0.05
h1 <- "\\neq"
```

`r virg`r i2 <- i2+1;item()` Sono stati intervistati `r virg`r n1` uomini 
e `r virg`r n2` donne, `r virg`r s1` su `r virg`r n1` uomini si sono 
dichiarati favorevoli, mentre sono favorevoli `r virg`r s2` su `r virg`r n2` 
donne. Testare al livello di significatività del 5% l'ipotesi che uomini e 
donna abbiano lo stesso parare contro l'alternativa che siano diversi.

**Soluzione**

```{r 25-test-functions-22,results='asis', echo=FALSE}`r ' '`
ztest_2c_pi(s1 = s1,s2 = s2,n1 = n1,n2 = n2,
                h1 = h1,alpha = alpha,a = "U",b = "D")
```

`r virg`r i2 <- i2+1;item()` Costruire un intervallo di confidenza al 95% per 
la proporzione di uomini favorevoli

**Soluzione**

```{r 25-test-functions-23,results='asis', echo=FALSE}`r ' '`
idc(xm = s1,alpha = .95,n = n1 ,dist_ = "z")
```
````

_________________________________

`r i2 <- i2+1;item()` Sono stati intervistati `r n1` uomini e `r n2` donne, 
`r s1` su `r n1` uomini si sono dichiarati favorevoli, mentre sono favorevoli 
`r s2` su `r n2` donne. Testare al livello di significatività del 5% l'ipotesi 
che uomini e donna abbiano lo stesso parare contro l'alternativa che siano 
diversi. 


**Soluzione**

```{r 25-test-functions-24,results='asis',echo=FALSE}
ztest_2c_pi(s1 = s1,s2 = s2,n1 = n1,n2 = n2,
                h1 = h1,alpha = alpha,a = "U",b = "D")
```

`r i2 <- i2+1;item()` Costruire un intervallo di confidenza al 95% per la proporzione di uomini 
favorevoli 

**Soluzione**

```{r 25-test-functions-25,results='asis',echo=FALSE}
idc(xm = s1,alpha = .95,n = n1 ,dist_ = "z")
```

_______________________________


<!--chapter:end:25-test-functions.Rmd-->

