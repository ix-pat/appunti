---
editor_options:
  chunk_output_type: console
output: pdf_document
---

```{r setupstim, include=FALSE}
source("intro.R")

```

# Elementi di Teoria della Stima

Stimare, in statistica, significa scegliere un valore o un insieme dei parametri di popolazione
alla luce di un campione. 

## Campionamento



Un campione casuale è un certo numero di $n$ di osservazioni prese a caso dalla popolazione $\mathscr{P}$


Un campione casuale è la migliore garanzia che abbiamo affinché la scelta dell'unità da rilevare non sia guidata da criteri che possano alterare i risultati.


Alcune informazioni possono essere usate per suddividere la Popolazione di partenza in sottogruppi prima di estrarre campioni. La _Teoria Dei Campioni_ è quella disciplina della statistica che studia come costruire un campione che sia, al tempo stesso sufficientemente piccolo e sufficientemente rappresentativo.


In questo corso guarderemo una sola forma di campionamento declinata in due modi

  - Campionamento Casuale Semplice Senza reintroduzione (equivalente delle estrazioni SR)
  - Campionamento Casuale Semplice Con reintroduzione (equivalente delle estrazioni CR)

### Lessico

Un campione in essere (prima di essere osservato) è una sequenza di VC
\[X_1,...,X_n\]

un campione osservato è un insieme di numeri
\[\mathbf{x}=(x_1,...,x_n)\]

Lo _Spazio dei Campioni_ $\mathcal{S}$ è il supporto della VC multipla $X_1,...,X_n$
è l'insieme di tutti i possibili campioni di ampiezza $n$: $(x_1,...,x_n)$ che possiamo osservare.

### Esempio al finito


Si dispone di un'urna contente 4 bussolotti
\[\{0,1,3,7\}\]

Se estraiamo SR, lo spazio dei campioni è di dimensione $\#\mathcal{S}=4(\times 6)$ ed è descritto in tabella (\@ref(tab:csr)).
Anziché tutti i 24 possibili campioni la tabella mostra solo i 4 campioni riordinati, perché il campione (0,1,3), ai nostri fini
è identico al campione (3,1,0) e al campione (0,3,1), ecc.

```{r,results="asis"}
xp <- c(0,1,3,7)

camp <- combn(xp,3)

dimnames(camp)[[1]]<- c("$x_{(1)}$","$x_{(2)}$","$x_{(3)}$")
dimnames(camp)[[2]] <- paste("C.",1:ncol(camp))
mu<-mean(xp) 
sig<-mean(xp^2)-mean(xp)^2
kable((camp),booktabs=T,escape=F,caption = "I 4 campioni riordinati nell'ipotesi di CCS SR",label = "csr") %>%
column_spec (1:ncol(camp),border_left = F, border_right = T) 
```

Se invece estraiamo CR, lo spazio dei campioni è di dimensione $\#\mathcal{S}=64$ ed è descritto in tabella (\@ref(tab:ccr)).

```{r,results="asis"}
xp <- c(0,1,3,7)
  
xxx <- expand.grid(xp,xp,xp)
xxx <- xxx[order(xxx[,1],xxx[,2],xxx[,3]),]
xord <- t(apply(xxx,1,sort))[,3:1]
cxord <- apply(xord,1,function(x)paste(x,collapse = ",") )
# cxord <- apply(xord,1,function(x)paste(x,collapse = "") )
nr <- tapply(rep(1,64),cxord,sum)
pr <- nr/sum(nr)
# 

camp <- unique(xord)

camp <- data.frame(camp)
camp$nr <- factor(nr)
camp$pr <- pr
names(camp)<- c("$x_{(1)}$","$x_{(2)}$","$x_{(3)}$","$\\#$ c.","$P(C.)$")
camp <- data.frame(t(camp))
dimnames(camp)[[2]] <- paste("C.",1:ncol(camp))
mu<-mean(xp) 
sig<-mean(xp^2)-mean(xp)^2
kable(t(camp),booktabs=T,escape=F,caption = "I 64 campioni riordinati estratti senza reinserimento.",label = "ccr") %>%
row_spec (1:ncol(camp)) %>%
#  column_spec(column = 4,background = "grey80",bold=F,col=iblue,align = 'c') %>%
kable_styling(font_size = 8)
```


### Lessico


Siano $X_1,..,X_n$ $n$ VC IID, replicazioni della stessa variabile $X\sim\mathscr{L}(\theta)$, $\theta\in\Theta$


Lettere greche per i parametri (incogniti) della popolazione e lettere latine per le osservazioni. 


 - Bernoulli: $X_1,...,X_n$ VC IID, $X_i\sim \text{Ber}(\pi)$, $\theta\equiv\pi$, $\Theta\equiv[0,1]$.
 - Poisson: $X_1,...,X_n$ VC IID, $X_i\sim \text{Pois}(\lambda)$, $\theta\equiv\lambda$, $\Theta\equiv\mathbb{R}^+$
 - Normale: $X_1,...,X_n$ VC IID, $X_i\sim N(\mu,\sigma^2)$, $\theta\equiv(\mu,\sigma^2)$, $\Theta\equiv\mathbb{R}\times\mathbb{R}^+$
 - Generico: $X_1,..,X_n$ $n$ VC IID, replicazioni della stessa variabile $X\sim\mathscr{L}(\theta)$, $\theta\in\Theta$


Nel _paradigma classico_ la probabilità si assegna alle $X_i$, in quanto risultato di un sorteggio casuale
\[P(X_1,...,X_n;\theta)\]


Ma non è consentito trattare con lo strumento della probabilità l'incertezza sul parametro $\theta$ che governa la popolazione. Perché $\theta$ è incognito ma non è il frutto di una selezione casuale.


Nel _paradigma Bayesiano_ l'incertezza sul parametro viene trattata con gli stessi strumenti dell'incertezza sui dati, dando vita ad un teoria coerente e molto utile per alcune applicazioni particolari.

## Gli stimatori

_Stimare_, in statistica, significa scegliere un punto (stima puntuale) o una regione (stima intervallare) dello spazio dei parametri $\Theta$ alla luce dei dati $x_1,...,x_n$.

:::: {.info data-latex=""}
::: {.definiton}
Uno **stimatore** puntuale (point estimator) è una statistica $h$ che trasforma il campione $X_1,...,X_n$ in un punto dello spazio dei parametri:
\[h:\mathcal{S}\to\Theta\]
:::
::::

Il campione $X_1,...,X_n$ casuale viene trasformato attraverso $h$ in un punto specifico di $\Theta$
\[h(X_1,...,X_n)=\hat\theta\in\Theta\]

Uno **stimatore** è una variabile casuale in quanto funzione di valori casuali.

:::: {.example}
Da una popolazione che ha $E(X_i)=\mu$ incognita, potremmo proporre di _stimare_ $\mu$ con la media dei dati che _otterremo_:
\[h(X_1,...,X_n)=\frac 1 n \sum_{i=1}^n X_i=\bar X=\hat \mu\]
::::

:::: {.example} 
Da una popolazione di Poisson che ha $X_i\sim\text{Poiss}(\lambda)$, $\lambda$ incognita, potremmo proporre di _stimare_ $\lambda$ con la mediana dei dati che _otterremo_:
\[h(X_1,...,X_n)=X_{0,5}=\hat\lambda\]
::::

### Stimatori e Stime

::: {.nota data-latex=""}
- Uno **Stimatore**: $h(X_1,...,X_n)$ è funzione di $X_1,...,X_n$ è una **VC**
- Una **Stima**: $h(x_1,...,x_n)$ è funzione di $x_1,...,x_n$ e dunque è un **numero**
:::

:::: {.example}
Da una popolazione che ha $E(X_i)=\mu$ incognita, potremmo proporre di _stimare_ $\mu$ con la media dei dati che _otterremo_:
\[h(X_1,...,X_n)=\frac 1 n \sum_{i=1}^n X_i=\hat \mu\]

Estraiamo $n=5$ individui dalla popolazione $x_1=2.1$, $x_2=2.4$, $x_3=3.2$, $x_4=1.7$, $x_5=3.0$,  
```{r}
xs <- c(2.1,2.4,3.2,7.1,3.0)
```

Per ottenere $\hat\mu$ la stima di $\mu$ applichiamo $h$ ai dati e otteniamo
\[\hat\mu = \frac 1 5\sum_{i=1}^5x_i=\frac 1 5 `r sum(xs)`=`r mean(xs)`\]
::::

:::: {.example} 
Da una popolazione di Poisson che ha $X_i\sim\text{Poiss}(\lambda)$, $\lambda$ incognita, potremmo proporre di _stimare_ $\lambda$ con la mediana dei dati che _otterremo_:
\[h(X_1,...,X_n)=X_{0,5}=\hat\lambda\]


Osserviamo $n=5$ valori (già riordinati)<br/> 
$x_{(1)}=0$, $x_{(2)}=0$, $x_{(3)}=2$, $x_{(4)}=2$, $x_{(5)}=3$, $x_{(6)}=4$, $x_{(7)}=7$,    


Per ottenere $\hat\lambda$ la stima di $\lambda$ applichiamo $h$ ai dati e otteniamo
\[\hat\lambda = x_{0.5}=x_{(4)}=2\]
::::

### Come scegliere uno stimatore


La definizione non offre un criterio per la scelta. 
Gli stimatori vengono costruiti per avere la miglior _precisione possibile_.
La precisione non si può valutare sulla singola _stima_ ma studiando, 
prima di osservare i dati, le proprietà probabilistiche dello _stimatore_.

Le proprietà auspicabili per uno stimatore sono di due tipi

- Esatte (per $n$ finito)
- Asintotiche (per $n$ che diverge)

### Proprietà Auspicabili di uno stimatore (per $n$ finito)


:::: {.info data-latex=""}
::: {.definition name="Correttezza di uno stimatore"}
Siano $X_1,...,X_n$, $n$ VC, IID, replicazioni della stessa $X\sim\mathscr{L}(\theta)$, sia $h$ uno stimatore per $\theta$.
Lo stimatore $h$ si dice **corretto** se
\[E(h(X_1,...,X_n))=E(h)=\theta\]
:::
::::

:::: {.info data-latex=""}
::: {.definition name="Mean Squared Error di uno stimatore"}
Si definisce **Errore Quadratico Medio** (_Mean Squared Error_) la quantità
\[MSE(h)=E((h-\theta)^2)=V(h)+B^2(h)\]
dove
\[B(h)=|E(h)-\theta|\]
:::
::::

:::: {.nota}
se $h$ è corretto allora
\[MSE(h)=V(h)\]
::::

:::: {.info data-latex=""}
::: {.definition name="Efficienza di uno stimatore"}
Siano $h_1$ e $h_2$ due stimatori per $\theta$, si dice che $h_1$ è **più efficiente** di $h_2$ se e solo se
\[MSE(h_1)<MSE(h_2)\]
:::
::::

:::: {.nota data-latex=""}
Se $h_1$ e $h_2$ sono entrambi corretti, allora, $h_1$ è **più efficiente** di $h_2$ se e solo se
\[V(h_1)<V(h_2)\]

L'**errore** di uno stimatore è l'_inverso_ della sua **precisione**.
::::

### Media aritmetica e varianza campionaria caso IID

Siano $X_1,...,X_n$, $n$ VC, IID, replicazioni della stessa $X$ tale che $E(X)=\mu$ e $V(X)=\sigma^2$, sia $h\equiv\hat \mu$ uno stimatore per $\mu$
\[
\hat \mu=\bar X=\frac 1 n \sum_{i=1}^n X_i
\]


Dai risultati che già conosciamo sappiamo che
\[
E(\hat \mu)=\mu
\]
e dunque $\hat \mu$ è sempre uno stimatore corretto per $\mu$.
Essendo $\hat \mu$ corretto per $\mu$ allora
\[MSE(\hat \mu)=V(\hat \mu)=\frac{\sigma^2}n\]

Si consideri la varianza campionaria:
\[
\hat\sigma^2=\frac 1 n \sum_{i=1}^n(X_i-\hat \mu)^2
\]

Si può dimostrare che

:::: {.info data-latex=""}
\[
E(\hat \sigma^2)=\frac {n-1}n \sigma^2<\sigma^2
\]
::::

### Media aritmetica campionamento SR (popolazioni finite)

Siano $X_1,...,X_n$, $n$ VC, osservazioni estratte SR da una popolazione $X$ di $N$ individui, tale che $E(X)=\mu$ e $V(X)=\sigma^2$, sia $h\equiv\hat \mu$ uno stimatore per $\mu$
\[
\hat \mu=\frac 1 n \sum_{i=1}^n X_i
\]
Allora
\[
E(\hat \mu)=\mu
\]
e dunque $\hat \mu$ è sempre uno stimatore corretto per $\mu$.
Essendo $\hat \mu$ corretto per $\mu$ allora
\[MSE(\hat \mu)=V(\hat \mu)\]

Per calcolare $V(\hat \mu)$ dobbiamo tenere conto della _frazione di campionamento_ $n/N$

:::: {.info data-latex=""}
\[
MSE(\hat \mu)=\frac{N-n}{N-1}\frac{\sigma^2} n
\]
::::

dove $\frac{N-n}{N-1}$ è chiamato _coefficiente di correzione per popolazioni finite_.
Osserviamo che più alto è $n$ più il coefficiente tende ad 1.
Se $n = N$ il coefficiente diventa zero il campione è diventato l'intera popolazione  e 
l'incertezza sulla media è zero.
  
### Esempi

:::: {.example name="Popolazione finita, estrazioni SR (Stima)"}

```{r,results="asis"}
xp <- c(0,1,3,7)

camp <- combn(xp,3)

dimnames(camp)[[1]]<- c("$x_1$","$x_2$","$x_2$")
dimnames(camp)[[2]] <- paste("C.",1:ncol(camp))
mu<-mean(xp) 
sig2<-mean(xp^2)-mean(xp)^2
# kable((camp)) %>%
# column_spec (1:ncol(camp),border_left = F, border_right = T) %>%
# kable_styling()


```


Si dispone di un'urna contente 4 bussolotti
\[\{0,1,3,7\}\]

È immediato calcolare
\[\mu=`r mu`, \qquad \sigma^2=`r sig2` \]

Proponiamo due stime diverse di $\mu$
\begin{eqnarray*}
h_1 &=& \frac 1 3 \sum_{i=1}^3 x_i\\
h_2 &=& x_{0.5}
\end{eqnarray*}

Se estraiamo SR, $\#\mathcal{S}=4(\times 6)$, lo spazio dei campioni è

```{r stima-sr,results="asis"}
xp <- c(0,1,3,7)

camp <- combn(xp,3)
h1 <- apply(camp,2,mean)
h2 <- apply(camp,2,median)
camp <- rbind(camp,h1,h2)+.000001
dimnames(camp)[[1]]<- c("$x_{(1)}$","$x_{(2)}$","$x_{(3)}$","$h_1$","$h_2$")
dimnames(camp)[[2]] <- paste("C.",1:ncol(camp))
mu<-mean(xp) 
sig2<-mean(xp^2)-mean(xp)^2
 kable((camp),booktabs=T,escape=F,caption = "Media e mediana dei campioni") %>%
 row_spec(row = 4:5,hline_after = T,bold=F,col=iblue) %>%
 column_spec (1:ncol(camp),border_left = F, border_right = T) %>%
  kable_styling(latex_options = "hold_position",font_size = 8)

```


In tabella \@ref(tab:stima-sr) la $h_1$ e $h_2$ associati ad ogni campione. Avremo che
Otteniamo:
\[E(h_1)=\frac 1{4}(`r camp[4,1]`+`r camp[4,2]`+`r camp[4,3]`+`r camp[4,4]`)=`r mean(camp[4,])`=\mu\]
$h_1$ è **corretto** 
mentre
\[E(h_2)=\frac 1{4}(`r camp[5,1]`+`r camp[5,2]`+`r camp[5,3]`+`r camp[5,4]`)=`r mean(camp[5,])`\neq\mu\]
non lo è.
::::


:::: {.example name="Popolazione finita, estrazioni SR (Efficienza)"}
Ricordiamo che l'**Errore Quadratico Medio** (_Mean Squared Error_) per $\mu$ è
\[MSE(h)=E((h-\mu)^2)=V(h)+B^2(h)\]
dove
\[B(h)=|E(h)-\mu|\]

Osserviamo la precisione di $h_1$:
\[MSE(h_1)=\frac 1{4}\left((`r camp[4,1]`-`r mu`)^2+(`r camp[4,2]`-`r mu`)^2+(`r camp[4,3]`-`r mu`)^2+(`r camp[4,4]`-`r mu`)^2\right)=`r mean(camp[4,]^2)-mu^2`\]

Osserviamo la precisione di $h_2$:
\[MSE(h_2)=\frac 1{4}((`r camp[5,1]`-`r mu`)^2+(`r camp[5,2]`-`r mu`)^2+(`r camp[5,3]`-`r mu`)^2+(`r camp[5,4]`-`r mu`)^2)=`r mean((camp[5,]-mu)^2)`\]

E quindi essendo $MSE(h_1)=`r mean(camp[4,]^2)-mu^2`<`r mean(camp[5,]^2)-mean(camp[5,])^2 + (mean(camp[5,])-mu)^2`=MSE(h_2)$, e dunque $h_1$ è _più efficiente_ di $h_2$.
 
:::: {.nota data-latex=""}
\[V(h_1)=V(\hat \mu)=\frac{N-n}{N-1}\frac{\sigma^2}{n}=\frac{4-3}{4-1}\frac{`r sig2`}{3}\]
::::
::::

:::: {.example name="Popolazione finita, estrazioni SR (Stima della Varianza)"}
Proponiamo la varianza campionaria $\hat\sigma^2$ per stimare $\sigma^2=`r sig2`$
\[\hat\sigma^2=\frac 1 3\sum_{i=1}^3(x_i-\hat \mu)^2\]


```{r var-sr,results="asis"}
xp <- c(0,1,3,7)

camp <- combn(xp,3)
hv <- apply(camp,2,vvv)
camp <- rbind(camp,hv)
dimnames(camp)[[1]]<- c("$x_{(1)}$","$x_{(2)}$","$x_{(3)}$","$\\hat\\sigma^2$")
dimnames(camp)[[2]] <- paste("C.",1:ncol(camp))
mu<-mean(xp) 
sig2<-mean(xp^2)-mean(xp)^2
 kable((camp),booktabs=T,escape=F,caption = "Varianza dei campioni SR di ampiezza $n=3$") %>%
 row_spec(row = 4,hline_after = T,bold=F,col=iblue) %>%
 column_spec (1:ncol(camp),border_left = F, border_right = T)%>%
  kableExtra::kable_styling(font_size = 8,latex_options = "hold_position")


```


In tabella \@ref(tab:var-sr) la varianza associata ad ogni campione. Avremo che
\[E(\hat\sigma^2)=\frac 1 4 (`r hv[1]`+`r hv[2]`+`r hv[3]`+`r hv[4]`)=`r mean(hv)`\]

:::: {.nota data-latex=""}
$E(\hat\sigma^2)=`r mean(hv)`<`r sig2`=\sigma^2$, $\hat\sigma^2$ è distorto e, in media, sottostima la vera $\sigma^2$
::::
::::

:::: {.example name="Popolazione finita, estrazioni CR (Stima)"}
Se estraiamo CR, $\#\mathcal{S}=64$ lo spazio dei campioni è
```{r stima-cr,results="asis"}
xp <- c(0,1,3,7)
  
xxx <- expand.grid(xp,xp,xp)
xxx <- xxx[order(xxx[,1],xxx[,2],xxx[,3]),]
xord <- t(apply(xxx,1,sort))
cxord <- apply(xord,1,function(x)paste(x,collapse = ",") )
pr <- tapply(rep(1,64),cxord,sum)
pr <- pr/sum(pr)
# 

xxm <- apply(xxx, 1, mean)


camp <- unique(xord)

camp <- cbind(camp,pr)
h1 <- apply(camp[,1:3], 1, mean)
h2 <- apply(camp[,1:3], 1, median)

eh1 <- sum(pr*h1)
eh2 <- sum(pr*h2)
mse1 <- sum(pr*(h1-mu)^2)
mse2 <- sum(pr*(h2-mu)^2)

camp <- data.frame(unique(cxord),pr,h1,h2)
names(camp)<- c("Camp.","$P(C.)$","$h_1$","$h_2$")
#camp <- t(camp)
dimnames(camp)[[1]] <- paste("C.",1:nrow(camp))
mu<-mean(xp) 
sig<-mean(xp^2)-mean(xp)^2
kable((camp),booktabs=T,escape=F,caption = "Media e dediana dei campioni e probabilità associate") %>%
#column_spec (1:ncol(camp),border_left = F, border_right = T) %>%
  kableExtra::kable_styling(font_size = 8,latex_options = "hold_position")


```

:::: {.att data-latex=""}
**Attenzione** i campioni qui non hanno tutti la stessa probabilità, in tabella \@ref(tab:stima-cr) i campioni, le loro probabilità e le corrispettive varianze. Otterremo:
\[E(h)=h_\text{Camp. 1}P(\text{Camp. 1})+...+h_\text{Camp. 20}P(\text{Camp. 20})\]

E dunque:
\begin{eqnarray*}
  E(h_1)  &=& `r h1[1]`\times`r pr[1]`+`r h1[2]`\times`r pr[2]`+...+`r h1[20]`\times`r pr[20]` = `r eh1`=\mu\\
  E(h_2)  &=& `r h2[1]`\times`r pr[1]`+`r h2[2]`\times`r pr[2]`+...+`r h2[20]`\times`r pr[20]` = `r eh2`\neq\mu
\end{eqnarray*}
::::

Anche nel CCS CR $h_1$ è corretto per $\mu$ e $h_2$ **non** è corretto per $\mu$
::::

:::: {.example name="Popolazione finita, estrazioni CR (Efficienza)"}
Mentre l'errore
\[MSE(h)=(h_\text{Camp. 1}-\mu)^2P(\text{Camp. 1})+...+(h_\text{Camp. 20}-\mu)^2P(\text{Camp. 20})\]


E dunque
\begin{eqnarray*}
  MSE(h_1) &=& (`r h1[1]`-`r mu`)^2\times`r pr[1]`+(`r h1[2]`-`r mu`)^2\times`r pr[2]`+...+
    (`r h1[20]`-`r mu`)^2\times`r pr[20]`=`r mse1`\\
  MSE(h_2) &=& (`r h2[1]`-`r mu`)^2\times`r pr[1]`+(`r h2[2]`-`r mu`)^2\times`r pr[2]`+...+
    (`r h2[20]`-`r mu`)^2\times`r pr[20]`=`r mse2`
\end{eqnarray*}


Anche in questo caso, l'errore medio quadratico di $h_1$ è minore di quello di $h_2$, e quindi $h_1$ è più efficiente di $h_2$ ovvero più _preciso_.

:::: {.nota data-latex=""}
come definito dalla teoria abbiamo
\[MSE(h_1)=MSE(\hat \mu)=V(\hat \mu)=\frac{\sigma^2}n=\frac{`r sig2`}{3}=`r mse1`\]
::::
::::

:::: {.example name="Popolazione finita, estrazioni CR (Efficienza)"}

Proponiamo la varianza campionaria $\hat\sigma^2$ per stimare $\sigma^2=`r sig2`$
\[\hat\sigma^2=\frac 1 3\sum_{i=1}^3(x_i-\hat \mu)^2\]


```{r,results="asis"}
xp <- c(0,1,3,7)
  
xxx <- expand.grid(xp,xp,xp)
xxx <- xxx[order(xxx[,1],xxx[,2],xxx[,3]),]
xord <- t(apply(xxx,1,sort))
cxord <- apply(xord,1,function(x)paste(x,collapse = ",") )
pr <- tapply(rep(1,64),cxord,sum)
pr <- pr/sum(pr)
# 
n <- 3
xxm <- apply(xxx, 1, mean)

camp <- unique(xord)

camp <- cbind(camp,pr)
hv <- apply(camp[,1:3], 1, vvv)

ehv <- sum(pr*hv)

camp <- data.frame(unique(cxord),pr,hv)
names(camp)<- c("Camp.","$P(C.)$","$\\hat\\sigma^2$")
# camp <- t(camp)
dimnames(camp)[[1]] <- paste("C.",1:nrow(camp))
mu<-mean(xp) 
sig<-mean(xp^2)-mean(xp)^2
kable((camp),booktabs=T,escape=F,caption = "Varianza dei campioni estratti CR di ampiezza $n=3$") %>%
#column_spec (1:ncol(camp),border_left = F, border_right = T) %>%
kable_styling(font_size = 8)
   
```

E quindi
\[
  E(\hat\sigma^2)  = `r hv[1]`\times`r pr[1]`+`r hv[2]`\times`r pr[2]`+...+`r hv[20]`\times`r pr[20]` = `r ehv`<
  `r sig2`=\sigma^2
\]


Osserviamo che
\[
E(\hat\sigma^2)=\frac{n-1}n\sigma^2=\frac{`r n-1`}{`r n`}`r sig2`
\]

:::: 

### Distribuzione delle statistiche

```{r, echo=FALSE, results='asis'}

#curve(dgamma(x,9,.9),0,25)
set.seed(1)
xpop <- rgamma(10000,9,.9)
mu <- mean(xpop)
s2 <- vvv(xpop)
```


Estraggo campioni di ampiezza $n=5$ da una popolazione di $N=10000$ individui con media $\mu=`r mu`$ e varianza $\sigma^2=`r s2`$.
Osservo tre diversi stimatori per $\mu$: $h_1$, $h_2$ e $h_3$

```{r, echo=FALSE, results='asis'}

#curve(dgamma(x,9,.9),0,25)
xpop <- rgamma(10000,9,.9)
n <- 5
mu <- mean(xpop)
s2 <- vvv(xpop)
se <- sqrt(s2/n)

camp <- sapply(1:20, function(x)sample(xpop,n,T))
h1 <- apply(camp, 2, mean)
h2 <- h1+rnorm(20,0,1)
h3 <- h2+rnorm(20,0,2)
```

Se ripetessi l'estrazione un grande numero di volte potre vedere i tre stimatori nel grafico qui di seguito.
Lo stimatore in con la distribuzione in blu è il più efficiente dei tre: la probabilità che si avveri lontano dal vero parametro è minore che per gli altri due.
Mentre lo stimatore in con la distribuzione in verde è il meno efficiente dei tre: la probabilità che si avveri lontano dal vero parametro è ,aggiore che per gli altri due.


```{r}
par(mfrow=c(1,1),cex=cex)
curve(dnorm(x,mu,se),mu-4*se,mu+4*se,xlab="x",ylab="",axes=F,col=4)
axis(1,c(mu-4*se,mu,mu+4*se),c(round(mu-4*se,1),expression(mu),round(mu+4*se,2)))
axis(2)
points(h1[1:20],rep(0,times=20),pch=4,col=4)
points(h2[1:20],rep(0,times=20),pch=2,col=ared)
points(h3[1:20],rep(0,times=20),pch=5,col=3)
curve(dnorm(x,mu,se+.5),col=ared,add=T)
curve(dnorm(x,mu,se+1),col=3,add=T)
segments(mu,0,mu,dnorm(mu,mu,se),lty=2)

```


### Proprietà Auspicabili di uno stimatore (per $n\to\infty$)


Siano $X_1,...,X_n$, $n$ VC, IID, replicazioni della stessa $X\sim\mathscr{L}(\theta)$, sia $h$ uno stimatore per $\theta$

:::: {.info data-latex=""}
::: {.definition name="Correttezza Asintotica"}
Lo stimatore $h$ si dice **asintoticamente corretto** se
\[\lim_{n\to\infty}E(h(X_1,...,X_n))=E(h)=\theta\]
:::
::::
   
:::: {.example}
  \[\lim_{n\to\infty}E(\hat\sigma^2)=\lim_{n\to\infty}\frac{n-1}n\sigma^2=\sigma^2\]
::::

:::: {.info data-latex=""}
::: {.definition name="Correttezza Asintotica"}
Lo stimatore $h$ si dice **consistente** (in media quadratica) se e solo se
\[\lim_{n\to\infty}MSE(h(X_1,...,X_n))=\lim_{n\to\infty}MSE(h)=0\]

Essendo
\[MSE(h)=V(h)+B^2(h)\]
allora
\[\lim_{n\to\infty} MSE(h)=0, \text{ se e solo se} \lim_{n\to\infty} V(h)=0 \text{ e } \lim_{n\to\infty} B^2(h)=0\]
:::
::::

:::: {.example name="Consistenza"}
Siano $X_1,...,X_n$, $n$ VC, IID, replicazioni della stessa VC $X$ con $E(X)=\mu$ e $V(X)=\sigma^2$
Usiamo $\hat \mu$ per stimare $\mu$:
\[\hat \mu=\frac 1 n \sum_{i=1}^n X_i=\frac {S_n}n\]

Siccome $\hat \mu$ è stimatore corretto per $\mu$:
\[E(\hat \mu)=E\left(\frac{X_1+...+X_n}{n}\right)=\frac 1 n(E(X_1)+...+E(X_n))=\frac 1 n (\mu+...+\mu)=\mu\]


Allora
\[MSE(\hat \mu)=V(\hat \mu)=\frac {\sigma^2}n\]


Al divergere di $n$
\[\lim_{n\to \infty}MSE(\hat \mu)=\lim_{n\to\infty}\frac{\sigma^2}n=0\]

Lo stimatore $\hat \mu$ per $\mu$ è stimatore **corretto** e **consistente**.
::::

## La $SD$ e lo $SE$

:::: {.info data-latex=""}
La _standard deviation_ (SD) $\sigma$, rappresenta la dispersione degli individui dalla media, è un indicatore di *variabilità* della *popolazione*, per esempio in una popolazione finita di $N$ individui:
\[\sigma=\sqrt{\sigma^2}=\sqrt{\frac 1 N\sum_{i=1}^N(x_i-\mu)^2},\]
la _deviazione standard_ $\sigma$ è la radice della varianza della popolazione $\sigma^2$.

Lo _standard error_ $SE(h)$ di uno stimatore $h$ per $\theta$ è un indicatore della *variabilità* dello stimatore nello *spazio dei parametri*
\[SE(h)=\sqrt{V(h)}\]
Lo _standard error_ $SE(h)$ di uno stimatore $h$ per $\theta$ è la radice della varianza della VC $h$.

La _standard deviation stimata_ $\sigma$, rappresenta la dispersione degli individui _del campione_ dalla media _del campione_, è un indicatore di *variabilità* del *campione*:
\[\hat\sigma=\sqrt{\hat\sigma^2}=\sqrt{\frac 1 n\sum_{i=1}^n(x_i-\hat\mu)^2}\]
La _deviazione standard stimata_ $\hat\sigma$ è la radice della varianza del campione $\hat\sigma^2$.
::::

:::: {.example}
Lo standard error dello stimatore media aritmetica campionaria $\hat\mu$ per $\mu$
\[SE(\hat \mu)=\sqrt\frac{\sigma^2} n=\frac\sigma {\sqrt{n}}\]

L'errore che si commette nello stimare una media dipende da due fattori

  - la _standard deviation_ $\sigma$ che indica la variabilità degli individui tra di loro
  - $1/\sqrt n$ che è l'inverso dell'ampiezza del campione

Se $\sigma$ è incognito viene stimato da (come vedremo nel paragrafo (\@ref(vnorm)))
\[S=\sqrt{S^2}=\sqrt{\frac{n}{n-1}\hat\sigma^2}\]

Ottenendo
\[\widehat{SE(\hat\mu)}=\frac S {\sqrt n}\]
::::
